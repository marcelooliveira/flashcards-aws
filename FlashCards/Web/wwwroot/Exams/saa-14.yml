questions:
  - id: q651
    type: multiple_choice
    question: |
      A company is designing a storage solution for a large volume of media files. The files are accessed frequently for the first 180 days. After that, they are accessed once or twice a year for auditing, but must be available instantly if requested. After 5 years, the files are only needed for long-term compliance and can have a retrieval time of several hours.
    options:
     - text: Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3 Glacier Instant Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.
       is_correct: true
     - text: Transition the objects to S3 One Zone-IA after 180 days, S3 Glacier Flexible Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.
       is_correct: false
     - text: Use S3 Intelligent-Tiering to manage all transitions automatically over the 5-year period.
       is_correct: false
     - text: Transition the objects to S3 Standard-IA after 180 days and then directly to S3 Glacier Flexible Retrieval after 360 days.
       is_correct: false
    explanation: |
      Correct: S3 Standard-IA provides high durability for infrequent access. S3 Glacier Instant Retrieval is the only archive tier that offers "instant" (millisecond) access for auditing needs. S3 Glacier Deep Archive is the most cost-effective tier for long-term compliance where hours of retrieval time are acceptable.
      Incorrect: 
        - S3 One Zone-IA lacks the multi-AZ durability typically required for "core company assets."
        - S3 Glacier Flexible Retrieval takes minutes to hours to retrieve data, violating the "instant availability" requirement for audits.
        - Intelligent-Tiering is effective but might not be as cost-optimized as a specific lifecycle policy for a known 5-year retention pattern.

  - id: q652
    type: multiple_choice
    question: |
      A data analytics company runs a daily 6-hour Apache Spark job on Amazon EMR. The workload is fault-tolerant and the company wants to minimize costs as much as possible while ensuring the job finishes within the 6-hour window.
    options:
     - text: Configure a transient cluster with primary and core nodes on On-Demand Instances and task nodes on Spot Instances.
       is_correct: true
     - text: Use a long-running cluster with all nodes on Reserved Instances to ensure 100% availability.
       is_correct: false
     - text: Run the entire cluster (primary, core, and task nodes) on Spot Instances to maximize savings.
       is_correct: false
     - text: Use a transient cluster with core nodes on Spot Instances and task nodes on On-Demand Instances.
       is_correct: false
    explanation: |
      Correct: For a fault-tolerant Spark job, using a transient cluster (which shuts down after the job) is cost-effective. Placing the primary/core nodes on On-Demand ensures the cluster's stability (HDFS and cluster management), while Spot Instances for task nodes provide the 90% discount needed for the heavy lifting.
      Incorrect: 
        - Long-running clusters with Reserved Instances are more expensive for a job that only runs for 6 hours a day.
        - Running primary nodes on Spot is risky; if the primary node is reclaimed, the entire cluster fails.

  - id: q682
    type: multiple_choice
    question: |
      A company must ensure that all Amazon EBS volumes in its AWS account are encrypted using a specific customer managed key (CMK). If a volume is found to be unencrypted or using the wrong key, it must be automatically identified and flagged for remediation.
    options:
     - text: Use AWS Config to detect noncompliant volumes and use AWS Systems Manager Automation to remediate or notify the team.
       is_correct: true
     - text: Enable Amazon GuardDuty to monitor EBS encryption events and trigger an AWS Lambda function to encrypt the volumes.
       is_correct: false
     - text: Use Amazon Macie to scan EBS volumes for encryption status and data classification.
       is_correct: false
     - text: Create an IAM policy that denies the 'ec2:CreateVolume' action unless the 'Encrypted' parameter is set to true.
       is_correct: false
    explanation: |
      Correct: AWS Config is the standard service for auditing resource configurations. Combined with Systems Manager, it can automatically remediate non-compliant resources.
      Incorrect: 
        - GuardDuty is for threat detection (malicious activity), not configuration auditing.
        - Macie is for discovering sensitive data (PII) within S3, not for managing EBS disk encryption.
        - IAM policies can prevent future mistakes but cannot "identify and flag" existing non-compliant volumes.

  - id: q683
    type: multiple_choice
    question: |
      A company is migrating a legacy PHP application to AWS. The application currently runs on a single server with a local MySQL database. The company requires high availability and minimal manual intervention during scaling.
    options:
     - text: Migrate the web tier to an Application Load Balancer with an Auto Scaling group; migrate the database to Amazon RDS in a Multi-AZ deployment.
       is_correct: true
     - text: Re-host the application on a single large EC2 instance with an EBS Provisioned IOPS volume for the database.
       is_correct: false
     - text: Use AWS Lambda for the PHP application and Amazon DynamoDB for the data layer.
       is_correct: false
     - text: Deploy the application on Amazon LightSail with a managed database add-on.
       is_correct: false
    explanation: |
      Correct: This follows the standard "High Availability" blueprint. ALB + Auto Scaling handles the web tier, while RDS Multi-AZ ensures the database can fail over automatically.
      Incorrect: 
        - A single large instance remains a single point of failure.
        - Moving to Lambda and DynamoDB requires a complete code refactor, which contradicts a standard "migration" of a legacy PHP/MySQL app.
        - LightSail is designed for simpler projects and lacks the enterprise scaling features of an ALB/ASG setup.

  - id: q684
    type: multiple_choice
    question: |
      A company located near the eu-central-1 Region wants to migrate its web applications to AWS. Due to strict data residency regulations, specific applications cannot be launched directly in eu-central-1, but they still require single-digit millisecond latency for local users.
    options:
     - text: Deploy the applications in AWS Local Zones by extending the company's VPC from eu-central-1 to the chosen Local Zone.
       is_correct: true
     - text: Use Amazon CloudFront with Lambda@Edge to process requests at the edge locations.
       is_correct: false
     - text: Deploy the applications in AWS Wavelength Zones to leverage 5G network speeds.
       is_correct: false
     - text: Create a VPC peering connection between eu-central-1 and another nearby region like eu-west-1.
       is_correct: false
    explanation: |
      Correct: AWS Local Zones place compute and storage closer to specific geographic areas. They allow you to satisfy residency requirements while maintaining ultra-low (single-digit millisecond) latency.
      Incorrect: 
        - Lambda@Edge is for compute at the edge, but it doesn't provide a full application environment or persistent database storage for residency.
        - Wavelength Zones are specifically for 5G mobile applications, not general web application residency.
        - Multi-region peering does not solve the residency requirement if the data must stay near the company's specific local jurisdiction.