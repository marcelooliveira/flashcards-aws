questions:
  - id: q501
    type: multiple_choice
    question: |
      A company wants to ingest customer payment data into the company's data lake in Amazon S3. The company receives payment data every minute on average. The company wants to analyze the payment data in real time. Then the company wants to ingest the data into the data lake.
    options:
     - text: Use Amazon Kinesis Data Firehose to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real time.
       is_correct: true
     - text: Create an AWS Lambda function to poll an SQS queue every minute and write the results to an S3 bucket for batch processing.
       is_correct: false
     - text: Use AWS DataSync to move payment data from on-premises servers to S3 and run Athena queries on a schedule.
       is_correct: false
     - text: Set up an Amazon EC2 instance running a custom Python script to intercept packets and store them in an Amazon EBS volume.
       is_correct: false
    explanation: |
      Correct: Amazon Kinesis Data Firehose is the standard for loading streaming data into S3 (data lakes). Combined with Kinesis Data Analytics, it allows for real-time SQL-based analysis on the data as it flows through the pipeline.
      Incorrect: 
        - SQS and Lambda polling are better suited for task decoupling rather than real-time streaming analytics and ingestion into a data lake.
        - AWS DataSync is for file/object migration, not for real-time streaming data ingestion.
        - Custom scripts on EC2 involve high operational overhead and do not scale automatically like Kinesis.



  - id: q502
    type: multiple_choice
    question: |
      A company runs a website that uses a content management system (CMS) on Amazon EC2. The CMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance for the data tier. Website images are stored on an Amazon Elastic Block Store (Amazon EBS) volume that is mounted inside the EC2 instance.
    options:
     - text: Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.
       is_correct: true
     - text: Increase the size of the EBS volume and use EBS Multi-Attach to share the volume across multiple instances.
       is_correct: false
     - text: Use Amazon S3 Transfer Acceleration to move images to a global bucket and mount the bucket using S3fs.
       is_correct: false
     - text: Replicate the EBS volume across multiple Availability Zones using AWS Backup and mount them individually.
       is_correct: false
    explanation: |
      Correct: Amazon EFS is a managed NFS file system that can be mounted simultaneously by multiple EC2 instances across different AZs. This is the recommended way to share a common media library for a CMS like WordPress or Drupal when scaling horizontally.
      Incorrect: 
        - EBS Multi-Attach is only supported for Provisioned IOPS (io1/io2) volumes and requires a cluster-aware file system; it is not a standard "shared folder" solution for web servers.
        - Mounting S3 as a local file system (S3fs) is not recommended for production CMS workloads due to high latency and lack of full POSIX compliance.
        - Replicating volumes provides a backup but doesn't allow real-time shared access to the same set of images across different servers.



  - id: q549
    type: multiple_choice
    question: |
      A company is deploying a new web application in a VPC. The application will be hosted on EC2 instances across multiple Availability Zones. The instances are in the private subnets. The MySQL database needs to retrieve product catalog and pricing information that is hosted on the internet by a third-party provider. A solutions architect must devise a strategy that maximizes security without increasing operational overhead.
    options:
     - text: Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all internet-bound traffic to the NAT gateway.
       is_correct: true
     - text: Assign a public IP address to each EC2 instance and configure the Security Groups to allow outbound traffic.
       is_correct: false
     - text: Deploy a NAT instance in a private subnet and manage the routing table manually for each AZ.
       is_correct: false
     - text: Create a Transit Gateway with a VPN connection to the third-party provider's network.
       is_correct: false
    explanation: |
      Correct: A NAT gateway is a managed service that provides outbound-only internet access for instances in private subnets. It requires less operational overhead than a NAT instance and is highly available by design when deployed per AZ.
      Incorrect: 
        - Assigning public IPs to instances in a private subnet violates the "private" architecture and increases the attack surface.
        - NAT instances are unmanaged, require manual patching, and do not scale automatically (higher operational overhead).
        - A VPN via Transit Gateway is complex and only works if the third-party provider supports a Site-to-Site VPN, which is unlikely for general internet-hosted catalog data.

  - id: q550
    type: multiple_choice
    question: |
      A company is using AWS Key Management Service (AWS KMS) keys to encrypt AWS Lambda environment variables. A solutions architect needs to ensure that the required permissions are in place to decrypt and use the environment variables.
    options:
     - text: Add AWS KMS permissions in the Lambda execution role.
       is_correct: true
     - text: Allow the Lambda execution role in the AWS KMS key policy.
       is_correct: true
     - text: Create an IAM User policy for the Lambda function and attach it to the VPC.
       is_correct: false
     - text: Enable "Default Encryption" in the Lambda console and use the AWS managed key (aws/lambda).
       is_correct: false
     - text: Grant the `kms:Decrypt` permission to the S3 bucket where the Lambda code is stored.
       is_correct: false
    explanation: |
      Correct: To use a Customer Managed Key (CMK) with Lambda, two permissions are required: the Lambda's IAM Execution Role must have the `kms:Decrypt` permission, and the KMS Key Policy itself must explicitly allow the Lambda role to use it.
      Incorrect: 
        - IAM policies are not attached to VPCs; they are attached to roles, users, or groups.
        - While using the managed key (`aws/lambda`) simplifies things, the question specifies that the company is "using KMS keys" (implying customer-managed keys) and asks for the "required permissions" to make that specific setup work.
        - Permitting the S3 bucket does not help the Lambda service decrypt environment variables at runtime.