questions:
  - id: q601
    type: multiple_choice
    question: |
      A company runs its critical database on an Amazon RDS for PostgreSQL DB instance. The company wants to migrate to Amazon Aurora PostgreSQL with minimal downtime and data loss.

      Which solution will meet these requirements with the LEAST operational overhead?
    options:
     - text: Create an Aurora read replica of the RDS for PostgreSQL DB instance. Promote the Aurora read replicate to a new Aurora PostgreSQL DB cluster.
       is_correct: true
     - text: Perform a logical dump using pg_dump and restore it to a new Aurora PostgreSQL cluster.
       is_correct: false
     - text: Use AWS Database Migration Service (AWS DMS) to perform a full load and ongoing replication between RDS and Aurora.
       is_correct: false
     - text: Take a snapshot of the RDS for PostgreSQL instance and migrate the snapshot to a new Aurora PostgreSQL cluster.
       is_correct: false
    explanation: |
      Correct: Creating an Aurora read replica is the most efficient native method. Aurora uses the same storage engine compatibility, allowing for low-lag replication. Promoting the replica ensures a fast cutover with minimal downtime.
      Incorrect: 
        - pg_dump involves significant downtime as the database grows and requires manual effort.
        - AWS DMS is powerful but introduces more operational complexity (managing replication instances) compared to a native Aurora replica.
        - Migrating a snapshot requires stopping writes to ensure data consistency, leading to more downtime than a live replica promotion.

  - id: q602
    type: multiple_choice
    question: |
      A company's infrastructure consists of hundreds of Amazon EC2 instances that use Amazon Elastic Block Store (Amazon EBS) storage. A solutions architect must ensure that every EC2 instance can be recovered after a disaster.

      What should the solutions architect do to meet this requirement with the LEAST amount of effort?
    options:
     - text: Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS Backup API or the AWS CLI to speed up the restore process for multiple EC2 instances.
       is_correct: true
     - text: Create a custom Python script using the AWS SDK (Boto3) to take daily snapshots of all EBS volumes and tag them for recovery.
       is_correct: false
     - text: Set up Amazon Data Lifecycle Manager (Amazon DLM) to manage snapshots for each individual EC2 instance.
       is_correct: false
     - text: Use an AWS Lambda function triggered by a CloudWatch Event to create AMIs of every instance once per day.
       is_correct: false
    explanation: |
      Correct: AWS Backup is a centralized, fully managed service that simplifies backup management across many EC2 instances and EBS volumes, significantly reducing the "effort" compared to manual scripting.
      Incorrect: 
        - Custom scripts require development and ongoing maintenance effort.
        - Amazon DLM is excellent for snapshots but lacks the centralized cross-service management and advanced restore features of AWS Backup.
        - Lambda-based AMI creation requires managing execution logs and handling failures manually.

  - id: q603
    type: multiple_choice
    question: |
      A company recently migrated to the AWS Cloud. The company wants a serverless solution for large-scale parallel on-demand processing of a semistructured dataset. The data consists of logs, media files, sales transactions, and IoT sensor data that is stored in Amazon S3. The company wants the solution to process thousands of items in the dataset in parallel.
    options:
     - text: Use the AWS Step Functions Map state in Distributed mode to process the data in parallel.
       is_correct: true
     - text: Use AWS Batch to submit thousands of jobs to a managed compute environment.
       is_correct: false
     - text: Configure an Amazon S3 Event Notification to trigger a Lambda function for every new object uploaded.
       is_correct: false
     - text: Launch an Amazon EMR cluster with Spark to process the data in S3 using an Auto Scaling policy.
       is_correct: false
    explanation: |
      Correct: The Distributed Map state in Step Functions is specifically optimized for serverless, high-concurrency processing (up to 10,000 parallel executions) of data stored in S3.
      Incorrect: 
        - AWS Batch is container-based and not strictly "serverless" in the same way as Step Functions + Lambda; it also involves more queue management.
        - S3 Event triggers are for real-time ingestion, not for "on-demand processing of a dataset" (batch-style).
        - EMR is not serverless (though Serverless EMR exists, it is generally more complex to set up than a Step Function Map state for simple parallel tasks).


  - id: q604
    type: multiple_choice
    question: |
      A company will migrate 10 PB of data to Amazon S3 in 6 weeks. The current data center has a 500 Mbps uplink to the internet. Other on-premises applications share the uplink. The company can use 80% of the internet bandwidth for this one-time migration task.
    options:
     - text: Order multiple AWS Snowball devices. Copy the data to the devices. Send the devices to AWS to copy the data to Amazon S3.
       is_correct: true
     - text: Set up an AWS Direct Connect connection with 1 Gbps capacity to facilitate the transfer.
       is_correct: false
     - text: Use AWS DataSync to transfer the data over the existing 500 Mbps internet connection.
       is_correct: false
     - text: Order an AWS Snowmobile to migrate the 10 PB of data.
       is_correct: false
    explanation: |
      Correct: Transferring 10 PB over a 400 Mbps (80% of 500) connection would take years. Snowball devices are the only viable way to meet the 6-week deadline.
      Incorrect: 
        - 1 Gbps Direct Connect is still too slow for 10 PB in 6 weeks.
        - DataSync over a limited uplink cannot overcome the laws of physics regarding bandwidth.
        - Snowmobile is typically for data sizes of 100 PB or more; 10 PB is better handled by a fleet of Snowball Edge devices.

  - id: q607
    type: multiple_choice
    question: |
      A company has migrated a two-tier application from its on-premises data center to the AWS Cloud. The data tier is a Multi-AZ deployment of Amazon RDS for Oracle with 12 TB of EBS storage. The application processes and stores documents as binary large objects (blobs) with an average size of 6 MB. The database size has grown over time, reducing performance and increasing costs.
    options:
     - text: Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store the object metadata in the existing database.
       is_correct: true
     - text: Enable Amazon RDS Storage Auto Scaling to handle the growing data volume.
       is_correct: false
     - text: Change the EBS volume type from General Purpose SSD (gp3) to Provisioned IOPS SSD (io2).
       is_correct: false
     - text: Use Amazon Elastic File System (Amazon EFS) to store the documents and mount it to the RDS instance.
       is_correct: false
    explanation: |
      Correct: Offloading Blobs to S3 is an architectural best practice. It reduces RDS storage costs and decreases the size of database backups/snapshots, improving overall performance.
      Incorrect: 
        - Auto Scaling solves capacity but doesn't solve the high cost and performance degradation of a bloated database.
        - Moving to io2 increases cost significantly without addressing the underlying issue of storing files in a relational engine.
        - You cannot mount an EFS file system directly to a managed Amazon RDS instance.


  - id: q611
    type: multiple_choice
    question: |
      A company has an application with a REST-based interface. The third-party vendor has received many 503 Service Unavailable Errors when sending data. When the data volume spikes, compute capacity reaches its limit.
    options:
     - text: Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.
       is_correct: true
     - text: Increase the instance size of the EC2 instances and set up an Auto Scaling group based on CPU utilization.
       is_correct: false
     - text: Use an Amazon SQS queue to buffer the incoming requests and use EC2 instances to process the queue.
       is_correct: false
     - text: Deploy an Application Load Balancer with a larger target group to handle the spikes.
       is_correct: false
    explanation: |
      Correct: Kinesis provides a buffer (stream) that decoupling ingestion from processing. Lambda scales automatically to the shards, preventing the 503 errors caused by compute saturation.
      Incorrect: 
        - Larger instances still have a limit and take time to scale out, which might not be fast enough for sudden spikes.
        - SQS is good, but for "near-real time" analysis and potential multiple consumers of the same data, Kinesis is often the preferred streaming solution for vendors.
        - Load balancers don't solve the backend compute capacity limit; they just distribute the failure.

  - id: q616
    type: multiple_choice
    question: |
      A company needs a solution that continuously monitors for malicious activity in the AWS account, workloads, and access patterns to the S3 bucket. Suspicious activity must be displayed on a dashboard.
    options:
     - text: Configure Amazon GuardDuty to monitor and report findings to AWS Security Hub.
       is_correct: true
     - text: Enable VPC Flow Logs and use Amazon Athena to query for malicious IP addresses.
       is_correct: false
     - text: Use AWS WAF to monitor S3 bucket access patterns and block unauthorized requests.
       is_correct: false
     - text: Configure AWS Config to detect changes in S3 bucket policies and trigger a Lambda function for remediation.
       is_correct: false
    explanation: |
      Correct: GuardDuty uses machine learning and threat intelligence to identify anomalies (like S3 exfiltration). Security Hub provides the centralized dashboard for these findings.
      Incorrect: 
        - Athena queries on Flow Logs are manual and do not provide continuous automated monitoring for malicious "activity patterns."
        - AWS WAF protects web applications (ALB/CloudFront), not direct S3 bucket access patterns.
        - AWS Config monitors resource configuration (state), not behavioral "activity" or "access patterns."