questions:
  - id: q151
    type: multiple_choice
    question: |
      A company wants to migrate its on-premises data center to AWS. According to the company's compliance requirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to connect VPCs to the internet.
      Which solutions will meet these requirements? (Choose two.)
    options:
     - text: Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.
       is_correct: true
     - text: Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0. Create an IAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3.
       is_correct: true
     - text: Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.
       is_correct: false
     - text: Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-3 in the AWS account settings.
       is_correct: false
     - text: Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3.
       is_correct: false
    explanation: |
      Correct: Organization-level SCPs (Service Control Policies) can restrict the use of regions and the creation of internet gateway resources. NACLs provide an additional network security layer at the subnet level to block outbound traffic.
      Incorrect:
        - AWS Control Tower can apply guardrails, but it is not the primary tool for granular network connectivity blocking in legacy VPCs without additional infrastructure.
        - AWS WAF is an application layer firewall for filtering HTTP/HTTPS traffic; it does not control account regional access or basic VPC connectivity.
        - AWS Config only detects and alerts (monitoring); it does not proactively prevent administrators from making prohibited connections.

    diagram: |
      graph TD
        A[Usuário] -->|Acesso permitido| B[ap-northeast-3]
        A -.->|Acesso negado| C[Outras regiões]
        B -->|Sem internet| D[VPC sem IGW]

  - id: q152
    type: multiple_choice
    question: |
      A company uses a three-tier web application to provide training to new employees. The application is accessed for only 12 hours every day. The company is using an Amazon RDS for MySQL DB instance to store information and wants to minimize costs.
      What should a solutions architect do to meet these requirements?
    options:
     - text: Create AWS Lambda functions to start and stop the DB instance. Use Amazon EventBridge (CloudWatch Events) to trigger the functions based on the daily schedule.
       is_correct: true
     - text: Convert the DB instance to an Amazon RDS Multi-AZ deployment.
       is_correct: false
     - text: Migrate the database to an Amazon Aurora Serverless cluster with a minimum capacity of zero.
       is_correct: false
     - text: Use an Amazon RDS Reserved Instance for a 3-year term.
       is_correct: false
    explanation: |
      Correct: Automating the shutdown of the RDS instance during the 12 hours it is not used is the most direct way to save on database compute costs.
      Incorrect:
        - Multi-AZ increases availability but doubles the cost, which is contrary to the goal of cost savings.
        - Although Aurora Serverless could scale to zero, the question specifically asks for a solution for the existing RDS MySQL instance.
        - Reserved Instances are better for 24/7 workloads; for something used only 50% of the time, scheduled shutdown is usually more efficient if startup time is acceptable.

    diagram: |
      graph TD
        A[EventBridge] --> B[Lambda Start/Stop]
        B --> C[RDS MySQL]
        C --> D[App Web]

  - id: q153
    type: multiple_choice
    question: |
      A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most accessed files readily available for its users.
      Which action should the company take to meet these requirements MOST cost-effectively?
    options:
     - text: Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.
       is_correct: true
     - text: Move all files to S3 Glacier Deep Archive immediately to maximize savings.
       is_correct: false
     - text: Configure S3 Intelligent-Tiering for all objects in the bucket.
       is_correct: false
     - text: Create a Lambda function to delete files older than 90 days.
       is_correct: false
    explanation: |
      Correct: S3 Standard-IA is ideal for data that is accessed less frequently but requires millisecond access when requested, offering lower storage cost than Standard.
      Incorrect:
        - Glacier Deep Archive has hours of retrieval latency, which would prevent ringtones from being "readily available".
        - Intelligent-Tiering has a per-object monitoring fee; for millions of small files (even >128KB), the fee can outweigh the savings if the access pattern is predictable (90 days).
        - Deleting files removes the company's product, which does not meet the requirement to keep access, even if infrequent.

    diagram: |
      graph TD
        A[S3 Standard] --90 dias--> B[S3 Standard-IA]
        B --> C[Usuário]

  - id: q154
    type: multiple_choice
    question: |
      A company needs to save the results from a medical trial to an Amazon S3 repository. The repository must allow a few scientists to add new files and must restrict all other users to read-only access. No users can have the ability to modify or delete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its creation date.
      Which solution will meet these requirements?
    options:
     - text: Use S3 Object Lock in compliance mode with a retention period of 365 days.
       is_correct: true
     - text: Enable S3 Versioning and set a lifecycle policy to transition old versions to Glacier.
       is_correct: false
     - text: Use a bucket policy that denies the s3:DeleteObject action for everyone.
       is_correct: false
     - text: Use AWS Shield Advanced to protect the S3 bucket from modification.
       is_correct: false
    explanation: |
      Correct: S3 Object Lock "Compliance Mode" ensures that not even the root user can delete or overwrite files until the retention period expires, meeting strict regulatory requirements.
      Incorrect:
        - Versioning allows keeping copies but does not prevent current versions from being deleted or versioning from being disabled.
        - Bucket policies can be changed by administrators, while Object Lock in compliance mode is immutable.
        - AWS Shield is for DDoS mitigation, not file retention control.

    diagram: |
      graph TD
        A[Cientista] --> B[S3 Bucket]
        B --Object Lock--> C[Retenção 1 ano]
        C --> D[Leitura permitida]

  - id: q155
    type: multiple_choice
    question: |
      A large media company hosts a web application on AWS. The company wants to start caching confidential media files so that users around the world will have reliable access to the files. The content is stored in Amazon S3 buckets. The company must deliver the content quickly, regardless of where the requests originate geographically.
      Which solution will meet these requirements?
    options:
     - text: Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers. Use signed URLs or signed cookies for access control.
       is_correct: true
     - text: Enable S3 Transfer Acceleration on the origin bucket.
       is_correct: false
     - text: Use AWS Global Accelerator to route traffic to the S3 bucket.
       is_correct: false
     - text: Replicate the S3 bucket to every AWS Region using Cross-Region Replication (CRR).
       is_correct: false
    explanation: |
      Correct: CloudFront is AWS's CDN that caches content in hundreds of edge locations, drastically reducing global latency.
      Incorrect:
        - Transfer Acceleration improves *upload* speed to S3, not cached download for end users.
        - Global Accelerator improves network performance via Anycast IP but does not cache content like CloudFront.
        - Replicating buckets to all regions is extremely expensive, complex to manage, and does not solve the "last mile" latency between the region and the end user.

    diagram: |
      graph TD
        A[S3 Bucket] --> B[CloudFront]
        B --> C[Usuário Global]

  - id: q156
    type: multiple_choice
    question: |
      A company produces batch data that comes from different databases. The company also produces live stream data from network sensors and application APIs. The company needs to consolidate all the data into one place for business analytics. The company needs to process the incoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool to show key performance indicators (KPIs).
      Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)
    options:
     - text: Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
       is_correct: true
     - text: Use Amazon Kinesis Data Firehose to ingest and process the live stream data directly into Amazon S3.
       is_correct: true
     - text: Create custom AWS Lambda functions to move the individual records from the databases to an Amazon Redshift cluster.
       is_correct: false
     - text: Use Amazon EMR to process the incoming data. Use Amazon Redshift to store the processed data.
       is_correct: false
     - text: Use AWS Glue to process the incoming data. Use Amazon S3 to store the processed data.
       is_correct: false
    explanation: |
      Correct: Kinesis Data Firehose is the simplest (low overhead) way to load streams into S3. Athena allows serverless SQL queries directly on S3, and QuickSight is the native BI tool.
      Incorrect:
        - Redshift and EMR require cluster and infrastructure management, increasing operational overhead compared to Athena and Firehose.
        - Lambda to move individual records is complex to scale for large volumes of sensor and API data in real time.

    diagram: |
      graph TD
        A[Stream de dados] --> B[Kinesis Firehose]
        B --> C[S3 Buckets]
        C --> D[Athena/QuickSight]

  - id: q157
    type: multiple_choice
    question: |
      A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora.
      Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)
    options:
     - text: Use AWS Backup to manage the DB cluster snapshots and set a lifecycle policy to delete them after 5 years.
       is_correct: true
     - text: Configure an Amazon CloudWatch Logs export for the DB cluster to store audit logs in CloudWatch Logs with an indefinite retention period.
       is_correct: true
     - text: Take a manual snapshot of the DB cluster.
       is_correct: false
     - text: Create a lifecycle policy for the automated backups.
       is_correct: false
     - text: Configure automated backup retention for 5 years.
       is_correct: false
    explanation: |
      Correct: AWS Backup centralizes snapshot management and allows you to set 5-year expiration policies automatically. Exporting logs to CloudWatch Logs ensures indefinite audit retention outside the database.
      Incorrect:
        - Manual snapshots do not expire on their own; they would require a manual deletion process after 5 years.
        - Aurora automated backups have a maximum retention window of 35 days, insufficient for the 5-year requirement.
        - There are no native lifecycle policies for RDS/Aurora automated backups beyond the 35-day retention window.

    diagram: |
      graph TD
        A[Aurora PostgreSQL] --> B[AWS Backup]
        B -->|Retenção 5 anos| C[Snapshots]
        A --> D[CloudWatch Logs]
        D -->|Retenção indefinida| E[Audit Logs]

  - id: q158
    type: multiple_choice
    question: |
      A solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience.
      Which service will improve the performance of both the real-time and on-demand streaming?
    options:
     - text: Amazon CloudFront
       is_correct: true
     - text: AWS Global Accelerator
       is_correct: false
     - text: Amazon Route 53
       is_correct: false
     - text: Amazon ElastiCache
       is_correct: false
    explanation: |
      Correct: CloudFront supports streaming protocols (like HLS/DASH) and caches video fragments at the edge, which is essential to reduce buffering for both live and on-demand video.
      Incorrect:
        - Global Accelerator optimizes the network (IP) layer but does not cache media content, being less efficient than a CDN for video streaming.
        - Route 53 is just DNS; it routes the user but does not improve video data delivery.
        - ElastiCache is for in-memory data caching (databases), not for large-scale video file delivery to end users.

    diagram: |
      graph TD
        A[Vídeo ao vivo] --> B[CloudFront]
        B --> C[Usuário Global]
        D[Vídeo sob demanda] --> B

  - id: q159
    type: multiple_choice
    question: |
      A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda. The application�s traffic recently spiked due to fraudulent requests from botnets.
      Which steps should a solutions architect take to block requests from unauthorized users? (Choose two.)
    options:
     - text: Create a usage plan with an API key that is shared with genuine users only.
       is_correct: true
     - text: Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.
       is_correct: true
     - text: Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.
       is_correct: false
     - text: Convert the existing public API to a private API. Update the DNS records to redirect users to the new API endpoint.
       is_correct: false
     - text: Create an IAM role for each user attempting to access the API. A user will assume the role when making the API call.
       is_correct: false
    explanation: |
      Correct: AWS WAF protects against common web attacks and botnets at the edge. Usage plans with API keys allow you to limit who can call the API and at what frequency (throttling).
      Incorrect:
        - Filtering IPs inside Lambda is inefficient and expensive, as you pay for function execution even to deny traffic.
        - Private APIs are only for access within a VPC (via Direct Connect/VPN), which would break access for legitimate users on the public internet.
        - Creating IAM roles for every public user of a botnet is technically impossible and administratively unfeasible.

    diagram: |
      graph TD
        A[Usuário] -->|API Key| B[API Gateway]
        B -->|WAF| C[Lambda]
        C --> D[Resposta]

  - id: q160
    type: multiple_choice
    question: |
      An ecommerce company hosts its analytics application in the AWS Cloud. The application generates about 300 MB of data each month. The data is stored in JSON format. The company is evaluating a disaster recovery solution to back up the data. The data must be accessible in milliseconds if it is needed, and the data must be kept for 30 days.
      Which solution meets these requirements MOST cost-effectively?
    options:
     - text: Amazon S3 Standard
       is_correct: true
     - text: Amazon OpenSearch Service (Amazon Elasticsearch Service)
       is_correct: false
     - text: Amazon S3 Glacier Flexible Retrieval
       is_correct: false
     - text: Amazon RDS for PostgreSQL
       is_correct: false
    explanation: |
      Correct: For only 300 MB per month with a need for millisecond access, S3 Standard is extremely cheap and perfectly meets the performance requirement.
      Incorrect:
        - OpenSearch and RDS have continuous instance costs that are orders of magnitude higher than simple S3 storage for this amount of data.
        - Glacier does not offer millisecond access (requires minutes or hours for restoration).

    diagram: |
      graph TD
        A[App Analytics] --> B[S3 Standard]
        B --> C[Recuperação em ms]

  - id: q161
    type: multiple_choice
    question: |
      A company has a small Python application that processes JSON documents and outputs the results to an on-premises SQL database. The application runs thousands of times each day. The company wants to move the application to the AWS Cloud. The company needs a highly available solution that maximizes scalability and minimizes operational overhead.
      Which solution will meet these requirements?
    options:
     - text: Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.
       is_correct: true
     - text: Deploy the Python application on a fleet of Amazon EC2 instances in an Auto Scaling group.
       is_correct: false
     - text: Use AWS Glue to process the documents in batch mode every hour.
       is_correct: false
     - text: Host the Python application on AWS Elastic Beanstalk with a Multi-AZ RDS MySQL database.
       is_correct: false
    explanation: |
      Correct: This is an event-driven and fully serverless architecture. S3 triggers Lambda (which scales automatically) and Aurora provides high availability for the data, with minimal server management.
      Incorrect:
        - EC2 and Elastic Beanstalk require patch, OS, and capacity management, increasing operational overhead.
        - AWS Glue is aimed at large-scale ETL and batch; to process documents as they arrive (thousands per day), Lambda is more agile and cost-effective.

    diagram: |
      graph TD
        A[S3 Bucket] -->|Novo JSON| B[Lambda]
        B --> C[Aurora DB]

  - id: q162
    type: multiple_choice
    question: |
      A company wants to use high performance computing (HPC) infrastructure on AWS for financial risk modeling. The company�s HPC workloads run on Linux. Each HPC workflow runs on hundreds of Amazon EC2 Spot Instances, is short-lived, and generates thousands of output files that are ultimately stored in persistent storage for analytics and long-term future use.
      The company seeks a cloud storage solution that permits the copying of on-premises data to long-term persistent storage to make data available for processing by all EC2 instances. The solution should also be a high performance file system that is integrated with persistent storage to read and write datasets and output files.
      Which combination of AWS services meets these requirements?
    options:
     - text: Amazon FSx for Lustre integrated with Amazon S3
       is_correct: true
     - text: Amazon Elastic File System (EFS) with Provisioned Throughput.
       is_correct: false
     - text: Amazon S3 with S3 Transfer Acceleration.
       is_correct: false
     - text: Amazon EBS Multi-Attach with Provisioned IOPS SSD (io2) volumes.
       is_correct: false
    explanation: |
      Correct: FSx for Lustre is specifically designed for HPC and offers native integration with S3, allowing you to process data in an ultra-fast file system and automatically persist results to S3.
      Incorrect:
        - EFS is a general-purpose file system; although scalable, it does not meet the extreme sub-millisecond performance demands of HPC Lustre workloads.
        - S3 alone is not a POSIX file system; Linux HPC applications usually need to mount a real file system.
        - EBS Multi-Attach allows only some instance types to access the volume and has strict scale limits (dozens of instances, not hundreds).

    diagram: |
      graph TD
        A[On-premises] --> B[S3]
        B --> C[FSx for Lustre]
        D[EC2 Spot] --> C
        C --> E[Resultados S3]

  - id: q163
    type: multiple_choice
    question: |
      A company is building a containerized application on premises and decides to move the application to AWS. The application will have thousands of users soon after it is deployed. The company is unsure how to manage the deployment of containers at scale. The company needs to deploy the containerized application in a highly available architecture that minimizes operational overhead.
      Which solution will meet these requirements?
    options:
     - text: Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.
       is_correct: true
     - text: Install and manage a Kubernetes cluster on Amazon EC2 instances.
       is_correct: false
     - text: Use AWS Elastic Beanstalk with the Docker platform to deploy the application.
       is_correct: false
     - text: Use Amazon Lightsail to run the containerized application.
       is_correct: false
    explanation: |
      Correct: ECS with Fargate removes the need to manage underlying EC2 instances (patching, cluster scaling), focusing only on container management, which minimizes overhead.
      Incorrect:
        - Managing Kubernetes on EC2 manually has the highest possible operational overhead.
        - Elastic Beanstalk adds abstraction layers that can be more complex to tune for thousands of users than native ECS.
        - Lightsail is for small and simple projects; it does not offer the same level of scalability and enterprise-grade high availability as ECS/Fargate.

    diagram: |
      graph TD
        A[Usuário] --> B[ALB]
        B --> C[ECS Fargate]
        C --> D[ECR]

  - id: q164
    type: multiple_choice
    question: |
      A company has two applications: a sender application that sends messages with payloads to be processed and a processing application intended to receive the messages with payloads. The company wants to implement an AWS service to handle messages between the two applications. The sender application can send about 1,000 messages each hour. The messages may take up to 2 days to be processed. If the messages fail to process, they must be retained so that they do not impact the processing of any remaining messages.
    options:
     - text: Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue (DLQ) to collect the messages that failed to process.
       is_correct: true
     - text: Use Amazon SNS to broadcast messages to the processor application.
       is_correct: false
     - text: Store the message payloads in an Amazon S3 bucket and use S3 Event Notifications.
       is_correct: false
     - text: Use Amazon Kinesis Data Streams to buffer the messages.
       is_correct: false
    explanation: |
      Correct: SQS provides the required decoupling. Support for message retention (up to 14 days) and Dead Letter Queue for failed messages ensures that processing continues for other messages without data loss.
      Incorrect:
        - SNS is a push model; if the processor is offline or fails, the message can be lost unless there is a complex retry system.
        - S3 is not a messaging system; managing the "processed" or "failed" state via files in S3 would have immense operational overhead.
        - Kinesis is for large-scale data ingestion (streaming); for 1,000 messages per hour, SQS is much simpler and more cost-effective.

    diagram: |
      graph TD
        A[Sender App] --> B[SQS Queue]
        B --> C[Processor App]
        B --> D[DLQ]

  - id: q165
    type: multiple_choice
    question: |
      A solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 origin to store a static website. The company�s security policy requires that all website traffic be inspected by AWS WAF.
    options:
     - text: Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the CloudFront distribution.
       is_correct: true
     - text: Enable AWS WAF directly on the Amazon S3 bucket.
       is_correct: false
     - text: Create a security group for the S3 bucket that only allows traffic from AWS WAF.
       is_correct: false
     - text: Use a Network Load Balancer in front of the S3 bucket to integrate with AWS WAF.
       is_correct: false
    explanation: |
      Correct: By using OAI/OAC, you ensure that S3 only accepts traffic coming from CloudFront. By associating WAF with CloudFront, all traffic is inspected before reaching the origin.
      Incorrect:
        - AWS WAF cannot be enabled directly on S3 buckets.
        - S3 buckets do not use Security Groups; they use Bucket Policies and ACLs.
        - NLBs do not natively integrate with S3 to serve static sites this way, and CloudFront is the recommended service for this function.

    diagram: |
      graph TD
        A[Usuário] --> B[CloudFront]
        B -->|WAF| C[S3 Bucket]

  - id: q166
    type: multiple_choice
    question: |
      Organizers for a global event want to put daily reports online as static HTML pages. The pages are expected to generate millions of views from users around the world. The files are stored in an Amazon S3 bucket. A solutions architect has been asked to design an efficient and effective solution.
      Which action should the solutions architect take to accomplish this?
    options:
     - text: Use Amazon CloudFront with the S3 bucket as its origin.
       is_correct: true
     - text: Enable S3 Cross-Region Replication to all AWS Regions.
       is_correct: false
     - text: Use an Amazon API Gateway in front of the S3 bucket.
       is_correct: false
     - text: Place an Application Load Balancer in front of the S3 bucket.
       is_correct: false
    explanation: |
      Correct: CloudFront is the standard solution for global scaling of static content, reducing S3 egress costs and drastically improving end-user speed.
      Incorrect:
        - CRR does not solve the end-user performance problem without a complex routing system and is much more expensive than CloudFront.
        - API Gateway is for REST/HTTP APIs, not the right tool to serve millions of static HTML pages directly from S3.
        - ALBs cannot use an S3 bucket directly as a "target group" to serve static files.

    diagram: |
      graph TD
        A[S3 HTML Pages] --> B[CloudFront]
        B --> C[Usuário Global]

  - id: q167
    type: multiple_choice
    question: |
      A company runs a production application on a fleet of Amazon EC2 instances. The application reads the data from an Amazon SQS queue and processes the messages in parallel. The message volume is unpredictable and often has intermittent traffic. This application should continually process messages without any downtime.
      Which solution meets these requirements MOST cost-effectively?
    options:
     - text: Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional capacity.
       is_correct: true
     - text: Use On-Demand instances for all capacity to ensure maximum uptime.
       is_correct: false
     - text: Use Spot Instances for all capacity to minimize costs.
       is_correct: false
     - text: Purchase Dedicated Hosts for the entire fleet.
       is_correct: false
    explanation: |
      Correct: Reserved Instances guarantee availability for the minimum constant load at the lowest price, while Spot Instances take advantage of up to 90% discount for unpredictable traffic fluctuations.
      Incorrect:
        - On-Demand is the most expensive option for constant load.
        - Using only Spot can cause downtime if AWS needs the capacity back, violating the "no downtime" requirement.
        - Dedicated Hosts are extremely expensive and used only for specific licensing or compliance requirements.

    diagram: |
      graph TD
        A[SQS] --> B[EC2 Reserved]
        A --> C[EC2 Spot]
        B --> D[App]
        C --> D

  - id: q168
    type: multiple_choice
    question: |
      A security team wants to limit access to specific services or actions in all of the team�s AWS accounts. All accounts belong to a large organization in AWS Organizations. The solution must be scalable and there must be a single point where permissions can be maintained.
    options:
     - text: Create a service control policy (SCP) in the root organizational unit to deny access to the services or actions.
       is_correct: true
     - text: Create a common IAM Group in each account and attach a managed policy.
       is_correct: false
     - text: Use AWS Shield Advanced to block access to specific services.
       is_correct: false
     - text: Set up a VPC Peering connection and use Network ACLs to block service endpoints.
       is_correct: false
    explanation: |
      Correct: SCPs allow you to set maximum permission limits across the organization in a centralized way. Even if a local administrator gives full permission to a user, the SCP will prevail and block the action.
      Incorrect:
        - IAM Groups in each account are not scalable (require management in each individual account) and do not prevent account administrators from creating new users with other permissions.
        - AWS Shield is for DDoS protection, not API permission control.
        - NACLs block network traffic but cannot control granular AWS service actions (like preventing the creation of an S3 bucket).

    diagram: |
      graph TD
        A[Root OU] -->|SCP| B[Account 1]
        A -->|SCP| C[Account 2]
        B --> D[Permissões restritas]
        C --> D

  - id: q169
    type: multiple_choice
    question: |
      A company is concerned about the security of its public web application due to recent web attacks. The application uses an Application Load Balancer (ALB). A solutions architect must reduce the risk of DDoS attacks against the application.
      What should the solutions architect do to meet this requirement?
    options:
     - text: Enable AWS Shield Advanced to provide enhanced protection and access to the DDoS Response Team (DRT).
       is_correct: true
     - text: Configure an Amazon Inspector agent on the EC2 instances.
       is_correct: false
     - text: Use Amazon GuardDuty to automatically block malicious IP addresses.
       is_correct: false
     - text: Deploy the application in a private subnet with a NAT Gateway.
       is_correct: false
    explanation: |
      Correct: Shield Advanced provides protection against volumetric DDoS attacks at layers 3, 4, and 7, as well as financial protection against cost spikes caused by attacks.
      Incorrect:
        - Amazon Inspector is for vulnerability analysis on instances, not for blocking DDoS attacks in real time.
        - GuardDuty detects threats but does not automatically block IPs without custom Lambda automations.
        - NAT Gateways do not protect a public application against DDoS; they only allow private instances to access the internet.

    diagram: |
      graph TD
        A[Usuário] --> B[ALB]
        B -->|Shield Advanced| C[App EC2]

  - id: q170
    type: multiple_choice
    question: |
      A company�s web application is running on Amazon EC2 instances behind an Application Load Balancer. The company recently changed its policy, which now requires the application to be accessed from one specific country only.
    options:
     - text: Configure AWS WAF on the Application Load Balancer and use a geo-match condition to allow traffic from the specific country.
       is_correct: true
     - text: Use Route 53 with a Latency routing policy.
       is_correct: false
     - text: Block all IP ranges manually using Network ACLs.
       is_correct: false
     - text: Use AWS Firewall Manager to block all ports except 80 and 443.
       is_correct: false
    explanation: |
      Correct: AWS WAF natively supports geographic filtering (Geo-match), allowing or blocking traffic based on the country of origin of the IP in a simple way.
      Incorrect:
        - Latency routing directs to the fastest region but does not prevent someone from another country from accessing the application.
        - Manually managing thousands of IP ranges for entire countries via NACLs is operationally impossible and error-prone.
        - Blocking ports does not solve the problem of filtering by geographic location.

    diagram: |
      graph TD
        A[Usuário] -->|Geo-match| B[WAF]
        B --> C[ALB]
        C --> D[App EC2]

  - id: q171
    type: multiple_choice
    question: |
      A company provides an API to its users that automates inquiries for tax computations based on item prices. The company experiences a larger number of inquiries during the holiday season only that cause slower response times. A solutions architect needs to design a solution that is scalable and elastic.
      What should the solutions architect do to accomplish this?
    options:
     - text: Design a REST API using Amazon API Gateway that passes requests to AWS Lambda for tax computations.
       is_correct: true
     - text: Deploy the API on a single large Amazon EC2 instance.
       is_correct: false
     - text: Use an Amazon SQS queue to buffer requests and an EC2 instance to process them.
       is_correct: false
     - text: Migrate the application to a legacy mainframe for high-performance processing.
       is_correct: false
    explanation: |
      Correct: API Gateway + Lambda is the definition of serverless scalability and elasticity. The system scales from zero to thousands of simultaneous executions instantly during holidays without manual intervention.
      Incorrect:
        - A single EC2 instance (even large) is a single point of failure and does not have the elasticity to handle extreme seasonal spikes.
        - SQS introduces latency (asynchronous), which may not be acceptable for a price query where the user expects an immediate (synchronous) response.
        - Mainframes are the opposite of modern, elastic cloud solutions.

    diagram: |
      graph TD
        A[Usuário] --> B[API Gateway]
        B --> C[Lambda]
        C --> D[Tax Computation]

  - id: q172
    type: multiple_choice
    question: |
      A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should be protected throughout the entire application stack, and access to the information should be restricted to certain applications.
      Which action should the solutions architect take?
    options:
     - text: Configure a CloudFront field-level encryption profile to encrypt sensitive data at the edge.
       is_correct: true
     - text: Use AWS KMS to encrypt the entire S3 bucket.
       is_correct: false
     - text: Enable S3 Versioning to keep track of changes to sensitive data.
       is_correct: false
     - text: Use a signed cookie to restrict access to the entire website.
       is_correct: false
    explanation: |
      Correct: Field-level encryption in CloudFront allows you to encrypt specific fields (such as card numbers) at the edge using a public key. Only applications with the corresponding private key can decrypt the data, ensuring end-to-end protection.
      Incorrect:
        - Encrypting the S3 bucket protects data at rest but does not guarantee that the data is encrypted throughout the stack (e.g., application logs).
        - Versioning is not a security or encryption measure for sensitive data.
        - Signed cookies control *who* accesses the file but do not protect the *content* if it is intercepted or logged.

    diagram: |
      graph TD
        A[Usuário] --> B[CloudFront]
        B -->|Field-level encryption| C[App Backend]

  - id: q173
    type: multiple_choice
    question: |
      A gaming company hosts a browser-based application on AWS. The users of the application consume a large number of videos and images that are stored in Amazon S3. This content is the same for all users.
      The application has increased in popularity, and millions of users worldwide accessing these media files. The company wants to provide the files to the users while reducing the load on the origin.
      Which solution meets these requirements MOST cost-effectively?
    options:
     - text: Deploy an Amazon CloudFront web distribution in front of the S3 bucket.
       is_correct: true
     - text: Increase the read capacity units (RCUs) on the S3 bucket.
       is_correct: false
     - text: Use S3 Transfer Acceleration for all user downloads.
       is_correct: false
     - text: Replicate the S3 bucket to 10 different regions.
       is_correct: false
    explanation: |
      Correct: CloudFront reduces the "Data Transfer Out" cost from S3 and eliminates repetitive load on the original bucket, serving images and videos directly from the edge cache.
      Incorrect:
        - S3 does not use the concept of "RCUs" (that's from DynamoDB). S3 scales automatically, but you pay for each GET request.
        - Transfer Acceleration is for fast uploads, not for mass download caching.
        - Multi-region replication is expensive and does not solve the origin load problem as efficiently as CDN caching.

    diagram: |
      graph TD
        A[S3 Vídeos/Imagens] --> B[CloudFront]
        B --> C[Usuário Global]

  - id: q174
    type: multiple_choice
    question: |
      A company has a multi-tier application that runs six front-end web servers in an Amazon EC2 Auto Scaling group in a single Availability Zone behind an Application Load Balancer (ALB). A solutions architect needs to modify the infrastructure to be highly available without modifying the application.
      Which architecture should the solutions architect choose that provides high availability?
    options:
     - text: Modify the Auto Scaling group to use three instances across each of two Availability Zones.
       is_correct: true
     - text: Increase the number of instances in the single Availability Zone to twelve.
       is_correct: false
     - text: Create a second ALB in a different region and use Route 53.
       is_correct: false
     - text: Move the instances to a Cluster Placement Group.
       is_correct: false
    explanation: |
      Correct: High availability (HA) requires geographic redundancy. By distributing instances across at least two AZs, the application survives the total failure of a datacenter (AZ).
      Incorrect:
        - Having twelve instances in the same AZ does not protect against the failure of that specific AZ.
        - A multi-region architecture is for "Disaster Recovery," not basic HA, and is much more complex and expensive.
        - Cluster Placement Groups place instances physically close for low network latency, which actually *increases* the risk of simultaneous failure.

    diagram: |
      graph TD
        A[ALB] --> B[AZ1: 3 EC2]
        A --> C[AZ2: 3 EC2]
        B --> D[Usuário]
        C --> D

  - id: q175
    type: multiple_choice
    question: |
      An ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS Lambda function. The application stores data in an Amazon Aurora PostgreSQL database. During a recent sales event, a sudden surge in customer orders occurred. Some customers experienced timeouts, and the application did not process the orders of those customers.
      A solutions architect determined that the CPU utilization and memory utilization were high on the database because of a large number of open connections. The solutions architect needs to prevent the timeout errors while making the least possible changes to the application.
      Which solution will meet these requirements?
    options:
     - text: Use Amazon RDS Proxy to create a proxy for the database and manage the connection pool.
       is_correct: true
     - text: Scale up the Aurora database instance to a larger size.
       is_correct: false
     - text: Implement a retry logic in the Lambda function.
       is_correct: false
     - text: Convert the Aurora database to a Multi-AZ deployment.
       is_correct: false
    explanation: |
      Correct: RDS Proxy manages a pool of database connections. Since Lambda functions open and close connections quickly, they can exhaust the database's memory; the Proxy allows connection reuse and protects the database from spikes.
      Incorrect:
        - Scaling the database (vertically) helps but does not solve the root cause of connection exhaustion as efficiently as the Proxy.
        - Retries in Lambda without solving the database problem can worsen the situation, creating a "connection storm."
        - Multi-AZ is for failover, not for managing the number of simultaneous connections on the primary instance.

    diagram: |
      graph TD
        A[Lambda] --> B[RDS Proxy]
        B --> C[Aurora DB]

  - id: q176
    type: multiple_choice
    question: |
      An application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon DynamoDB table.
      What is the MOST secure way to access the table while ensuring that the traffic does not leave the AWS network?
    options:
     - text: Create a Gateway VPC Endpoint for DynamoDB and update the route tables.
       is_correct: true
     - text: Use a NAT Gateway in a public subnet to route traffic to DynamoDB.
       is_correct: false
     - text: Set up an AWS Site-to-Site VPN to access the DynamoDB endpoint.
       is_correct: false
     - text: Assign a public IP address to each EC2 instance.
       is_correct: false
    explanation: |
      Correct: Gateway VPC Endpoints allow instances in private subnets to reach DynamoDB using AWS internal routes, without needing IGW, NAT, or traversing the public internet.
      Incorrect:
        - NAT Gateways send traffic over the internet to reach DynamoDB public endpoints.
        - VPNs are for connecting on-premises to AWS, not for internal VPC-to-service traffic.
        - Public IPs on private instances break security isolation and require an Internet Gateway.

    diagram: |
      graph TD
        A[EC2 Private] -->|VPC Endpoint| B[DynamoDB]

  - id: q177
    type: multiple_choice
    question: |
      An entertainment company is using Amazon DynamoDB to store media metadata. The application is read intensive and experiencing delays. The company does not have staff to handle additional operational overhead and needs to improve the performance efficiency of DynamoDB without reconfiguring the application.
      What should a solutions architect recommend to meet this requirement?
    options:
     - text: Use Amazon DynamoDB Accelerator (DAX) to provide in-memory caching.
       is_correct: true
     - text: Migrate the data to Amazon ElastiCache for Redis.
       is_correct: false
     - text: Increase the Provisioned Read Capacity Units (RCUs) on the table.
       is_correct: false
     - text: Use an Amazon CloudFront distribution in front of DynamoDB.
       is_correct: false
    explanation: |
      Correct: DAX is a fully managed in-memory cache compatible with the DynamoDB API. It improves response times from milliseconds to microseconds without requiring logical changes to the application.
      Incorrect:
        - Migrating to ElastiCache would require rewriting the application code to handle a different database engine (Redis).
        - Increasing RCUs helps throughput but does not reduce inherent disk latency like an in-memory cache does.
        - CloudFront is not used to cache direct NoSQL database queries.

    diagram: |
      graph TD
        A[App] --> B[DAX]
        B --> C[DynamoDB]

  - id: q178
    type: multiple_choice
    question: |
      A company�s infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance in a single AWS Region. The company wants to back up its data in a separate Region.
      Which solution will meet these requirements with the LEAST operational overhead?
    options:
     - text: Use AWS Backup to create a backup plan with cross-region copy enabled for both EC2 and RDS.
       is_correct: true
     - text: Write a custom script that takes snapshots and uses the AWS CLI to copy them to S3 in another region.
       is_correct: false
     - text: Enable RDS Multi-AZ replication to a different region.
       is_correct: false
     - text: Configure an AWS DataSync task to replicate EBS volumes and RDS snapshots.
       is_correct: false
    explanation: |
      Correct: AWS Backup is a managed service that allows you to centralize and automate backups of various AWS services, including automatic copying to other regions with just a few clicks.
      Incorrect:
        - Custom scripts increase the operational overhead of maintenance and monitoring.
        - RDS Multi-AZ is intra-region (between AZs); for different regions, it would be Read Replicas, which is not a simple backup.
        - DataSync is for file and data migration from file systems, not the native tool for managing instance and database snapshots.

    diagram: |
      graph TD
        A[EC2/RDS] --> B[AWS Backup]
        B -->|Cross-region| C[Backup em outra região]

  - id: q179
    type: multiple_choice
    question: |
      A solutions architect needs to securely store a database user name and password that an application uses to access an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter Store.
      What should the solutions architect do to meet this requirement?
    options:
     - text: Create an IAM role with read access to the Parameter Store. Allow Decrypt access to the KMS key used. Assign this role to the EC2 instance profile.
       is_correct: true
     - text: Store the password in the EC2 User Data script as a plain text variable.
       is_correct: false
     - text: Hardcode the credentials in the application's configuration file.
       is_correct: false
     - text: Create a public S3 bucket and store the credentials in a JSON file.
       is_correct: false
    explanation: |
      Correct: Using Parameter Store (SecureString) with IAM roles and KMS keys is the recommended practice to avoid storing secrets in code or plain text.
      Incorrect:
        - User Data and hardcoding are serious security vulnerabilities.
        - Public S3 buckets should never be used to store any sensitive data.

    diagram: |
      graph TD
        A[EC2] -->|IAM Role| B[Parameter Store]
        B -->|KMS| C[Credenciais RDS]

  - id: q180
    type: multiple_choice
    question: |
      A company is designing a cloud communications platform that is driven by APIs. The application is hosted on Amazon EC2 instances behind a Network Load Balancer (NLB). The company uses Amazon API Gateway to provide external users with access to the application through APIs. The company wants to protect the platform against web exploits like SQL injection and also wants to detect and mitigate large, sophisticated DDoS attacks.
      Which combination of solutions provides the MOST protection? (Choose two.)
    options:
     - text: Use AWS Shield Advanced with the NLB.
       is_correct: true
     - text: Use AWS WAF to protect Amazon API Gateway.
       is_correct: true
     - text: Implement a VPC Peering connection with a security partner.
       is_correct: false
     - text: Use Amazon Inspector to block SQL injection attacks in real time.
       is_correct: false
     - text: Configure a NAT Gateway to hide the EC2 instances' IP addresses.
       is_correct: false
    explanation: |
      Correct: Shield Advanced protects against DDoS at the NLB level. AWS WAF integrates with API Gateway to filter layer 7 attacks, such as SQL Injection.
      Incorrect:
        - Amazon Inspector is not a firewall; it only scans for software vulnerabilities.
        - NAT Gateways do not provide protection against inbound attacks on the API.

    diagram: |
      graph TD
        A[Usuário] --> B[API Gateway]
        B -->|WAF| C[NLB]
        C -->|Shield Advanced| D[EC2]

  - id: q181
    type: multiple_choice
    question: |
      A company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed sequentially, but the order of results does not matter. The application uses a monolithic architecture. The only way that the company can scale the application to meet increased demand is to increase the size of the instances.
      The company�s developers have decided to rewrite the application to use a microservices architecture on Amazon Elastic Container Service (Amazon ECS).
      What should a solutions architect recommend for communication between the microservices?
    options:
     - text: Create an Amazon Simple Queue Service (Amazon SQS) queue to decouple the producer and consumer services.
       is_correct: true
     - text: Use Amazon EBS Multi-Attach to share data between containers.
       is_correct: false
     - text: Implement a direct TCP connection between the container IP addresses.
       is_correct: false
     - text: Use an Amazon Route 53 private hosted zone for each service.
       is_correct: false
    explanation: |
      Correct: SQS queues allow microservices to communicate asynchronously and independently, allowing each part of the application to scale as needed without data loss.
      Incorrect:
        - EBS Multi-Attach is not for microservice communication and has performance and instance type limitations.
        - Direct TCP connections create strong coupling; if one service fails, the other fails immediately, which negates the benefit of microservices architecture.

    diagram: |
      graph TD
        A[Producer] --> B[SQS]
        B --> C[Consumer]

  - id: q182
    type: multiple_choice
    question: |
      A company wants to migrate its MySQL database from on premises to AWS. The company recently experienced a database outage that significantly impacted the business. To ensure this does not happen again, the company wants a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes.
      Which solution meets these requirements?
    options:
     - text: Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled.
       is_correct: true
     - text: Deploy a single RDS MySQL instance and take snapshots every hour.
       is_correct: false
     - text: Use an EC2 instance with an EBS Cold HDD volume for storage.
       is_correct: false
     - text: Create a Read Replica in the same availability zone.
       is_correct: false
    explanation: |
      Correct: RDS Multi-AZ synchronously replicates data to a standby instance in another AZ, ensuring high availability and durability with no data loss in case the primary fails.
      Incorrect:
        - Hourly snapshots can result in up to 60 minutes of data loss.
        - Simple EC2 instances do not offer automatic database-level replication.
        - Read Replicas in the same AZ do not protect against datacenter (AZ) failure.

    diagram: |
      graph TD
        A[App] --> B[RDS MySQL]
        B -->|Sync Replication| C[Standby em outra AZ]

  - id: q183
    type: multiple_choice
    question: |
      A company is building a new dynamic ordering website. The company wants to minimize server maintenance and patching. The website must be highly available and must scale read and write capacity as quickly as possible to meet changes in user demand.
      Which solution will meet these requirements?
    options:
     - text: Host static content in Amazon S3. Host dynamic content using API Gateway and AWS Lambda. Use Amazon DynamoDB with on-demand capacity. Configure Amazon CloudFront.
       is_correct: true
     - text: Use an EC2 Auto Scaling group with an Application Load Balancer and RDS MySQL.
       is_correct: false
     - text: Deploy the website on AWS Elastic Beanstalk with an EFS file system.
       is_correct: false
     - text: Use Amazon Lightsail with a managed database.
       is_correct: false
    explanation: |
      Correct: This is a complete "Serverless Stack." S3/Lambda/DynamoDB eliminate server and patch management, while DynamoDB's on-demand mode scales instantly.
      Incorrect:
        - EC2 requires OS and patch management.
        - RDS MySQL does not scale writes as quickly as DynamoDB On-Demand.
        - Lightsail is not designed for massive and dynamic enterprise-scale.

    diagram: |
      graph TD
        A[Usuário] --> B[CloudFront]
        B --> C[S3 Static]
        B --> D[API Gateway]
        D --> E[Lambda]
        E --> F[DynamoDB]

  - id: q184
    type: multiple_choice
    question: |
      A company has an AWS account used for software engineering. The AWS account has access to the company�s on-premises data center through a pair of AWS Direct Connect connections. All non-VPC traffic routes to the virtual private gateway.
      A development team recently created an AWS Lambda function through the console. The development team needs to allow the function to access a database that runs in a private subnet in the company�s data center.
      Which solution will meet these requirements?
    options:
     - text: Configure the Lambda function to run in the VPC with the appropriate subnets and security group.
       is_correct: true
     - text: Create a public endpoint for the on-premises database.
       is_correct: false
     - text: Use AWS AppSync to connect the Lambda to the data center.
       is_correct: false
     - text: Assign an Elastic IP to the Lambda function.
       is_correct: false
    explanation: |
      Correct: By configuring Lambda to run inside the VPC, it gains access to the VPC routes, including the Direct Connect route to the on-premises datacenter.
      Incorrect:
        - Exposing a private database on the public internet is an unacceptable security risk.
        - AppSync is for GraphQL APIs, not for basic network connectivity between functions and private databases.

    diagram: |
      graph TD
        A[Lambda] -->|VPC| B[Direct Connect]
        B --> C[DB On-premises]

  - id: q185
    type: multiple_choice
    question: |
      A company runs an application using Amazon ECS. The application creates resized versions of an original image and then makes Amazon S3 API calls to store the resized images in Amazon S3.
      How can a solutions architect ensure that the application has permission to access Amazon S3?
    options:
     - text: Create an IAM role with S3 permissions and specify it as the taskRoleArn in the ECS task definition.
       is_correct: true
     - text: Store AWS Access Keys inside the container image.
       is_correct: false
     - text: Assign the S3 permission to the EC2 Instance Role where the ECS cluster is running.
       is_correct: false
     - text: Use a bucket policy that allows access to all IP addresses in the VPC.
       is_correct: false
    explanation: |
      Correct: The "Task Role" is the recommended way to give granular permissions to containers in ECS, following the principle of least privilege for each individual task.
      Incorrect:
        - Embedding access keys in container images is a bad security practice and difficult to rotate.
        - The "Instance Role" would give permission to *all* containers running on that machine, which violates privilege isolation.

    diagram: |
      graph TD
        A[ECS Task] -->|Task Role| B[S3 Bucket]

  - id: q186
    type: multiple_choice
    question: |
      A company has a Windows-based application that must be migrated to AWS. The application requires the use of a shared Windows file system attached to multiple Amazon EC2 Windows instances that are deployed across multiple Availability Zones.
      What should a solutions architect do to meet this requirement?
    options:
     - text: Configure Amazon FSx for Windows File Server in Multi-AZ mode.
       is_correct: true
     - text: Use Amazon EFS and mount it using the NFS client on Windows.
       is_correct: false
     - text: Create an Amazon S3 bucket and use a third-party tool to mount it as a drive.
       is_correct: false
     - text: Use an EBS Multi-Attach Provisioned IOPS volume.
       is_correct: false
    explanation: |
      Correct: FSx for Windows File Server provides a native and fully managed SMB file system that natively supports Windows instances across multiple AZs.
      Incorrect:
        - EFS is focused on Linux/NFS; support for Windows via NFS is limited and not performant for native Windows applications.
        - S3 is not a real file system and does not support file locking required by many Windows applications.

    diagram: |
      graph TD
        A[EC2 Windows] --> B[FSx Windows File Server]
        B --> C[EC2 Windows em outra AZ]

  - id: q187
    type: multiple_choice
    question: |
      A company is developing an ecommerce application that will consist of a load-balanced front end, a container-based application, and a relational database. A solutions architect needs to create a highly available solution that operates with as little manual intervention as possible.
      Which solutions meet these requirements? (Choose two.)
    options:
     - text: Create an Amazon RDS DB instance in Multi-AZ mode.
       is_correct: true
     - text: Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type.
       is_correct: true
     - text: Use an Amazon EC2 Auto Scaling group with a single instance.
       is_correct: false
     - text: Set up a manual failover process using Route 53.
       is_correct: false
     - text: Host the database on an EC2 instance with daily snapshots.
       is_correct: false
    explanation: |
      Correct: RDS Multi-AZ and ECS Fargate provide high availability and automated scaling/failover, eliminating almost all manual intervention in the infrastructure.
      Incorrect:
        - A single instance in an ASG is not "highly available."
        - Manual failover is the opposite of the "minimal manual intervention" requirement.

    diagram: |
      graph TD
        A[Usuário] --> B[ALB]
        B --> C[ECS Fargate]
        C --> D[RDS Multi-AZ]

  - id: q188
    type: multiple_choice
    question: |
      A company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data files. A solutions architect needs to implement a highly available SFTP solution that minimizes operational overhead.
      Which solution will meet these requirements?
    options:
     - text: Use AWS Transfer Family to configure an SFTP-enabled server with Amazon S3 as the storage backend.
       is_correct: true
     - text: Deploy an EC2 instance running a standard SFTP server and mount S3 using S3FS.
       is_correct: false
     - text: Use Amazon API Gateway to receive file uploads via HTTP.
       is_correct: false
     - text: Set up an AWS Storage Gateway File Gateway.
       is_correct: false
    explanation: |
      Correct: AWS Transfer Family is a fully managed service for SFTP/FTPS/FTP that scales automatically and integrates directly with S3 without the need to manage servers.
      Incorrect:
        - Managing SFTP servers on EC2 significantly increases operational overhead and high availability complexity.
        - The partner specifically requires the use of the SFTP protocol, not HTTP/API.

    diagram: |
      graph TD
        A[Parceiro] -->|SFTP| B[AWS Transfer Family]
        B --> C[S3 Bucket]

  - id: q189
    type: multiple_choice
    question: |
      A company needs to store contract documents. A contract lasts for 5 years. During the 5-year period, the company must ensure that the documents cannot be overwritten or deleted. The company needs to encrypt the documents at rest and rotate the encryption keys automatically every year.
      Which combination of steps should a solutions architect take to meet these requirements with the LEAST operational overhead? (Choose two.)
    options:
     - text: Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.
       is_correct: true
     - text: Use server-side encryption with AWS Key Management Service (AWS KMS) and enable automatic key rotation.
       is_correct: false
     - text: Use a customer-managed key in AWS KMS and configure annual manual rotation.
       is_correct: false
     - text: Enable MFA Delete on the S3 bucket.
       is_correct: false
     - text: Set up an IAM policy to prevent all deletions.
       is_correct: false
    explanation: |
      Correct: S3 Object Lock in Compliance Mode guarantees immutability for 5 years. KMS with automatic rotation meets the encryption requirement with minimal effort.
      Incorrect:
        - MFA Delete prevents accidental deletion, but an administrator can still delete files if they have the MFA device, which does not guarantee the required immutability.
        - Manual key rotation increases operational overhead.

    diagram: |
      graph TD
        A[Usuário] --> B[S3 Bucket]
        B --Object Lock--> C[5 anos]
        B --KMS--> D[Chave rotacionada]

  - id: q190
    type: multiple_choice
    question: |
      A company has a web application that is based on Java and PHP. The company plans to move the application from on premises to AWS. The company needs the ability to test new site features frequently. The company also needs a highly available and managed solution that requires minimum operational overhead.
      Which solution will meet these requirements?
    options:
     - text: Deploy the web application to an AWS Elastic Beanstalk environment. Use Blue/Green deployments for feature testing.
       is_correct: true
     - text: Use AWS CloudFormation to manually provision EC2 instances for every test.
       is_correct: false
     - text: Deploy the application on Amazon Lightsail.
       is_correct: false
     - text: Migrate the application to AWS Lambda using a custom runtime for PHP and Java.
       is_correct: false
    explanation: |
      Correct: Elastic Beanstalk is ideal for traditional Java/PHP applications and natively supports deployment strategies like Blue/Green, making frequent testing easy with low overhead.
      Incorrect:
        - CloudFormation is powerful, but managing code deployments and tests manually in it generates much more overhead compared to Beanstalk.
        - Lambda for a monolithic Java/PHP web application would require massive refactoring, violating the "simple migration" requirement.

    diagram: |
      graph TD
        A[Usuário] --> B[Elastic Beanstalk]
        B -->|Blue/Green| C[App Java/PHP]
 