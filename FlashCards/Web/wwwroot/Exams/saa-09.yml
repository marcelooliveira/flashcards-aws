questions:
  - id: q401
    type: multiple_choice
    question: |
      A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version resides in a data center and recently experienced data loss after a database server crashed.
      The company needs a solution that avoids any single points of failure and can scale to meet demand.
      Which solution will meet these requirements?
    options:
      - text: Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.
        is_correct: true
      - text: Deploy the application on a single large EC2 instance with an RDS DB instance in a Single-AZ configuration.
        is_correct: false
      - text: Use an EC2 instance in one AZ and a secondary EC2 instance in another AZ with manual failover.
        is_correct: false
      - text: Use a Network Load Balancer with EC2 instances in a single private subnet.
        is_correct: false
    explanation: |
      Correct: Multi-AZ for RDS provides synchronous replication and automatic failover, preventing data loss from a single server crash. Auto Scaling across multiple AZs ensures the application tier remains available and scales with demand.
      Incorrect:
        - Single-AZ RDS is a single point of failure (SPOF).
        - Manual failover increases downtime and operational overhead.
        - A single private subnet/AZ is an SPOF for the application tier.
    diagram: |
      graph TD
        A[User] -->|Load Balancer| B[ASG EC2 Instances]
        B -->|Multi-AZ| C[RDS Multi-AZ]

  - id: q402
    type: multiple_choice
    question: |
      An application sends large amounts of streaming data to Amazon Kinesis Data Streams (default settings). Data is consumed every other day and written to S3. The company observes that S3 is not receiving all the data sent to Kinesis.
      What should a solutions architect do to resolve this issue?
    options:
      - text: Increase the data retention period of the Kinesis data stream.
        is_correct: true
      - text: Increase the number of shards in the Kinesis data stream.
        is_correct: false
      - text: Enable enhanced fan-out for the Kinesis data consumers.
        is_correct: false
      - text: Use Kinesis Data Firehose to write the data directly to Amazon S3.
        is_correct: false
    explanation: |
      Correct: By default, Kinesis Data Streams retains data for 24 hours. Since the application only consumes data "every other day" (48 hours), data is being expired and deleted before it is read. Increasing retention (up to 365 days) solves this.
      Incorrect:
        - Increasing shards helps with throughput capacity, but the problem here is data loss over time, not throughput limits.
        - Firehose is a valid alternative, but the prompt asks how to resolve the current issue with the existing stream configuration.
    diagram: |
      graph TD
        A[Application] -->|Stream Data| B[Kinesis Data Streams]
        B -->|Consume Every Other Day| C[S3]
        B -->|Increase Retention| D[Resolve Data Loss]

  - id: q403
    type: multiple_choice
    question: |
      A developer has an application using a Lambda function to upload files to S3. They need the required permissions.
      What should a solutions architect do to grant the permissions?
    options:
      - text: Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.
        is_correct: true
      - text: Create an IAM user and store the credentials in the Lambda function's environment variables.
        is_correct: false
      - text: Grant the Lambda function access via an S3 Bucket Policy using the function's name as the principal.
        is_correct: false
      - text: Use the IAM user's access keys within the Lambda code to authenticate to S3.
        is_correct: false
    explanation: |
      Correct: IAM roles are the secure way to grant permissions to AWS services like Lambda. It avoids hardcoding credentials and uses temporary security tokens.
      Incorrect:
        - Storing IAM user credentials in environment variables or code is a security risk and violates best practices.
        - Lambda functions themselves are not principals; the execution role assigned to them is.
    diagram: |
      graph TD
        A[Lambda Function] -->|Execution Role| B[IAM Role]
        B -->|Permissions| C[S3 Upload]

  - id: q404
    type: multiple_choice
    question: |
      A serverless app invokes Lambda when documents are uploaded to S3. After a marketing campaign, many documents were not processed.
      What should a solutions architect do to improve the architecture?
    options:
      - text: Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda.
        is_correct: true
      - text: Increase the memory and timeout of the Lambda function.
        is_correct: false
      - text: Increase the concurrency limit of the Lambda function.
        is_correct: false
      - text: Use an S3 batch operation to process the documents.
        is_correct: false
    explanation: |
      Correct: Adding SQS acts as a buffer. If Lambda hits concurrency limits during a traffic spike, the messages stay in the queue until they can be processed, ensuring no data is lost.
      Incorrect:
        - Increasing memory/timeout doesn't help if the volume of triggers exceeds the concurrency limits.
        - Increasing concurrency might help but doesn't provide the durability and "throttling" protection that a queue provides.
    diagram: |
      graph TD
        A[S3 Upload] -->|Event| B[SQS Queue]
        B -->|Trigger| C[Lambda Function]

  - id: q405
    type: multiple_choice
    question: |
      A serverless app runs on EC2 instances in an ASG behind an ALB. Traffic spikes during working hours but the system is not needed on weekends.
      Which combination of actions ensures scaling and cost-efficiency? (Choose two.)
    options:
     - text: Use target tracking scaling to adjust the ASG capacity based on ALB request rate.
       is_correct: true
     - text: Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends.
       is_correct: true
     - text: Use AWS Step Functions to stop the ALB on Friday night.
       is_correct: false
     - text: Use Spot Instances for all nodes in the Auto Scaling group.
       is_correct: false
     - text: Configure the ALB to use a Fixed Response on weekends.
       is_correct: false
    explanation: |
      Correct: Target tracking ensures the fleet grows/shrinks with traffic during the week. Scheduled scaling to zero on weekends eliminates instance costs when the environment isn't needed.
      Incorrect:
        - You cannot "stop" an ALB to save costs; you must delete it or scale the backend to zero (the latter is more efficient for ASG management).
        - Spot instances might be interrupted during a demo, which is risky for this use case.
    diagram: |
      graph TD
        A[ALB] -->|Target Tracking| B[ASG Scaling]
        C[Scheduled Scaling] -->|Zero on Weekends| B

  - id: q406
    type: multiple_choice
    question: |
      A two-tier architecture: public subnet (web servers on port 443) and database subnet (RDS MySQL on port 3306). RDS must only be accessible by the web servers.
      Which steps should be taken? (Choose two.)
    options:
     - text: Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.
       is_correct: true
     - text: Create a security group for the DB instance. Add a rule to allow traffic from the web servers’ security group on port 3306.
       is_correct: true
     - text: Add a rule to the DB security group to allow 0.0.0.0/0 on port 3306.
       is_correct: false
     - text: Use a Network ACL on the database subnet to block all traffic except from the public subnet.
       is_correct: false
     - text: Create a peering connection between the public and database subnets.
       is_correct: false
    explanation: |
      Correct: The web SG allows public traffic. The DB SG uses "Security Group Referencing" to allow only the specific web server group to connect, which is the most secure method.
      Incorrect:
        - 0.0.0.0/0 on port 3306 exposes the database to the internet.
        - Peering is for VPCs, not subnets within the same VPC.
    diagram: |
      graph TD
        A[Public Subnet] -->|443| B[Web SG]
        B -->|3306| C[DB SG]

  - id: q407
    type: multiple_choice
    question: |
      A company needs a shared storage solution for a gaming application that requires Lustre clients. The solution must be fully managed.
      Which solution meets these requirements?
    options:
     - text: Create an Amazon FSx for Lustre file system.
       is_correct: true
     - text: Use Amazon EFS with the Lustre mount helper.
       is_correct: false
     - text: Deploy a Lustre cluster on EC2 instances using EBS.
       is_correct: false
     - text: Use Amazon S3 and mount it as a drive using S3FS.
       is_correct: false
    explanation: |
      Correct: FSx for Lustre is the only fully managed AWS service that provides a Lustre-compatible file system for high-performance workloads.
      Incorrect:
        - EFS is NFS-based, not Lustre.
        - Deploying on EC2 is self-managed, not "fully managed".
    diagram: |
      graph TD
        A[Gaming Application] -->|Lustre Clients| B[FSx for Lustre]

  - id: q408
    type: multiple_choice
    question: |
      An application receives UDP data from thousands of remote devices, processes it immediately (no storage), and sends a response. It needs minimal latency and rapid failover to another Region.
      Which solution meets these requirements?
    options:
     - text: Use AWS Global Accelerator. Create an NLB in each of the two Regions as an endpoint. Use Amazon ECS on Fargate as the target for the NLB.
       is_correct: true
     - text: Use Route 53 with a Latency routing policy pointing to an Application Load Balancer (ALB).
       is_correct: false
     - text: Use Amazon CloudFront with a Lambda@Edge function to process the UDP packets.
       is_correct: false
     - text: Use Amazon API Gateway with a regional endpoint and a Lambda function.
       is_correct: false
    explanation: |
      Correct: Global Accelerator provides low-latency routing and near-instant failover between regions. NLB and Fargate handle the UDP traffic and processing without managing servers.
      Incorrect:
        - ALB does not support UDP.
        - CloudFront/Lambda@Edge does not support UDP.
        - API Gateway is for REST/HTTP, not raw UDP.
    diagram: |
      graph TD
        A[UDP Devices] -->|Global Accelerator| B[NLB in Region 1]
        A -->|Failover| C[NLB in Region 2]
        B -->|ECS Fargate| D[Processing]

  - id: q409
    type: multiple_choice
    question: |
      An IIS app needs a replacement for an on-premises file share. It must be resilient, durable, and shareable across multiple AZs.
      Which replacement is MOST resilient and durable?
    options:
     - text: Migrate the file share to Amazon FSx for Windows File Server.
       is_correct: true
     - text: Use an Amazon EBS volume with Multi-Attach enabled.
       is_correct: false
     - text: Use Amazon S3 and map it as a network drive.
       is_correct: false
     - text: Deploy a Windows File Server on a single EC2 instance.
       is_correct: false
    explanation: |
      Correct: FSx for Windows File Server supports Multi-AZ deployments, providing a managed, resilient, and durable SMB share native to Windows.
      Incorrect:
        - EBS Multi-Attach is limited to a single AZ and requires specific cluster-aware file systems.
        - Mapping S3 as a drive for an IIS application is neither standard nor highly resilient/performant for file-level operations.
    diagram: |
      graph TD
        A[IIS App] -->|SMB| B[FSx for Windows File Server]
        B -->|Multi-AZ| C[High Availability]

  - id: q410
    type: multiple_choice
    question: |
      A company needs to ensure all data written to EBS volumes is encrypted at rest.
      Which solution will meet this requirement?
    options:
     - text: Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.
       is_correct: true
     - text: Encrypt the data at the application layer before writing it to the volume.
       is_correct: false
     - text: Use an IAM policy to enforce encryption for all S3 buckets.
       is_correct: false
     - text: Use a lifecycle policy to encrypt the data after 24 hours.
       is_correct: false
    explanation: |
      Correct: Creating the volume with encryption enabled ensures that all data, snapshots, and I/O between the instance and volume are encrypted at rest using AWS KMS.
      Incorrect:
        - S3 policies do not affect EBS volumes.
        - Application-layer encryption is possible but has higher overhead and is more complex than managed EBS encryption.
    diagram: |
      graph TD
        A[EC2 Instance] -->|Encrypted| B[EBS Volume]
        B -->|KMS Key| C[At Rest Encryption]

  - id: q411
    type: multiple_choice
    question: |
      A MySQL application has sporadic usage (heavy start of month, moderate start of week, unpredictable otherwise). The company wants a cost-effective platform with no database modifications.
      Which solution meets these requirements?
    options:
     - text: MySQL-compatible Amazon Aurora Serverless.
       is_correct: true
     - text: Amazon RDS for MySQL with Multi-AZ.
       is_correct: false
     - text: Amazon DynamoDB.
       is_correct: false
     - text: Amazon Redshift.
       is_correct: false
    explanation: |
      Correct: Aurora Serverless automatically scales capacity up and down based on demand and can even pause during periods of zero activity, making it ideal for sporadic workloads. It is MySQL-compatible.
      Incorrect:
        - RDS with Multi-AZ is always "on" and costs the same regardless of sporadic usage.
        - DynamoDB is NoSQL and would require significant application modifications.
    diagram: |
      graph TD
        A[Application] -->|Sporadic Usage| B[Aurora Serverless]
        B -->|Auto Scale| C[Cost Effective]

  - id: q412
    type: multiple_choice
    question: |
      A company uses AWS Lake Formation. The data science team wants to securely share selective data from its accounts with the engineering team for analytical purposes with the LEAST operational overhead.
      Which solution meets these requirements?
    options:
     - text: Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data.
       is_correct: true
     - text: Create IAM roles in each account and use STS for cross-account access.
       is_correct: false
     - text: Use S3 Bucket Policies to grant access to the engineering team's IAM users.
       is_correct: false
     - text: Copy the selective data into a central S3 bucket and manage access via IAM.
       is_correct: false
    explanation: |
      Correct: Lake Formation tag-based access control (LF-TBAC) simplifies sharing data at scale. You define tags (e.g., "Project:Engineering") and grant access based on those tags, avoiding the need for hundreds of resource policies or data copies.
      Incorrect:
        - Copying data creates redundancy and increases storage costs.
        - IAM and bucket policies become too complex in a petabyte-scale, multi-account environment.
    diagram: |
      graph TD
        A[Data Science Account] -->|LF-TBAC| B[Engineering Account]
        B -->|Access| C[Selective Data]

  - id: q413
    type: multiple_choice
    question: |
      An ecommerce site has delays sending marketing/order emails. They want to reduce operational overhead and complexity of email delivery.
      What should a solutions architect do?
    options:
     - text: Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).
       is_correct: true
     - text: Install a Postfix mail server on a new EC2 instance.
       is_correct: false
     - text: Use Amazon SNS to send the emails.
       is_correct: false
     - text: Use Amazon SQS to trigger a Lambda function that calls a third-party SMTP provider.
       is_correct: false
    explanation: |
      Correct: Amazon SES is a managed service for sending bulk and transactional emails, handling delivery issues and reputation management automatically.
      Incorrect:
        - Managing your own Postfix server increases operational overhead.
        - SNS is for pub/sub notifications; while it can send emails, it lacks the professional formatting and delivery features of SES.
    diagram: |
      graph TD
        A[Web Instance] -->|Send Email| B[Amazon SES]

  - id: q415
    type: multiple_choice
    question: |
      A company has petabytes of data in S3 with varying and unknown access patterns. They need to optimize costs for each bucket with the most operational efficiency.
      Which solution meets these requirements?
    options:
     - text: Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.
       is_correct: true
     - text: Manually analyze access logs and move objects to S3 Standard-IA.
       is_correct: false
     - text: Move all data to S3 Glacier Deep Archive.
       is_correct: false
     - text: Use S3 Inventory to identify rarely accessed objects and delete them.
       is_correct: false
    explanation: |
      Correct: S3 Intelligent-Tiering is designed for data with unknown or changing access patterns. It automatically moves data between tiers based on use, maximizing efficiency without manual analysis.
      Incorrect:
        - Manual analysis is not efficient for petabytes of data across multiple buckets.
        - Moving all data to Glacier makes it inaccessible for immediate use.
    diagram: |
      graph TD
        A[S3 Bucket] -->|Lifecycle| B[S3 Intelligent-Tiering]

  - id: q416
    type: multiple_choice
    question: |
      A global ecommerce site has slow page loads for static and dynamic content. They use RDS for OLTP data.
      Which combination of actions resolves this? (Choose two.)
    options:
     - text: Set up an Amazon CloudFront distribution.
       is_correct: true
     - text: Create a read replica for the RDS DB instance.
       is_correct: true
     - text: Migrate the database to Amazon Redshift.
       is_correct: false
     - text: Use AWS Global Accelerator for the static content.
       is_correct: false
     - text: Increase the instance size of the web servers.
       is_correct: false
    explanation: |
      Correct: CloudFront (CDN) speeds up static and dynamic content delivery by caching at the edge. RDS Read Replicas offload read traffic from the primary DB, speeding up dynamic data retrieval.
      Incorrect:
        - Redshift is for analytics, not OLTP.
        - Global Accelerator is typically for non-HTTP protocols or specific failover needs; CloudFront is better for web content.
    diagram: |
      graph TD
        A[Global Users] -->|CloudFront| B[Static Content]
        A -->|Read Replica| C[Dynamic Content]

  - id: q418
    type: multiple_choice
    question: |
      Users in a Dev account need access to an S3 bucket in a Prod account. The Prod account has an IAM role with appropriate bucket permissions.
      How can this be achieved following least privilege?
    options:
     - text: Add the development account as a principal in the trust policy of the role in the production account.
       is_correct: true
     - text: Give the Dev users the production account's root password.
       is_correct: false
     - text: Create identical IAM users in the production account.
       is_correct: false
     - text: Use a Cross-Region Replication rule to copy the bucket to the Dev account.
       is_correct: false
    explanation: |
      Correct: Trusting the Dev account as a principal allows the Dev users to "AssumeRole" into the Prod account role, gaining temporary, secure access to the production bucket.
      Incorrect:
        - Sharing root passwords is a major security violation.
        - Creating identical users is high overhead and breaks centralized identity management.
    diagram: |
      graph TD
        A[Dev Account] -->|AssumeRole| B[Prod Account IAM Role]
        B -->|Access| C[S3 Bucket]

  - id: q419
    type: multiple_choice
    question: |
      A company wants to ensure all new EBS volumes in a specific region (ap-southeast-2) are encrypted. They want a solution with minimal effect on employees.
      Which combination of steps meets these requirements? (Choose two.)
    options:
     - text: Create an SCP to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.
       is_correct: true
     - text: Enable the "Always encrypt new EBS volumes" regional setting in ap-southeast-2.
       is_correct: true
     - text: Manually encrypt volumes after they are created using a Lambda function.
       is_correct: false
     - text: Use AWS Shield to monitor unencrypted volumes.
       is_correct: false
     - text: Delete any EC2 instances launched with unencrypted volumes.
       is_correct: false
    explanation: |
      Correct: Enabling the regional EBS encryption setting is the best "minimal effect" step because it happens automatically. The SCP provides the enforcement (guardrail) to ensure the rule cannot be bypassed.
      Incorrect:
        - Deleting instances or manual remediation causes more disruption to employees.
    diagram: |
      graph TD
        A[Regional Setting] -->|Encrypt New Volumes| B[EBS Volumes]
        C[SCP] -->|Enforce| B

  - id: q420
    type: multiple_choice
    question: |
      A company needs an RDS PostgreSQL database that is highly available, has failover in <40 seconds, and can offload reads to keep costs low.
      Which solution meets these requirements?
    options:
     - text: Use an Amazon RDS Multi-AZ DB cluster deployment. Point the read workload to the reader endpoint.
       is_correct: true
     - text: Use a Single-AZ RDS instance with manual snapshots.
       is_correct: false
     - text: Use a Multi-AZ DB instance deployment with a separate Read Replica.
       is_correct: false
     - text: Use Amazon Aurora with a single instance.
       is_correct: false
    explanation: |
      Correct: A Multi-AZ DB **cluster** (not just "instance") provides one writer and two readers. It supports failover typically faster than a Multi-AZ instance (<35-40s) and allows the readers to be used for traffic, making it cost-effective compared to adding separate replicas.
      Incorrect:
        - Multi-AZ **instance** failover is usually 60-120 seconds and the standby cannot be read from.
        - Single instance Aurora has no high availability.
    diagram: |
      graph TD
        A[Application] -->|Writes| B[Writer Instance]
        A -->|Reads| C[Reader Endpoint]
        C -->|Offload| D[Reader Instances]

  - id: q421
    type: multiple_choice
    question: |
      A company runs a highly available SFTP service using two EC2 Linux instances with Elastic IPs and shared storage. User accounts are managed as Linux users. 
      The company wants a serverless option with high IOPS, highly configurable security, and control over user permissions.
      Which solution meets these requirements?
    options:
     - text: Create an encrypted Amazon EFS volume. Create an AWS Transfer Family SFTP service with Elastic IPs and a VPC endpoint. Attach the EFS volume to the SFTP service endpoint.
       is_correct: true
     - text: Use Amazon S3 with AWS Transfer Family. Use S3 Bucket Policies to manage Linux-style permissions.
       is_correct: false
     - text: Use an Amazon EBS Provisioned IOPS volume shared across multiple EC2 instances using Multi-Attach.
       is_correct: false
     - text: Use Amazon FSx for Windows File Server as the backend for the SFTP service.
       is_correct: false
    explanation: |
      Correct: AWS Transfer Family is the serverless SFTP option. Using EFS as the backend provides POSIX-compliant storage with high IOPS and allows maintaining Linux-style user permissions (UID/GID).
      Incorrect: 
        - S3 doesn't support POSIX permissions natively.
        - EBS Multi-Attach is not serverless.
        - FSx for Windows doesn't emulate Linux permissions.
    diagram: |
      graph TD
        A[SFTP Clients] -->|AWS Transfer Family| B[EFS Volume]
        B -->|High IOPS| C[Encrypted Storage]

  - id: q422
    type: multiple_choice
    question: |
      A company develops ML models as independent microservices. Models fetch 1 GB of data from S3 at startup. Usage is irregular (unused for days or thousands of requests).
      Which design meets these requirements (asynchronous API)?
    options:
     - text: Direct requests to an Amazon SQS queue. Deploy models as Amazon ECS services that read from the queue. Enable Auto Scaling based on queue size.
       is_correct: true
     - text: Use AWS Lambda for the ML models and trigger them via API Gateway.
       is_correct: false
     - text: Deploy models on EC2 Reserved Instances and use an ALB.
       is_correct: false
     - text: Use Amazon SageMaker Real-Time Inference endpoints for all models.
       is_correct: false
    explanation: |
      Correct: SQS decouples the API for asynchronous processing. ECS with Auto Scaling (potentially to zero) is ideal for models that load 1 GB in memory, exceeding Lambda limits. Scaling based on queue size ensures resources match demand.
      Incorrect:
        - Lambda has 6 vCPU/10 GB limits, insufficient for 1 GB models.
        - Reserved Instances are wasteful for irregular usage.
        - SageMaker Real-Time is expensive for unused models.
    diagram: |
      graph TD
        A[API Requests] -->|SQS Queue| B[ECS Services]
        B -->|Auto Scaling| C[ML Models]

  - id: q424
    type: multiple_choice
    question: |
      An application has frontend nodes (24/7) and backend nodes (short time, varying workload).
      Which pricing model is MOST cost-effective?
    options:
     - text: Use Reserved Instances for the frontend nodes. Use Spot Instances for the backend nodes.
       is_correct: true
     - text: Use On-Demand Instances for both tiers.
       is_correct: false
     - text: Use Spot Instances for all nodes.
       is_correct: false
     - text: Use Dedicated Hosts for the frontend and Reserved Instances for the backend.
       is_correct: false
    explanation: |
      Correct: Reserved Instances offer significant discounts for 24/7 workloads. Spot Instances provide the lowest cost (up to 90% off) for flexible, short-term backend workloads.
      Incorrect:
        - On-Demand is the most expensive for constant workloads.
        - Spot for frontend risks interruption for critical services.
    diagram: |
      graph TD
        A[Frontend] -->|Reserved Instances| B[24/7 Cost Savings]
        C[Backend] -->|Spot Instances| D[Variable Cost Savings]

  - id: q425
    type: multiple_choice
    question: |
      A company needs 15,000 IOPS and wants to provision disk performance independent of storage capacity.
      Which EBS volume type is MOST cost-effective?
    options:
     - text: gp3
       is_correct: true
     - text: gp2
       is_correct: false
     - text: io1
       is_correct: false
     - text: st1
       is_correct: false
    explanation: |
      Correct: gp3 allows provisioning IOPS and Throughput independently of storage size, costing less than io1/io2 for up to 16,000 IOPS.
      Incorrect:
        - gp2 ties IOPS to storage size (3 IOPS/GB), requiring a huge disk for 15,000 IOPS.
        - io1 is much more expensive per IOPS.
        - st1 is HDD and doesn't achieve high IOPS.
    diagram: |
      graph TD
        A[Application] -->|15,000 IOPS| B[gp3 Volume]

  - id: q426
    type: multiple_choice
    question: |
      A healthcare company needs to migrate data that changes frequently and requires audit access at all levels.
      Which solution meets these requirements?
    options:
     - text: Use AWS Storage Gateway to move data to S3. Use AWS CloudTrail to log management events.
       is_correct: true
     - text: Use AWS Snowball to migrate data. Use VPC Flow Logs for auditing.
       is_correct: false
     - text: Use AWS DataSync to move data to S3. Use S3 Server Access Logging for audit access.
       is_correct: false
     - text: Use AWS Transfer Family to move data. Use IAM reports for auditing.
       is_correct: false
    explanation: |
      Correct: Storage Gateway allows seamless migration with ongoing changes. CloudTrail provides comprehensive audit logs for all AWS API calls, meeting healthcare compliance needs.
      Incorrect:
        - Snowball is for one-time migration, not frequent changes.
        - Flow Logs audit network traffic, not data access.
    diagram: |
      graph TD
        A[On-premises Data] -->|Storage Gateway| B[S3]
        B -->|Audit| C[CloudTrail]
  - id: q427
    type: multiple_choice
    question: |
      A Java application must be deployed on Apache Tomcat and be highly available.
      What should a solutions architect recommend?
    options:
      - text: Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment.
        is_correct: true
      - text: Run the application on a single EC2 instance with an Elastic IP.
        is_correct: false
      - text: Use AWS Lambda with an API Gateway.
        is_correct: false
      - text: Use Amazon S3 Static Website Hosting.
        is_correct: false
    explanation: |
      Correto: O Elastic Beanstalk é o serviço ideal para aplicações Java/Tomcat, gerenciando automaticamente o Provisionamento, Load Balancing e Auto Scaling para garantir Alta Disponibilidade.
      Incorreto:
        - Instância única não é altamente disponível.
        - Lambda exigiria refatorar a aplicação Java Tomcat para um modelo orientado a eventos.

  - id: q428
    type: multiple_choice
    question: |
      A Lambda function needs to read and write to a DynamoDB table.
      Which solution is MOST secure?
    options:
      - text: Create an IAM role for Lambda with a policy for DynamoDB access and assign it as the execution role.
        is_correct: true
      - text: Hardcode the IAM User access keys in the Lambda function.
        is_correct: false
      - text: Use a DynamoDB Resource-Based Policy to allow the Lambda function.
        is_correct: false
      - text: Enable public access on the DynamoDB table.
        is_correct: false
    explanation: |
      Correto: O uso de IAM Roles (Execution Role) é o padrão de segurança da AWS para conceder permissões temporárias e automáticas a funções Lambda (Princípio do Menor Privilégio).
      Incorreto:
        - DynamoDB não suporta Resource-Based Policies (apenas Identity-Based Policies).
        - Hardcoding de chaves é uma falha grave de segurança.

  - id: q430
    type: multiple_choice
    question: |
      Sensor data (.csv) is uploaded to S3. They must be converted to images ASAP. Images are irrelevant after 1 month. .csv files are kept for ML training twice a year.
      Which combination is MOST cost-effective? (Choose two.)
    options:
      - text: Design an AWS Lambda function to convert .csv to images upon upload.
        is_correct: true
      - text: Create S3 Lifecycle rules to transition .csv to Glacier after 1 day and expire images after 30 days.
        is_correct: true
      - text: Use Amazon EMR to process the files daily.
        is_correct: false
      - text: Store everything in S3 Standard forever.
        is_correct: false
      - text: Use S3 Intelligent-Tiering for all files.
        is_correct: false
    explanation: |
      Correto: Lambda processa os dados quase em tempo real (ASAP). O Lifecycle para Glacier reduz custos dos arquivos .csv que só são usados raramente (treinamento planejado), e a expiração automática limpa as imagens inúteis.
      Incorreto:
        - EMR é muito caro para essa tarefa simples.
        - S3 Standard é caro para retenção de longo prazo de dados raramente acessados.

  - id: q431
    type: multiple_choice
    question: |
      A video game needs a top-10 scoreboard in near-real time and the ability to restore game state.
      What should be used?
    options:
      - text: Set up an Amazon ElastiCache for Redis cluster.
        is_correct: true
      - text: Use a DynamoDB table with a Global Secondary Index.
        is_correct: false
      - text: Use a Large RDS MySQL instance with Query Cache.
        is_correct: false
      - text: Use Amazon Redshift for real-time analytics.
        is_correct: false
    explanation: |
      Correto: O Redis possui uma estrutura de dados chamada "Sorted Sets" que é perfeita para scoreboards em tempo real, além de ser extremamente rápido (em memória).
      Incorreto:
        - DynamoDB é rápido, mas o Redis é superior para cálculos de ranking em tempo real.
        - RDS teria latência de disco superior ao cache em memória.

  - id: q433
    type: multiple_choice
    question: |
      A company wants to prevent the modification of cost usage tags across multiple accounts.
      Which solution meets this?
    options:
      - text: Create a Service Control Policy (SCP) to prevent tag modification.
        is_correct: true
      - text: Use an IAM policy on each user.
        is_correct: false
      - text: Enable Cost Explorer and set an alert.
        is_correct: false
      - text: Use AWS Budget alerts.
        is_correct: false
    explanation: |
      Correto: SCPs são guardrails preventivos que se aplicam a todos os usuários das contas de uma organização, inclusive administradores, garantindo que as tags não sejam alteradas.
      Incorreto:
        - Alertas avisam após a modificação ter ocorrido; não a previnem.
        - IAM policies em cada usuário é um pesadelo administrativo em múltiplas contas.

  - id: q437
    type: multiple_choice
    question: |
      An ecommerce site is suffering performance issues due to high request rates from changing IPs (DDoS concern).
      How to block these requests with minimal impact?
    options:
      - text: Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.
        is_correct: true
      - text: Block the IP ranges in the Network ACL.
        is_correct: false
      - text: Change the ALB to a Network Load Balancer.
        is_correct: false
      - text: Use Route 53 to block the requests.
        is_correct: false
    explanation: |
      Correto: O WAF permite criar regras de taxa (Rate-limiting). Se um IP faz requisições demais, ele é bloqueado automaticamente, protegendo o backend sem afetar usuários legítimos.
      Incorreto:
        - Network ACLs não escalam bem para IPs que mudam constantemente e têm um limite de regras.
        - O NLB sozinho não possui inteligência de filtragem de camada 7 (HTTP).

  - id: q438
    type: multiple_choice
    question: |
      A company needs to share a copy of an RDS database in a private subnet with an external auditor's AWS account.
      Which is the MOST secure way?
    options:
      - text: Create an encrypted snapshot, share it with the auditor's account, and grant access to the KMS key.
        is_correct: true
      - text: Export the database to a public S3 bucket.
        is_correct: false
      - text: Create a Read Replica in the auditor's VPC.
        is_correct: false
      - text: Set up a Client VPN for the auditor.
        is_correct: false
    explanation: |
      Correto: Compartilhar snapshots criptografados é o método oficial e mais seguro para transferir volumes de dados entre contas AWS, mantendo a criptografia durante todo o processo.
      Incorreto:
        - S3 público é uma falha de segurança gravíssima.
        - Replicas de leitura não funcionam nativamente entre contas diferentes de forma simples para este cenário de auditoria pontual.

  - id: q439
    type: multiple_choice
    question: |
      A VPC has run out of IP addresses for new EC2 instances.
      Which solution has the LEAST operational overhead?
    options:
      - text: Add an additional IPv4 CIDR block to the VPC and create new subnets.
        is_correct: true
      - text: Create a new VPC and peer it with the old one.
        is_correct: false
      - text: Change the existing CIDR block size.
        is_correct: false
      - text: Use IPv6 instead of IPv4 for all instances.
        is_correct: false
    explanation: |
      Correto: Você pode expandir uma VPC existente adicionando blocos CIDR secundários. Isso é muito mais simples do que migrar recursos para uma nova VPC ou gerenciar peering.
      Incorreto:
        - Você não pode alterar o tamanho de um CIDR block existente após a criação da VPC.

  - id: q441
    type: multiple_choice
    question: |
      A company hosts a multi-tier web application on EC2 behind an ALB. The ASG launches more On-Demand instances when users access high volumes of static web content.
      How should a solutions architect redesign the application MOST cost-effectively?
    options:
      - text: Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.
        is_correct: true
      - text: Increase the instance size of the EC2 instances to handle more traffic.
        is_correct: false
      - text: Use Spot Instances instead of On-Demand instances in the Auto Scaling group.
        is_correct: false
      - text: Store the static content on Amazon EFS and mount it to the EC2 instances.
        is_correct: false
    explanation: |
      Correto: Offloading de conteúdo estático para o S3 + CloudFront reduz drasticamente a carga nos servidores EC2 e no ALB. O CloudFront (CDN) faz o cache global, diminuindo a latência e os custos de processamento de rede/CPU nas instâncias.
      Incorreto:
        - Aumentar instâncias ou usar Spot não resolve a ineficiência de servir arquivos estáticos via EC2.
        - EFS é mais caro para entrega de conteúdo público que o S3/CloudFront.

  - id: q442
    type: multiple_choice
    question: |
      A company uses AWS Lake Formation. The data science team wants to securely share selective data from its accounts with the engineering team for analytical purposes with the LEAST operational overhead.
      Which solution meets these requirements?
    options:
      - text: Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data.
        is_correct: true
      - text: Create IAM roles in each account and use STS for cross-account access.
        is_correct: false
      - text: Use S3 Bucket Policies to grant access to the engineering team's IAM users.
        is_correct: false
      - text: Copy the selective data into a central S3 bucket and manage access via IAM.
        is_correct: false
    explanation: |
      Correto: O Lake Formation (LF-TBAC) simplifica o compartilhamento de dados em escala. Você define tags (ex: "Project:Engineering") e concede acesso baseado nessas tags, eliminando a necessidade de gerenciar centenas de políticas de recursos ou cópias de dados.
      Incorreto:
        - Copiar dados gera redundância e aumenta custos de armazenamento.
        - IAM e políticas de bucket tornam-se complexos demais em um ambiente de petabytes e múltiplas contas.

  - id: q443
    type: multiple_choice
    question: |
      A scalable web application will be accessed globally. Users download and upload unique data up to gigabytes in size. 
      Which cost-effective solution minimizes latency and maximizes performance?
    options:
      - text: Use Amazon S3 with Transfer Acceleration to host the application data.
        is_correct: true
      - text: Use AWS Global Accelerator for all S3 traffic.
        is_correct: false
      - text: Deploy the application in every AWS Region globally.
        is_correct: false
      - text: Use an Amazon CloudFront distribution for both uploads and downloads.
        is_correct: false
    explanation: |
      Correto: O S3 Transfer Acceleration usa a rede global da AWS (Edge Locations) para otimizar transferências de arquivos grandes a longas distâncias, sendo ideal para uploads e downloads de GBs de dados.
      Incorreto:
        - Global Accelerator é excelente para protocolos TCP/UDP, mas o Transfer Acceleration é especificamente otimizado para o protocolo do S3.
        - CloudFront é ótimo para downloads (cache), mas menos eficiente que o Transfer Acceleration para uploads massivos de arquivos únicos de GBs.

  - id: q444
    type: multiple_choice
    question: |
      An application has 1 RDS instance and 2 EC2 instances in a single AZ. After an accidental deletion of the DB, the app was down for 24 hours.
      What maximizes reliability?
    options:
      - text: Update the DB to Multi-AZ, enable deletion protection. Run EC2 in an ASG across multiple AZs behind an ALB.
        is_correct: true
      - text: Create daily snapshots of the DB and store them in S3.
        is_correct: false
      - text: Increase the instance size of the DB and EC2 instances.
        is_correct: false
      - text: Use a Network Load Balancer and move the instances to a public subnet.
        is_correct: false
    explanation: |
      Correto: Multi-AZ resolve o SPOF (Ponto Único de Falha) do banco. Deletion Protection previne o erro humano citado. ASG + ALB + Multi-AZ para EC2 garante que a falha de uma AZ inteira não derrube a aplicação.
      Incorreto:
        - Snapshots ajudam na recuperação, mas não evitam a indisponibilidade (RTO alto).
        - Mover para subnets públicas ou aumentar instâncias não aumenta a resiliência contra falhas de infraestrutura.

  - id: q445
    type: multiple_choice
    question: |
      A company must move 700 TB from NAS to AWS within 90 days. They have a 10 Gbps Direct Connect and need to access/update data during the transfer.
      Which solution meets this?
    options:
      - text: Create an AWS DataSync agent in the data center and start the transfer to Amazon S3.
        is_correct: true
      - text: Order several AWS Snowball Edge devices to move the data.
        is_correct: false
      - text: Use the AWS CLI to sync data from the NAS to S3.
        is_correct: false
      - text: Use AWS Storage Gateway in File Gateway mode to copy the data.
        is_correct: false
    explanation: |
      Correto: O DataSync é otimizado para transferências via rede (usando o Direct Connect existente). Ele lida com a sincronização contínua, verificações de integridade e permite que os dados sejam atualizados durante a janela de migração.
      Incorreto:
        - Snowball é offline; você não conseguiria "atualizar" facilmente os dados em trânsito.
        - CLI é ineficiente e difícil de gerenciar para 700 TB em comparação à automação do DataSync.

  - id: q446
    type: multiple_choice
    question: |
      A legal requirement requires retaining S3 data for 7 years. 
      Which solution has the LEAST operational overhead?
    options:
      - text: Turn on S3 Object Lock with compliance retention mode for 7 years. Use S3 Batch Operations for existing data.
        is_correct: true
      - text: Create a Lambda function to check for deletions and restore files.
        is_correct: false
      - text: Enable S3 Versioning and a lifecycle policy to move data to Glacier after 7 years.
        is_correct: false
      - text: Use AWS Backup to create snapshots every day for 7 years.
        is_correct: false
    explanation: |
      Correto: O Object Lock em modo Compliance é a única forma de garantir que ninguém (nem o root) apague os dados. O S3 Batch Operations permite aplicar isso retroativamente a arquivos existentes de forma massiva.
      Incorreto:
        - Versionamento permite deleção (ainda que reversível), o que pode não atender requisitos legais rigorosos.
        - Lambda e AWS Backup geram custos e gestão extra desnecessários comparados ao recurso nativo de Lock.

  - id: q447
    type: multiple_choice
    question: |
      A stateless Lambda app with API Gateway needs to be deployed across multiple Regions for failover.
      How to route traffic?
    options:
      - text: Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.
        is_correct: true
      - text: Use an Application Load Balancer with a Global Accelerator.
        is_correct: false
      - text: Configure API Gateway to replicate traffic to other regions automatically.
        is_correct: false
      - text: Use CloudFront with a custom origin that points to all regions simultaneously.
        is_correct: false
    explanation: |
      Correto: O Route 53 com roteamento de Failover ou Geoproximidade baseado em Health Checks é o método padrão para gerenciar failover regional de APIs.
      Incorreto:
        - API Gateway não replica tráfego nativamente.
        - ALB é um recurso regional e não faz roteamento entre múltiplas regiões por si só (embora o Global Accelerator pudesse ser usado, a opção do Route 53 é a resposta clássica para failover DNS).

  - id: q449
    type: multiple_choice
    question: |
      A company needs to migrate an Oracle database quickly. The app uses third-party features requiring privileged access (OS-level).
      Which solution is MOST cost-effective?
    options:
      - text: Migrate the database to Amazon RDS Custom for Oracle.
        is_correct: true
      - text: Migrate to a standard Amazon RDS for Oracle.
        is_correct: false
      - text: Rehost the database on Amazon EC2 instances.
        is_correct: false
      - text: Use Aurora PostgreSQL with Oracle compatibility.
        is_correct: false
    explanation: |
      Correto: O RDS Custom é projetado especificamente para casos onde você precisa dos benefícios do RDS (backup, patching), mas ainda requer acesso ao sistema operacional para instalar drivers ou softwares de terceiros.
      Incorreto:
        - RDS padrão não permite acesso privilegiado ao OS.
        - EC2 exige gestão manual total (backups, patches), o que a empresa quer evitar.

  - id: q450
    type: multiple_choice
    question: |
      A single-server app needs to migrate to AWS following Well-Architected practices (Security, Scalability, Resiliency).
      Which combination meets this? (Choose three.)
    options:
      - text: Create a VPC across two AZs. Private subnets for each tier with ASGs for web and app tiers.
        is_correct: true
      - text: Use Elastic Load Balancers in front of the web tier. Control access using SG referencing.
        is_correct: true
      - text: Use an Amazon RDS Multi-AZ deployment in private subnets.
        is_correct: true
      - text: Host the entire application on a single large EC2 instance for simplicity.
        is_correct: false
      - text: Use a Public Subnet for the database to allow remote administration.
        is_correct: false
      - text: Use an Internet Gateway as the only security measure.
        is_correct: false
    explanation: |
      Correto: O modelo de 3 camadas (3-tier) com Multi-AZ, ASG, ELB e subnets privadas é o padrão "ouro" de arquitetura na AWS. O referenciamento de Security Groups garante que apenas a camada anterior acesse a próxima.
      Incorreto:
        - Banco de dados em subnet pública viola o pilar de segurança.
        - Servidor único viola o pilar de resiliência.
 