questions:
  - id: q401
    type: multiple_choice
    question: |
      A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage.

      The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand.

      Which solution will meet these requirements?
    options:
      - text: Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Auto Scaling Across Multiple Availability Zones: Deploying application servers using EC2 instances in an Auto Scaling group across multiple Availability Zones (AZs) helps avoid a single point of failure. If one AZ experiences an issue, the application can continue to operate in another AZ.
      Incorrect: 
        "***replace later***"

  - id: q403
    type: multiple_choice
    question: |
      A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. The developer already has an IAM user with valid IAM credentials required for Amazon S3.

      What should a solutions architect do to grant the permissions?
    options:
      - text: Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: To grant the necessary permissions to an AWS Lambda function to upload files to Amazon S3, a solutions architect should create an IAM execution role with the required permissions and attach the IAM role to the Lambda function. This approach follows the principle of least privilege and ensures that the Lambda function can only access the resources it needs to perform its specific task.
      Incorrect: 
        "***replace later***"

  - id: q404
    type: multiple_choice
    question: |
      A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents.

      What should a solutions architect do to improve the architecture of this application?
    options:
      - text: Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Introducing Amazon SQS as a queue allows for better decoupling between the S3 events and the document processing. This ensures that the Lambda function is not overwhelmed with spikes in incoming events, leading to missed document processing.
      Incorrect: 
        "***replace later***"

  - id: q405
    type: multiple_choice
    question: |
      A solutions architect is designing the architecture for a software demonstration environment. The environment will run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will experience significant increases in traffic during working hours but is not required to operate on weekends.

      Which combination of actions should the solutions architect take to ensure that the system can scale to meet demand? (Choose two.)
    options:
      - text: Use AWS Auto Scaling to adjust the ALB capacity based on request rate.
        is_correct: true
      - text: Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.
        is_correct: true
    explanation: |
      Correct: This allows the ALB to automatically scale its capacity based on the incoming request rate, ensuring that the system can handle varying traffic loads.
      Correct: This allows you to save costs and resources during weekends when the system is not required to operate. Scaling down the Auto Scaling group to zero instances during weekends and reverting to the default values at the start of the week ensures that you only incur costs when the system is actively in use.
      Incorrect: 
        "***replace later***"

  - id: q406
    type: multiple_choice
    question: |
      A solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306.

      Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)
    options:
      - text: Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.
        is_correct: true
      - text: Create a security group for the DB instance. Add a rule to allow traffic from the web servers’ security group on port 3306.
        is_correct: true
    explanation: |
      Correct: This allows inbound traffic from the internet on port 443 to the web servers.
      Correct: This ensures that the RDS instance is accessible only from the web servers in the public subnet.
      Incorrect: 
        "***replace later***"

  - id: q407
    type: multiple_choice
    question: |
      A company is implementing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution must be fully managed.

      Which solution meets these requirements?
    options:
      - text: Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Amazon FSx for Lustre is a fully managed service that provides high-performance shared storage. It is specifically designed to be used with Lustre, making it a suitable solution for Lustre clients.
      Incorrect: 
        "***replace later***"

  - id: q408
    type: multiple_choice
    question: |
      A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored.

      The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region.

      Which solution will meet these requirements?
    options:
      - text: Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLB. Process the data in Amazon ECS.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: AWS Global Accelerator provides static IP addresses that act as a fixed entry point to your application. It routes traffic over the AWS global network to the optimal AWS endpoint based on health, geography, and routing policies.
      Correct: NLB is well-suited for UDP-based traffic, and it's designed for high-performance, low-latency applications. In this case, it can efficiently handle the thousands of geographically dispersed remote devices sending UDP traffic.
      Correct: Using ECS with Fargate allows you to deploy and run containers without managing the underlying infrastructure. This setup can efficiently handle the immediate processing of data without the need to manage the underlying servers.
      Incorrect: 
        "***replace later***"

  - id: q409
    type: multiple_choice
    question: |
      A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and configuring an Elastic Load Balancer attached to the instances.

      Which replacement to the on-premises file share is MOST resilient and durable?
    options:
      - text: Migrate the file share to Amazon FSx for Windows File Server.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Amazon FSx for Windows File Server is a fully managed file storage service that is compatible with Windows file systems. Amazon FSx for Windows File Server is specifically designed for Windows workloads, including IIS web applications. It provides a highly available and durable file system that can be accessed by multiple EC2 instances in different Availability Zones.
      Incorrect: 
        "***replace later***"

  - id: q410
    type: multiple_choice
    question: |
      A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest.

      Which solution will meet this requirement?
    options:
      - text: Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: By creating the EBS volumes as encrypted volumes, you ensure that all data written to those volumes is automatically encrypted. This provides a straightforward and effective solution for meeting the encryption-at-rest requirement.
      Incorrect: 
        "***replace later***"

  - id: q411
    type: multiple_choice
    question: |
      A company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modifications.

      Which solution will meet these requirements?
    options:
      - text: MySQL-compatible Amazon Aurora Serverless
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Aurora Serverless is a serverless option for MySQL-compatible databases. It automatically adjusts the database capacity based on actual usage, making it suitable for sporadic usage patterns. It is MySQL-compatible, so it won't require significant database modifications.
      Incorrect: 
        "***replace later***"

  - id: q412
    type: multiple_choice
    question: |
      An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private.

      Which solution will meet these requirements?
    options:
      - text: Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: AWS Organizations allows you to create service control policies (SCPs) that set fine-grained permissions for member accounts. In this case, you can create an SCP that prevents IAM users from changing the S3 Block Public Access settings. Applying this SCP to the account ensures that the configured public access settings remain in place and cannot be altered by IAM users.
      Incorrect: 
        "***replace later***"

  - id: q413
    type: multiple_choice
    question: |
      An ecommerce company is experiencing an increase in user traffic. The company’s store is deployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As traffic increases, the company notices that the architecture is causing significant delays in sending timely marketing and order confirmation email to users. The company wants to reduce the time it spends resolving complex email delivery issues and minimize operational overhead.

      What should a solutions architect do to meet these requirements?
    options:
      - text: Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Amazon Simple Email Service (Amazon SES) is a fully managed email sending service. By configuring the web instances to send emails through Amazon SES, the ecommerce company can offload the complexity of email delivery to a reliable and scalable service.
      Incorrect: 
        "***replace later***"

  - id: q415
    type: multiple_choice
    question: |
      A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is accessed with varying frequency. The company does not know access patterns for all the data. The company needs to implement a solution for each S3 bucket to optimize the cost of S3 usage.

      Which solution will meet these requirements with the MOST operational efficiency?
    options:
      - text: Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: S3 Intelligent-Tiering is designed to automatically and dynamically move objects between two access tiers – frequent and infrequent access – based on changing access patterns. It is a good fit for data with unknown or changing access patterns. It provides cost savings compared to S3 Standard while maintaining low-latency access to frequently accessed objects.
      Incorrect: 
        "***replace later***"

  - id: q416
    type: multiple_choice
    question: |
      A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon RDS database The website’s users are experiencing slow page loads.

      Which combination of actions should a solutions architect take to resolve this issue? (Choose two.)
    options:
      - text: Set up an Amazon CloudFront distribution.
        is_correct: true
      - text: Create a read replica for the RDS DB instance.
        is_correct: true
    explanation: |
      Correct: Amazon CloudFront is a content delivery network (CDN) that can improve the performance of a website by caching static content closer to the users. This reduces latency and improves page load times. Configure CloudFront to distribute static content such as images, stylesheets, and JavaScript files. This will offload the serving of static assets from the web servers, improving overall website performance.
      Correct: Creating a read replica for the Amazon RDS database allows you to offload read traffic from the primary database, improving the overall database performance.
      Incorrect: 
        "***replace later***"

  - id: q417
    type: multiple_choice
    question: |
      A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work.

      The application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low.

      Which solution will meet these requirements?
    options:
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: "***replace later***"
      Incorrect: 
        "***replace later***"

  - id: q418
    type: multiple_choice
    question: |
      A solutions architect needs to allow team members to access Amazon S3 buckets in two different AWS accounts: a development account and a production account. The team currently has access to S3 buckets in the development account by using unique IAM users that are assigned to an IAM group that has appropriate permissions in the account.

      The solutions architect has created an IAM role in the production account. The role has a policy that grants access to an S3 bucket in the production account.

      Which solution will meet these requirements while complying with the principle of least privilege?
    options:
      - text: Add the development account as a principal in the trust policy of the role in the production account.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: By adding the development account as a principal in the trust policy of the IAM role in the production account, you enable IAM users in the development account to assume the role and gain temporary permissions to access the S3 bucket in the production account. This approach follows the principle of least privilege because it allows users in the development account to access only the specific resources (S3 bucket) defined in the trust policy of the IAM role.
      Incorrect: 
        "***replace later***"

  - id: q419
    type: multiple_choice
    question: |
      A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest.

      An audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes.

      Which combination of steps will meet these requirements? (Choose two.)
    options:
      - text: Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.
        is_correct: true
      - text: In the Organizations management account, specify the Default EBS volume encryption setting.
        is_correct: true
    explanation: |
      Correct: Creating an SCP and attaching it to the root organizational unit ensures that no unencrypted EBS volumes can be created in the specified Region.
      Correct: Specifying the Default EBS volume encryption setting in the Organizations management account ensures that all new EBS volumes are encrypted by default, reducing the risk of human error.
      Incorrect: 
        "***replace later***"

  - id: q420
    type: multiple_choice
    question: |
      A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to offload reads off of the primary instance and keep costs as low as possible.

      Which solution will meet these requirements?
    options:
      - text: Use an Amazon RDS Multi-AZ DB cluster deployment. Point the read workload to the reader endpoint.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Amazon RDS Multi-AZ DB Cluster Deployment provides high availability by automatically replicating data to a standby instance in a different Availability Zone. In case of a failure, Amazon RDS automatically fails over to the standby instance.
      Incorrect: 
        "***replace later***"

  - id: q421
    type: multiple_choice
    question: |
      A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept traffic from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers.

      The company wants a serverless option that provides high IOPS performance and highly configurable security. The company also wants to maintain control over user permissions.

      Which solution will meet these requirements?
    options:
      - text: Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Using AWS Transfer Family with an encrypted Amazon EFS volume provides a serverless, high-performance, and secure solution for the SFTP service. The VPC endpoint with internet-facing access and security group ensures that only trusted IP addresses can access the service.
      Incorrect: 
        "***replace later***"

  - id: q422
    type: multiple_choice
    question: |
      A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent.

      The company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time.

      Which design should a solutions architect recommend to meet these requirements?
    options:
      - text: Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Deploying the models as Amazon ECS services allows for flexibility in managing the containerized applications. ECS services can efficiently handle the startup process of fetching model data from Amazon S3 and loading it into memory. Using SQS ensures that requests are queued and processed asynchronously, while Auto Scaling dynamically adjusts resources based on demand.
      Incorrect: 
        "***replace later***"

  - id: q423
    type: multiple_choice
    question: |
      Josn format
    options:
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: "***replace later***"
      Incorrect: 
        "***replace later***"

  - id: q424
    type: multiple_choice
    question: |
      A company is running a custom application on Amazon EC2 On-Demand Instances. The application has frontend nodes that need to run 24 hours a day, 7 days a week and backend nodes that need to run only for a short time based on workload. The number of backend nodes varies during the day.

      The company needs to scale out and scale in more instances based on workload.

      Which solution will meet these requirements MOST cost-effectively?
    options:
      - text: Use Reserved Instances for the frontend nodes. Use Spot Instances for the backend nodes.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Reserved Instances provide a significant cost savings for frontend nodes that need to run continuously. Spot Instances are a cost-effective option for backend nodes with flexible availability requirements.
      Incorrect: 
        "***replace later***"

  - id: q425
    type: multiple_choice
    question: |
      A company uses high block storage capacity to runs its workloads on premises. The company's daily peak input and output transactions per second are not more than 15,000 IOPS. The company wants to migrate the workloads to Amazon EC2 and to provision disk performance independent of storage capacity.

      Which Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements MOST cost-effectively?
    options:
      - text: GP3 volume type
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: General Purpose SSD (gp3) volumes are designed to provide a balance of price and performance. They allow you to provision IOPS independently of storage capacity, making them suitable for workloads with varying performance requirements. GP3 volumes offer a lower price per IOPS compared to io1 volumes and are a good fit for general-purpose workloads.
      Incorrect: 
        "***replace later***"

  - id: q426
    type: multiple_choice
    question: |
      A company needs to store data from its healthcare application. The application’s data frequently changes. A new regulation requires audit access at all levels of the stored data.

      The company hosts the application on an on-premises infrastructure that is running out of storage capacity. A solutions architect must securely migrate the existing data to AWS while satisfying the new regulation.

      Which solution will meet these requirements?
    options:
      - text: Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: AWS Storage Gateway provides a secure and efficient way to migrate existing data to Amazon S3. AWS CloudTrail ensures that all management events are logged, satisfying the audit requirements.
      Incorrect: 
        "***replace later***"

  - id: q427
    type: multiple_choice
    question: |
      A solutions architect is implementing a complex Java application with a MySQL database. The Java application must be deployed on Apache Tomcat and must be highly available.

      What should the solutions architect do to meet these requirements?
    options:
      - text: Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment and a rolling deployment policy.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: AWS Elastic Beanstalk is a fully managed service that simplifies the deployment and operation of applications, including web applications running Apache Tomcat. Elastic Beanstalk handles the deployment details, capacity provisioning, load balancing, auto-scaling, and application health monitoring, making it easier to deploy and manage your applications.
      Incorrect: 
        "***replace later***"

  - id: q428
    type: multiple_choice
    question: |
      A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. The Lambda function needs permissions to read and write to the DynamoDB table.

      Which solution will give the Lambda function access to the DynamoDB table MOST securely?
    options:
      - text: Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the DynamoDB table. Update the configuration of the Lambda function to use the new role as the execution role.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Creating an IAM role with Lambda as a trusted service and attaching a policy that grants the required permissions ensures secure access to the DynamoDB table. This approach follows the principle of least privilege.
      Incorrect: 
        "***replace later***"

  - id: q429
    type: multiple_choice
    question: |
      Json format
    options:
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: "***replace later***"
      Incorrect: 
        "***replace later***"

  - id: q430
    type: multiple_choice
    question: |
      A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. These .csv files must be converted into images and must be made available as soon as possible for the automatic generation of graphical reports.

      The images become irrelevant after 1 month, but the .csv files must be kept to train machine learning (ML) models twice a year. The ML trainings and audits are planned weeks in advance.

      Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)
    options:
      - text: Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket. Invoke the Lambda function when a .csv file is uploaded.
        is_correct: true
      - text: Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days.
        is_correct: true
    explanation: |
      Correct: Using AWS Lambda to convert .csv files into images ensures that the images are generated as soon as possible. S3 Lifecycle rules optimize storage costs by transitioning .csv files to S3 Glacier and expiring image files after 30 days.
      Incorrect: 
        "***replace later***"

  - id: q431
    type: multiple_choice
    question: |
      A company has developed a new video game as a web application. The application is in a three-tier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players will compete concurrently online. The game’s developers want to display a top-10 scoreboard in near-real time and offer the ability to stop and restore the game while preserving the current scores.

      What should a solutions architect do to meet these requirements?
    options:
      - text: Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Redis is an in-memory data store that is well-suited for caching and real-time data processing. By setting up an ElastiCache for Redis cluster, you can compute and cache the scores in-memory, allowing for fast retrieval and updates.
      Incorrect: 
        "***replace later***"

  - id: q432
    type: multiple_choice
    question: |
      An ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company will use the models to visualize complex scenarios and to detect trends in customer data. The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards.

      Which solution will meet these requirements with the LEAST operational overhead?
    options:
      - text: Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Amazon SageMaker simplifies the ML workflow and reduces operational overhead. QuickSight can directly connect to Amazon SageMaker models and use the results for visualization without the need for extensive data movement or transformation.
      Incorrect: 
        "***replace later***"

  - id: q433
    type: multiple_choice
    question: |
      A company is running its production and nonproduction environment workloads in multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent the modification of cost usage tags.

      Which solution will meet these requirements?
    options:
      - text: Create a service control policy (SCP) to prevent tag modification except by authorized principals.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: SCPs in AWS Organizations are used to set fine-grained permissions on what actions AWS accounts within the organization can perform. You can create a custom SCP to specifically control access to tag modification.
      Incorrect: 
        "***replace later***"

  - id: q434
    type: multiple_choice
    question: |
      A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in another AWS Region with minimal downtime.

      What should a solutions architect do to meet these requirements with the LEAST amount of downtime?
    options:
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: "***replace later***"
      Incorrect: 
        "***replace later***"

  - id: q435
    type: multiple_choice
    question: |
      A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The database is 20 TB in size. The company wants to complete the migration with minimal downtime.

      Which solution will migrate the database MOST cost-effectively?
    options:
      - text: Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Send the Snowball Edge device to AWS to finish the migration and continue the ongoing replication.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: This is a cost-effective solution for shipping large amounts of data to AWS. Snowball Edge devices are designed for efficient data transfer, and they can handle the 20 TB database. AWS DMS is a managed service for migrating databases to AWS, and AWS SCT can assist in converting the database schema. Using these tools in combination allows for a smooth migration process.
      Incorrect: 
        "***replace later***"

  - id: q436
    type: multiple_choice
    question: |
      A company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. The company successfully launched a new product. The workload on the database has increased. The company wants to accommodate the larger workload without adding infrastructure.

      Which solution will meet these requirements MOST cost-effectively?
    options:
      - text: Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB instance larger.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: When you commit to using a database instance for a longer time (with reserved instances), AWS gives you a discount compared to paying on a month-to-month basis. Making the instance larger means upgrading the power of your virtual computer.
      Incorrect: 
        "***replace later***"

  - id: q437
    type: multiple_choice
    question: |
      A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users.

      What should a solutions architect recommend?
    options:
      - text: Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: AWS WAF is a web application firewall service that helps protect your web applications from common web exploits. By associating AWS WAF with the ALB, you can inspect and filter incoming traffic before it reaches your instances, providing a layer of protection against DDoS attacks and other malicious activities.
      Incorrect: 
        "***replace later***"

  - id: q438
    type: multiple_choice
    question: |
      A company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the database.

      What is the MOST secure way for the company to share the database with the auditor?
    options:
      - text: Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access to the AWS Key Management Service (AWS KMS) encryption key.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Creating an encrypted snapshot ensures that the database data is protected during the transfer and storage process. Sharing the encrypted snapshot with the auditor allows them to create their own copy of the database securely. By allowing access to the AWS KMS encryption key, the auditor can decrypt the snapshot and restore it to their own environment.
      Incorrect: 
        "***replace later***"

  - id: q439
    type: multiple_choice
    question: |
      A solutions architect configured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP addresses for future workloads.

      Which solution resolves this issue with the LEAST operational overhead?
    options:
      - text: Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: By adding an additional IPv4 CIDR block to the existing VPC, you can effectively increase the number of available IP addresses within the same VPC. Creating additional subnets using the new CIDR block allows you to organize your resources and maintain segmentation within the VPC.
      Incorrect: 
        "***replace later***"

  - id: q440
    type: multiple_choice
    question: |
      A company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the final DB snapshot option on RDS termination.

      The company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition of Amazon Aurora to host the DB instance.

      Which solutions will create the new DB instance? (Choose two.)
    options:
      - text: Import the RDS snapshot directly into Aurora.
        is_correct: true
      - text: Upload the database dump to Amazon S3. Then import the database dump into Aurora.
        is_correct: true
    explanation: |
      Correct: Amazon Aurora allows you to directly import an Amazon RDS snapshot into Aurora. This is a straightforward process for migrating data from RDS to Aurora. Uploading the database dump to Amazon S3 and then importing the database dump into Aurora is a common method. You can use the MySQL-compatible version of Aurora to restore the data from a database dump stored in Amazon S3.
      Incorrect: 
        "***replace later***"

  - id: q441
    type: multiple_choice
    question: |
      A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost.

      What should a solutions architect do to redesign the application MOST cost-effectively?
    options:
      - text: Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, including images, videos, CSS, and JavaScript, with low latency and high transfer speeds. By creating a CloudFront distribution and hosting static web content in an Amazon S3 bucket, you offload the serving of static content to the CDN, which can significantly reduce the load on your EC2 instances.
      Incorrect: 
        "***replace later***"

  - id: q442
    type: multiple_choice
    question: |
      A company stores several petabytes of data across multiple AWS accounts. The company uses AWS Lake Formation to manage its data lake. The company's data science team wants to securely share selective data from its accounts with the company's engineering team for analytical purposes.

      Which solution will meet these requirements with the LEAST operational overhead?
    options:
      - text: Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Lake Formation allows you to use tag-based access control to authorize and grant permissions for data in the data lake. By applying tags to the relevant data and using tag-based access control, you can easily manage access to specific data sets without having to create additional IAM roles or copy data to a common account.
      Incorrect: 
        "***replace later***"

  - id: q443
    type: multiple_choice
    question: |
      A company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance.

      What should a solutions architect do to accomplish this?
    options:
      - text: Use Amazon S3 with Transfer Acceleration to host the application.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your S3 bucket. It takes advantage of Amazon CloudFront's globally distributed edge locations to accelerate uploads and downloads.
      Incorrect: 
        "***replace later***"

  - id: q444
    type: multiple_choice
    question: |
      A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone.

      An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment.

      What should the solutions architect do to maximize reliability of the application's infrastructure?
    options:
      - text: Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Multi-AZ RDS Instance ensures high availability by replicating data to a standby instance in a different Availability Zone. Enabling deletion protection prevents accidental deletion. Using an Application Load Balancer and Auto Scaling group across multiple Availability Zones improves the reliability and scalability of the web servers.
      Incorrect: 
        "***replace later***"

  - id: q445
    type: multiple_choice
    question: |
      A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in its corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct Connect connection.

      After an audit from a regulator, the company has 90 days to move the data to the cloud. The company needs to move the data efficiently and without disruption. The company still needs to be able to access and update the data during the transfer window.

      Which solution will meet these requirements?
    options:
      - text: Create an AWS DataSync agent in the corporate data center. Create a data transfer task. Start the transfer to an Amazon S3 bucket.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: AWS DataSync is designed for efficiently transferring large amounts of data between on-premises storage and Amazon S3. It allows you to create data transfer tasks and initiate the transfer to an Amazon S3 bucket.
      Incorrect: 
        "***replace later***"

  - id: q446
    type: multiple_choice
    question: |
      A company stores data in PDF format in an Amazon S3 bucket. The company must follow a legal requirement to retain all new and existing data in Amazon S3 for 7 years.

      Which solution will meet these requirements with the LEAST operational overhead?
    options:
      - text: Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Use S3 Batch Operations to bring the existing data into compliance.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: S3 Object Lock with compliance retention mode ensures that data cannot be deleted or overwritten during the retention period. S3 Batch Operations can be used to apply the retention settings to existing data, ensuring compliance with the legal requirement.
      Incorrect: 
        "***replace later***"

  - id: q447
    type: multiple_choice
    question: |
      A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities.

      What should a solutions architect do to route traffic to multiple Regions?
    options:
      - text: Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: By creating Amazon Route 53 health checks for each Region and configuring an active-active failover configuration, Route 53 can monitor the health of the endpoints in each Region and route traffic to healthy endpoints. In the event of a failure in one Region, Route 53 automatically routes traffic to the healthy endpoints in other Regions.
      Incorrect: 
        "***replace later***"

  - id: q448
    type: multiple_choice
    question: |
      A company has two VPCs named Management and Production. The Management VPC uses VPNs through a customer gateway to connect to a single device in the data center. The Production VPC uses a virtual private gateway with two attached AWS Direct Connect connections. The Management and Production VPCs both use a single VPC peering connection to allow communication between the applications.

      What should a solutions architect do to mitigate any single point of failure in this architecture?
    options:
      - text: Add a second set of VPNs to the Management VPC from a second customer gateway device.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Adding a second set of VPN connections from the Management VPC to a second customer gateway device provides redundancy and eliminates this single point of failure.
      Incorrect: 
        "***replace later***"

  - id: q449
    type: multiple_choice
    question: |
      A company runs its application on an Oracle database. The company plans to quickly migrate to AWS because of limited resources for the database, backup administration, and data center maintenance. The application uses third-party database features that require privileged access.

      Which solution will help the company migrate the database to AWS MOST cost-effectively?
    options:
      - text: Migrate the database to Amazon RDS Custom for Oracle. Customize the database settings to support third-party features.
        is_correct: true
      - text: "***dont't touch, replace later***"
        is_correct: false
    explanation: |
      Correct: Amazon RDS Custom for Oracle allows you to customize the database settings to support third-party features while benefiting from the managed service capabilities of RDS. This is a cost-effective solution for migrating Oracle databases to AWS.
      Incorrect: 
        "***replace later***"

  - id: q450
    type: multiple_choice
    question: |
      A company has a three-tier web application that is in a single server. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency.

      Which combination of solutions will meet these requirements? (Choose three.)
    options:
      - text: Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.
        is_correct: true
      - text: Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups.
        is_correct: true
      - text: Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups.
        is_correct: true
    explanation: |
      Correct: Creating a VPC across two Availability Zones aligns with best practices by using separate subnets for each tier, allowing for better security and scalability. Auto Scaling groups provide elasticity and resiliency. Elastic Load Balancers enhance scalability and resiliency, while security groups add an additional layer of security. Using Amazon RDS for the database tier with Multi-AZ deployment ensures high availability and security.
      Incorrect: 
        "***replace later***"