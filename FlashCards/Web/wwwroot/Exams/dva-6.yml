questions:
  - id: q251
    type: multiple_choice
    question: Your application is trying to upload a 6 GB file to Simple Storage Service and receive a 'Your proposed upload exceeds the maximum allowed object size.' error message. What is a possible solution for this?
    options:
     - text: None, Simple Storage Service objects are limited to 5 GB.
       is_correct: false
     - text: Use the multi-part upload API for this object.
       is_correct: true
     - text: Use the large object upload API for this object.
       is_correct: false
     - text: Contact support to increase your object size limit.
       is_correct: false
     - text: Upload to a different region.
       is_correct: false
    explanation: |
      Correct: Using the S3 multipart upload API allows you to upload objects larger than the single PUT limit (5 GB) by splitting the file into parts and assembling them on the server, which is the appropriate solution for a 6 GB file.

      Incorrect: The other options are wrong because S3 supports objects larger than 5 GB via multipart uploads; there is no separate "large object upload API", contacting support does not change the single PUT limit, and changing region does not address the technical requirement.
    tags: 
    difficulty: 
    points: 

  - id: q252
    type: multiple_choice
    question: What AWS products and features can be deployed by Elastic Beanstalk? (Choose THREE)
    options:
     - text: Auto scaling groups.
       is_correct: true
     - text: Route 53 hosted zones.
       is_correct: false
     - text: Elastic Load Balancers.
       is_correct: true
     - text: RDS Instances.
       is_correct: true
     - text: Elastic IP addresses.
       is_correct: false
     - text: SQS Queues.
       is_correct: false
    explanation: |
      Correct: Elastic Beanstalk can provision and manage resources such as Auto Scaling groups, Elastic Load Balancers, and RDS instances as part of an environment to simplify deployment and scaling of applications.

      Incorrect: Route 53 hosted zones, Elastic IP addresses and SQS queues are not automatically provisioned as standard managed resources by Beanstalk; they can be used with Beanstalk but are not part of the typical managed resource set.
    tags: 
    difficulty: 
    points: 

  - id: q253
    type: multiple_choice
    question: Games-R-Us is launching a new game app for mobile devices. Users will log into the game using their existing Facebook account and the game will record player data and scoring information directly to a DynamoDB table. What is the most secure approach for signing requests to the DynamoDB API?
    options:
     - text: Create an IAM user with access credentials that are distributed with the mobile app to sign the requests.
       is_correct: false
     - text: Distribute the AWS root account access credentials with the mobile app to sign the requests.
       is_correct: false
     - text: Request temporary security credentials using web identity federation to sign the requests.
       is_correct: true
     - text: Establish cross account access between the mobile app and the DynamoDB table to sign the requests
       is_correct: false
    explanation: |
      Correct: Requesting temporary security credentials via web identity federation (for example using Cognito or STS with Facebook tokens) is secure because it avoids embedding long-lived credentials in the app and limits permissions and lifetime.

      Incorrect: Distributing permanent credentials (IAM user or root) in the mobile app is insecure; cross-account access alone does not provide a secure, short-lived credential flow for mobile clients.
    tags: 
    difficulty: 
    points: 

  - id: q254
    type: multiple_choice
    question: Which of the following programming languages have an officially supported AWS SDK? (Choose TWO)
    options:
     - text: Perl.
       is_correct: false
     - text: PHP.
       is_correct: true
     - text: Pascal.
       is_correct: false
     - text: Java.
       is_correct: true
     - text: SQL.
       is_correct: false
    explanation: |
      Correct: AWS provides official SDKs for many languages, including PHP and Java, which are maintained by AWS to access AWS services.

      Incorrect: Perl, Pascal and SQL are not listed as languages with official AWS SDKs (though third-party libraries may exist), so they are not correct choices.
    tags: 
    difficulty: 
    points: 

  - id: q255
    type: multiple_choice
    question: A meteorological system monitors 600 temperature gauges, obtaining temperature samples every minute and saving each sample to a DynamoDB table Each sample involves writing 1K of data and the writes are evenly distributed over time. How much write throughput is required for the target table?
    options:
     - text: 1 write capacity unit.
       is_correct: false
     - text: 10 write capacity units.
       is_correct: true
     - text: 60 write capacity units.
       is_correct: false
     - text: 600 write capacity units.
       is_correct: false
     - text: 3600 write capacity units.
       is_correct: false
    explanation: |
      Correct: 600 writes per minute is approximately 10 writes per second; each write is 1 KB which consumes 1 write capacity unit, so about 10 WCU are required.

      Incorrect: The other options either under- or overestimate the required throughput; 1 WCU is insufficient for 10 writes/sec and values like 60/600/3600 WCU are excessive for this scenario.
    tags: 
    difficulty: 
    points: 

  - id: q256
    type: multiple_choice
    question: In DynamoDB, what type of HTTP response codes indicate that a problem was found with the client request sent to the service?
    options:
     - text: 5xx HTTP response code.
       is_correct: false
     - text: 200 HTTP response code.
       is_correct: false
     - text: 306 HTTP response code.
       is_correct: false
     - text: 4xx HTTP response code.
       is_correct: true
    explanation: |
      Correct: 4xx HTTP response codes indicate client errors (bad request, unauthorized, not found, etc.) and mean there was a problem with the request sent to DynamoDB.

      Incorrect: 5xx codes are server errors, 200 indicates success, and 306 is not generally used to indicate client request errors.
    tags: 
    difficulty: 
    points: 

  - id: q257
    type: multiple_choice
    question: Company B provides an online image recognition service and utilizes SQS to decouple system components for scalability The SQS consumers poll the imaging queue as often as possible to keep end-to-end throughput as high as possible. However, Company B is realizing that polling in tight loops is burning CPU cycles and increasing costs with empty responses. How can Company B reduce the number of empty responses?
    options:
     - text: Set the imaging queue visibility 'Timeout' attribute to 20 seconds.
       is_correct: false
     - text: Set the Imaging queue 'ReceiveMessageWaitTimeSeconds' attribute to 20 seconds.
       is_correct: true
     - text: Set the imaging queue 'MessageRetentionPeriod' attribute to 20 seconds.
       is_correct: false
     - text: Set the 'DelaySeconds' parameter of a message to 20 seconds.
       is_correct: false
    explanation: |
      Correct: Setting 'ReceiveMessageWaitTimeSeconds' enables long polling, which reduces empty responses by allowing the receive call to wait up to the specified time for messages instead of returning immediately.

      Incorrect: VisibilityTimeout affects how long a message remains invisible after being received; MessageRetentionPeriod controls how long messages are stored; DelaySeconds delays individual messages—none of these reduce polling empty responses like long polling does.
    tags: 
    difficulty: 
    points: 

  - id: q258
    type: multiple_choice
    question: An Amazon S3 bucket, 'myawsbucket' is configured with website hosting in Tokyo region, what is the region-specific website endpoint?
    options:
     - text: 'www.myawsbucket.ap-northeast-1.amazonaws.com'
       is_correct: false
     - text: 'myawsbucket.s3-website-ap-northeast-1.amazonaws.com'
       is_correct: true
     - text: 'myawsbucket.amazonaws.com'
       is_correct: false
     - text: 'myawsbucket.tokyo.amazonaws.com'
       is_correct: false
    explanation: |
      Correct: The S3 website endpoint for the Tokyo region (ap-northeast-1) uses the format 'myawsbucket.s3-website-ap-northeast-1.amazonaws.com', which is the correct region-specific website endpoint.

      Incorrect: Other formats with 'www', the global S3 endpoint, or '.tokyo' are not the S3 website endpoint for that region.
    tags: 
    difficulty: 
    points: 

  - id: q259
    type: multiple_choice
    question: You are inserting 1000 new items every second in a DynamoDB table. Once an hour these items are analyzed and then are no longer needed. You need to minimize provisioned throughput, storage, and API calls. Given these requirements, what is the most efficient way to manage these Items after the analysis?
    options:
     - text: Retain the items in a single table.
       is_correct: false
     - text: Delete items individually over a 24 hour period.
       is_correct: false
     - text: Delete the table and create a new table per hour.
       is_correct: true
     - text: Create a new table per hour.
       is_correct: false
    explanation: |
      Correct: Creating and deleting tables on an hourly basis allows you to remove large sets of items efficiently by deleting the table, saving on storage and avoiding many individual delete operations.

      Incorrect: Keeping all items in one table or deleting items individually would consume significant throughput and API calls; creating new tables without deleting them does not solve the removal problem.
    tags: 
    difficulty: 
    points: 

  - id: q260
    type: multiple_choice
    question: You have written an application that uses the Elastic Load Balancing service to spread traffic to several web servers. Your users complain that they are sometimes forced to login again in the middle of using your application, after they have already logged in. This is not behavior you have designed. What is a possible solution to prevent this happening?
    options:
     - text: Use instance memory to save session state.
       is_correct: false
     - text: Use instance storage to save session state.
       is_correct: false
     - text: Use EBS to save session state.
       is_correct: false
     - text: Use ElastiCache to save session state.
       is_correct: true
     - text: Use Glacier to save session slate.
       is_correct: false
    explanation: |
      Correct: Externalizing session state to a shared cache like ElastiCache ensures session data is available to all instances behind the load balancer and prevents users from being logged out when requests are routed to different servers.

      Incorrect: Storing session state in instance memory, local storage, EBS, or Glacier does not provide the shared, low-latency access needed across instances and may lead to lost sessions.
    tags: 
    difficulty: 
    points: 

  - id: q261
    type: multiple_choice
    question: You run an ad-supported photo sharing website using S3 to serve photos to visitors of your site. At some point you find out that other sites have been linking to the photos on your site, causing loss to your business. What is an effective method to mitigate this?
    options:
     - text: Store photos on an EBS volume of the web server.
       is_correct: false
     - text: Remove public read access and use signed URLs with expiry dates.
       is_correct: true
     - text: Use CloudFront distributions for static content.
       is_correct: false
     - text: Block the IPs of the offending websites in Security Groups.
       is_correct: false
    explanation: |
      Correct: Removing public read access and using signed, time-limited URLs ensures only authorized requests can access your objects, preventing hotlinking and unauthorized use.

      Incorrect: Storing on EBS, using CloudFront without access controls, or blocking IPs via security groups are not effective ways to prevent hotlinking; security groups do not control HTTP access from arbitrary web clients.
    tags: 
    difficulty: 
    points: 

  - id: q262
    type: multiple_choice
    question: Which statements about DynamoDB are true? (Choose TWO)
    options:
     - text: DynamoDB uses a pessimistic locking model.
       is_correct: false
     - text: DynamoDB uses optimistic concurrency control.
       is_correct: true
     - text: DynamoDB uses conditional writes for consistency.
       is_correct: true
     - text: DynamoDB restricts item access during reads.
       is_correct: false
     - text: DynamoDB restricts item access during writes.
       is_correct: false
    explanation: |
      Correct: DynamoDB uses optimistic concurrency control and supports conditional writes to help prevent conflicting updates and maintain consistency without using pessimistic locks.

      Incorrect: DynamoDB does not employ a pessimistic locking model by default and does not restrict access in the way implied by the incorrect statements; reads and writes are managed via consistency models and conditional operations.
    tags: 
    difficulty: 
    points: 

  - id: q263
    type: multiple_choice
    question: You are providing AWS consulting services for a company developing a new mobile application that will be leveraging Amazon SNS Mobile Push for push notifications. In order to send direct notification messages to individual devices each device registration identifier or token needs to be registered with SNS; however the developers are not sure of the best way to do this. You advise them to
    options:
     - text: Bulk upload the device tokens contained in a CSV file via the AWS Management Console.
       is_correct: false
     - text: Let the push notification service (e.g. Amazon Device Messaging) handle the registration.
       is_correct: false
     - text: Implement a token vending service to handle the registration.
       is_correct: false
     - text: Call the 'CreatePlatformEndPoint' API function to register multiple device tokens.
       is_correct: true
    explanation: |
      Correct: The 'CreatePlatformEndpoint' API is used to register device tokens (endpoints) with SNS programmatically, which is the recommended way to register and manage device endpoints for push notifications.

      Incorrect: Uploading CSV via the console does not scale, relying solely on the push service does not register tokens with SNS, and while a token vending service is a pattern, the direct recommended API for registering endpoints is CreatePlatformEndpoint.
    tags: 
    difficulty: 
    points: 

  - id: q264
    type: multiple_choice
    question: You are writing to a DynamoDB table and receive the following exception 'ProvisionedThroughputExceededException'. though according to your Cloudwatch metrics for the table, you are not exceeding your provisioned throughput. What could be an explanation for this?
    options:
     - text: You haven't provisioned enough DynamoDB storage instances.
       is_correct: false
     - text: You're exceeding your capacity on a particular 'Range Key'
       is_correct: false
     - text: You're exceeding your capacity on a particular 'Hash Key'
       is_correct: true
     - text: You're exceeding your capacity on a particular 'Sort Key'
       is_correct: false
     - text: You haven't configured DynamoDB Auto Scaling triggers.
       is_correct: false
    explanation: |
      Correct: Throttling can occur due to a hot partition caused by a heavily used partition key (hash key); even if the table's aggregate capacity is available, a single partition can exceed its allocated throughput.

      Incorrect: The other options do not explain the common cause of this exception; storage instances, range/sort key limits, or missing auto scaling configuration are not the primary reason in this scenario.
    tags: 
    difficulty: 
    points: 

  - id: q265
    type: multiple_choice
    question: If an application is storing hourly log files from thousands of instances from a high traffic web site, which naming scheme would give optimal performance on S3?
    options:
     - text: Sequential.
       is_correct: false
     - text: 'instanceID_log-HH-DD-MM-YYYY'
       is_correct: true
     - text: 'instanceIDLog-YYYY-MM-DD-HH'
       is_correct: false
     - text: 'HH-DD-MM-YYYY-log_instanceID'
       is_correct: false
     - text: 'YYYY-MM-DD-HH-logInstanceID'
       is_correct: false
    explanation: |
      Correct: Using a prefix that varies by instance (for example starting with 'instanceID_...') helps distribute object keys across S3 partitions and avoids hotspots, improving performance for many parallel writers.

      Incorrect: Sequential names or names that start with identical timestamp prefixes for many instances can cause S3 prefix hotspots, degrading performance.
    tags: 
    difficulty: 
    points: 

  - id: q266
    type: multiple_choice
    question: Which of the following statements about SQS is true?
    options:
     - text: Messages will be delivered exactly once and messages will be delivered in First in, First out order.
       is_correct: false
     - text: Messages will be delivered exactly once and message delivery order is indeterminate.
       is_correct: false
     - text: Messages will be delivered one or more times and messages will be delivered in First in, First out order.
       is_correct: false
     - text: Messages will be delivered one or more times and message delivery order is indeterminate.
       is_correct: true
    explanation: |
      Correct: Standard SQS delivers messages at-least-once (one or more times) and does not guarantee ordering; FIFO queues provide ordering and deduplication but are a different queue type.

      Incorrect: The other statements mix guarantee types incorrectly; exactly-once delivery and FIFO ordering only apply to FIFO queues under specific conditions, not to standard SQS behavior.
    tags: 
    difficulty: 
    points: 

  - id: q267
    type: multiple_choice
    question: A corporate web application is deployed within an Amazon VPC, and is connected to the corporate data center via IPSec VPN. The application must authenticate against the on-premise LDAP server. Once authenticated, logged-in users can only access an S3 keyspace specific to the user. Which two approaches can satisfy the objectives? (Choose TWO)
    options:
     - text: The application authenticates against LDAP. The application then calls the IAM Security Service to login to IAM using the LDAP credentials. The application can use the 1AM temporary credentials to access the appropriate S3 bucket.
       is_correct: false
     - text: The application authenticates against LDAP, and retrieves the name of an IAM role associated with the user. The application then calls the IAM Security Token Service to assume that IAM Role. The application can use the temporary credentials to access the appropriate S3 bucket.
       is_correct: true
     - text: The application authenticates against IAM Security Token Service using the LDAP credentials. The application uses those temporary AWS security credentials to access the appropriate S3 bucket.
       is_correct: false
     - text: Develop an identity broker which authenticates against LDAP, and then calls IAM Security Token Service to get IAM federated user credentials. The application calls the identity broker to get IAM federated user credentials with access to the appropriate S3 bucket.
       is_correct: true
     - text: Develop an identity broker which authenticates against IAM Security Token Service to assume an IAM Role to get temporary AWS security credentials. The application calls the identity broker to get AWS temporary security credentials with access to the appropriate S3 bucket.
       is_correct: false
    explanation: |
      Correct: Two valid approaches are: authenticate against LDAP and then assume an IAM role via STS mapped to the user, or implement an identity broker that authenticates against LDAP and requests federated credentials from STS to provide scoped access to S3.

      Incorrect: Options that imply authenticating directly to STS with LDAP credentials or calling a generic "IAM Security Service" with LDAP credentials are incorrect because STS requires proper credential mappings and is typically accessed via a broker or role assumption flow.
    tags: 
    difficulty: 
    points: 

  - id: q268
    type: multiple_choice
    question: Company C is currently hosting their corporate site in an Amazon S3 bucket with Static Website Hosting enabled. Currently, when visitors go to 'http://www.companyc.com' the 'index.html' page is returned. Company C now would like a new page welcome.html to be returned when a visitor enters 'http://www.companyc.com' in the browser. Which of the following steps will allow Company C to meet this requirement? (Choose TWO)
    options:
     - text: Upload an html page named welcome.html to their S3 bucket.
       is_correct: true
     - text: Create a welcome subfolder in their S3 bucket.
       is_correct: false
     - text: Set the Index Document property to welcome.html.
       is_correct: true
     - text: Move the 'index.html' page to a welcome subfolder.
       is_correct: false
     - text: Set the Error Document property to welcome.html.
       is_correct: false
    explanation: |
      Correct: You must upload 'welcome.html' to the bucket and set the bucket's Website Hosting Index Document to 'welcome.html' so it will be served as the root page.

      Incorrect: Creating subfolders or moving the existing index without changing the index document will not make 'welcome.html' the root document; setting the Error Document is for error responses, not the index.
    tags: 
    difficulty: 
    points: 

  - id: q269
    type: multiple_choice
    question: What type of block cipher does Amazon S3 offer for server side encryption?
    options:
     - text: Triple DES.
       is_correct: false
     - text: Advanced Encryption Standard.
       is_correct: true
     - text: Blowfish.
       is_correct: false
     - text: RC5.
       is_correct: false
    explanation: |
      Correct: Amazon S3 server-side encryption uses the Advanced Encryption Standard (AES), commonly AES-256, as the block cipher for encrypting objects.

      Incorrect: Triple DES, Blowfish and RC5 are not the standard algorithms used by S3 for server-side encryption; AES is the recommended and implemented algorithm.
    tags: 
    difficulty: 
    points: 

  - id: q270
    type: multiple_choice
    question: A Development team wants to instrument their code to provide more detailed information to AWS X-Ray than simple outgoing and incoming requests. This will generate large amounts of data, so the Development team wants to implement indexing so they can filter the data. What should the Development team do to achieve this?
    options:
     - text: Add annotations to the segment document and the code.
       is_correct: true
     - text: Add metadata to the segment document and the code.
       is_correct: false
     - text: Configure the necessary X-Ray environment variables.
       is_correct: false
     - text: Install required plugins for the appropriate AWS SDK.
       is_correct: false
    explanation: |
      Correct: Add annotations to segments in X-Ray; annotations are indexed and let you filter and query traces efficiently, so adding them in code and segment documents enables the desired indexing.

      Incorrect: Metadata is stored but not indexed for filtering; environment variables and SDK plugins do not by themselves provide indexable fields for filtering traces.
    tags: 
    difficulty: 
    points: 

  - id: q271
    type: multiple_choice
    question: A team of Developers must migrate an application running inside an AWS Elastic Beanstalk environment from a Classic Load Balancer to an Application Load Balancer. Which steps should be taken to accomplish the task using the AWS Management Console?
    options:
     - text: 1. Update the application code in the existing deployment. 2. Select a new load balancer type before running the deployment. 3. Deploy the new version of the application code to the environment.
       is_correct: true
     - text: 1. Create a new environment with the same configurations except for the load balancer type. 2. Deploy the same application version as used in the original environment. 3. Run the 'swap-environment-cnames' action.
       is_correct: false
     - text: 1. Clone the existing environment, changing the associated load balancer type. 2. Deploy the same application version as used in the original environment. 3. Run the 'swap-environment-cnames' action.
       is_correct: false
     - text: 1. Edit the environment definitions in the existing deployment. 2. Change the associated load balancer type according to the requirements. 3. Rebuild the environment with the new load balancer type.
       is_correct: false
    explanation: |
      Correct: In the Elastic Beanstalk console you can change the environment configuration to select a new load balancer type and then deploy a new version; updating the configuration and redeploying is the straightforward approach.

      Incorrect: The other choices describe more complex or incorrect workflows (cloning and swapping CNAMEs or invalid edits) rather than the direct configuration update in the environment.
    tags: 
    difficulty: 
    points: 

  - id: q272
    type: multiple_choice
    question: A company needs a version control system for collaborative software development. Features of the system must include the following Support for batches of changes across multiple files. Parallel branching Version tracking. Which AWS service will meet these requirements?
    options:
     - text: AWS CodePipeline.
       is_correct: false
     - text: Amazon S3.
       is_correct: false
     - text: AWS Code Build.
       is_correct: false
     - text: AWS CodeCommit.
       is_correct: true
    explanation: |
      Correct: AWS CodeCommit is a managed source control service (Git) that supports commits across multiple files, branching and version history for collaborative development.

      Incorrect: CodePipeline is for CI/CD orchestration, CodeBuild handles builds, and S3 is object storage—none provide distributed version control like CodeCommit.
    tags: 
    difficulty: 
    points: 

  - id: q273
    type: multiple_choice
    question: A company is using continuous integration and continuous delivery systems. A Developer now needs to automate a software package deployment to both Amazon EC2 instances and virtual servers running on-premises. Which AWS service should be used to accomplish this?
    options:
     - text: AWS CodePipeline.
       is_correct: false
     - text: AWS CodeBuild.
       is_correct: false
     - text: AWS Elastic Beanstalk.
       is_correct: false
     - text: AWS CodeDeploy.
       is_correct: true
    explanation: |
      Correct: AWS CodeDeploy supports automated deployments to EC2 instances and on-premises servers, making it the appropriate service for this requirement.

      Incorrect: CodePipeline orchestrates pipelines, CodeBuild runs builds, and Elastic Beanstalk manages application environments; they do not specifically handle coordinated deployments to both EC2 and on-premises servers like CodeDeploy does.
    tags: 
    difficulty: 
    points: 

  - id: q274
    type: multiple_choice
    question: A Developer created a new AWS account and must create a scalable AWS Lambda function that meets the following requirements for concurrent execution Average execution time of 100 seconds 50 requests per second. Which step must be taken prior to deployment to prevent errors?
    options:
     - text: Implement dead-letter queues to capture invocation errors.
       is_correct: false
     - text: Add an event source from Amazon API Gateway to the Lambda function.
       is_correct: false
     - text: Implement error handling within the application code.
       is_correct: false
     - text: Contact AWS Support to increase the concurrent execution limits.
       is_correct: true
    explanation: |
      Correct: To support 50 requests/sec with 100s average execution, the required concurrency may exceed the account's default limit; you must request a concurrency limit increase from AWS Support before deployment.

      Incorrect: Dead-letter queues and error handling are good practices but do not address account concurrency limits; adding an event source does not increase concurrency quotas.
    tags: 
    difficulty: 
    points: 

  - id: q275
    type: multiple_choice
    question: A Developer is building a three-tier web application that should be able to handle a minimum of 5000 requests per minute. Requirements state that the web tier should be completely stateless while the application maintains session state for the users. How can session data be externalized, keeping latency at the LOWEST possible value?
    options:
     - text: Create an Amazon RDS instance, then implement session handling at the application level to leverage a database inside the RDS database instance for session data storage.
       is_correct: false
     - text: Implement a shared file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the shared file system for session data storage.
       is_correct: false
     - text: Create an Amazon ElastiCache Memcached cluster, then implement session handling at the application level to leverage the cluster for session data storage.
       is_correct: true
     - text: Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage.
       is_correct: false
    explanation: |
      Correct: Using ElastiCache (Memcached) provides in-memory session storage with very low latency, which is ideal for externalizing session state while keeping the web tier stateless and meeting performance requirements.

      Incorrect: RDS or a shared file system introduces higher latency; DynamoDB is scalable but typically has higher latency than an in-memory cache for session access.
    tags: 
    difficulty: 
    points: 

  - id: q276
    type: multiple_choice
    question: An Amazon DynamoDB table uses a Global Secondary Index (GSI) to support read queries. The primary table is write-heavy, whereas the GSI is used for read operations. Looking at Amazon CloudWatch metrics, the Developer notices that write operations to the primary table are throttled frequently under heavy write activity. However, write capacity units to the primary table are available and not fully consumed. Why is the table being throttled?
    options:
     - text: The GSI write capacity units are underprovisioned.
       is_correct: true
     - text: There are not enough read capacity units on the primary table.
       is_correct: false
     - text: Amazon DynamoDB Streams is not enabled on the table.
       is_correct: false
     - text: A large write operation is being performed against another table.
       is_correct: false
    explanation: |
      Correct: Updates to the primary table must also be written to the GSI; if the GSI's write capacity is underprovisioned, writes to the primary table can be throttled even while the table's own WCU appear available.

      Incorrect: Read capacity on the primary table, missing Streams, or activity on another table do not explain the observed throttling; the GSI write capacity is the relevant factor.
    tags: 
    difficulty: 
    points: 

  - id: q277
    type: multiple_choice
    question: A company runs an e-commerce website that uses Amazon DynamoDB where pricing for items is dynamically updated in real time. At any given time, multiple updates may occur simultaneously for pricing information on a particular product. This is causing the original editor's changes to be overwritten without a proper review process. Which DynamoDB write option should be selected to prevent this overwriting?
    options:
     - text: Concurrent writes.
       is_correct: false
     - text: Conditional writes.
       is_correct: true
     - text: Atomic writes.
       is_correct: false
     - text: Batch writes.
       is_correct: false
    explanation: |
      Correct: Conditional writes allow you to specify a condition (for example only update if the current value matches an expected value) which prevents accidental overwrites in concurrent update scenarios.

      Incorrect: Labels like "concurrent" or "atomic" in the options are not the specific DynamoDB mechanism; batch writes group operations but do not prevent conflicting concurrent updates without conditions.
    tags: 
    difficulty: 
    points: 

  - id: q278
    type: multiple_choice
    question: A Developer has been asked to create an AWS Lambda function that is triggered any time updates are made to items in an Amazon DynamoDB table. The function has been created, and appropriate permissions have been added to the Lambda execution role. Amazon DynamoDB streams have been enabled for the table, but the function is still not being triggered. Which option would enable DynamoDB table updates to trigger the Lambda function?
    options:
     - text: Change the 'StreamViewType' parameter value to 'NEW_AND_OLD_IMAGES' for the DynamoDB table.
       is_correct: false
     - text: Configure event source mapping for the Lambda function.
       is_correct: true
     - text: Map an Amazon SNS topic to the DynamoDB streams.
       is_correct: false
     - text: Increase the maximum execution time (timeout) setting of the Lambda function.
       is_correct: false
    explanation: |
      Correct: You must configure an event source mapping between the DynamoDB stream and the Lambda function so that stream records trigger Lambda invocations.

      Incorrect: Changing StreamViewType controls what data is captured but does not wire the stream to Lambda; mapping an SNS topic or increasing timeout does not create the required event source mapping.
    tags: 
    difficulty: 
    points: 

  - id: q279
    type: multiple_choice
    question: A company is running a Docker application on Amazon ECS. The application must scale based on user load in the last 15 seconds. How should a Developer instrument the code so that the requirement can be met?
    options:
     - text: Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds.
       is_correct: false
     - text: Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds.
       is_correct: true
     - text: Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds.
       is_correct: false
     - text: Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds.
       is_correct: false
    explanation: |
      Correct: High-resolution CloudWatch metrics allow publishing at sub-minute intervals (as low as 1 second); publishing every 5 seconds gives the granularity needed to base scaling on the last 15 seconds of activity.

      Incorrect: Standard-resolution metrics do not support the required high frequency and 30-second publishing is too coarse for a 15-second scaling requirement.
    tags: 
    difficulty: 
    points: 

  - id: q280
    type: multiple_choice
    question: A company needs to ingest terabytes of data each hour from thousands of sources that are delivered almost continually throughout the day. The volume of messages generated varies over the course of the day. Messages must be delivered in real time for fraud detection and live operational dashboards. Which approach will meet these requirements?
    options:
     - text: Send the messages to an Amazon SQS queue, then process the messages by using a fleet of Amazon EC2 instances.
       is_correct: false
     - text: Use the Amazon S3 API to write messages to an S3 bucket, then process the messages by using Amazon Redshift.
       is_correct: false
     - text: Use AWS Data Pipeline to automate the movement and transformation of data.
       is_correct: false
     - text: Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages.
       is_correct: true
    explanation: |
      Correct: Amazon Kinesis Data Streams is designed for real-time ingestion of large volumes of streaming data from many producers and provides low-latency processing suitable for fraud detection and live dashboards.

      Incorrect: SQS, S3, and Data Pipeline do not provide the same level of low-latency real-time streaming and scaling for thousands of concurrent producers as Kinesis.
    tags: 
    difficulty: 
    points: 

  - id: q281
    type: multiple_choice
    question: A Developer accesses AWS CodeCommit over SSH. The SSH keys configured to access AWS CodeCommit are tied to a user with the following permissions. The Developer needs to create/delete branches. Which specific IAM permissions need to be added, based on the principle of least privilege?
    img: images/question281.jpg
    options:
     - text: '"codecommit:CreateBranch" "codecommit:DeleteBranch"'
       is_correct: true
     - text: '"codecommit:Put*"'
       is_correct: false
     - text: '"codecommit:Update*"'
       is_correct: false
     - text: '"codecommit:*"'
       is_correct: false
    explanation: |
      Correct: The least-privilege approach is to add the specific actions 'codecommit:CreateBranch' and 'codecommit:DeleteBranch' which are exactly what is required to create and delete branches.

      Incorrect: Broad permissions like 'Put*', 'Update*' or 'codecommit:*' grant excessive access and violate the principle of least privilege.
    tags: 
    difficulty: 
    points: 

  - id: q282
    type: multiple_choice
    question: An AWS Lambda function must access an external site by using a regularly rotated user name and password. These items must be kept securely and cannot be stored in the function code. What combination of AWS services can be used to accomplish this? (Choose TWO)
    options:
     - text: AWS Certificate Manager (ACM).
       is_correct: false
     - text: AWS Systems Manager Parameter Store.
       is_correct: true
     - text: AWS Trusted Advisor.
       is_correct: false
     - text: AWS KMS.
       is_correct: true
     - text: Amazon GuardDuty.
       is_correct: false
    explanation: |
      Correct: AWS Systems Manager Parameter Store (with SecureString parameters) can store secrets and AWS KMS can be used to encrypt them; together they provide secure storage and encryption for rotated credentials used by Lambda.

      Incorrect: ACM, Trusted Advisor, and GuardDuty do not provide managed secret storage and encryption for application credentials in the same way as Parameter Store plus KMS.
    tags: 
    difficulty: 
    points: 

  - id: q283
    type: multiple_choice
    question: A Developer is trying to deploy a serverless application using AWS CodeDeploy. The application was updated and needs to be redeployed. What file does the Developer need to update to push that change through CodeDeploy?
    options:
     - text: 'dockerrun.aws.json'
       is_correct: false
     - text: 'buildspec.yml'
       is_correct: false
     - text: 'appspec.yml'
       is_correct: true
     - text: 'ebextensions.config'
       is_correct: false
    explanation: |
      Correct: 'appspec.yml' is the file used by CodeDeploy to define deployment actions and hooks; updating it (or ensuring it is present and correct) is required for CodeDeploy to apply the deployment.

      Incorrect: 'dockerrun.aws.json' is used for Elastic Beanstalk Docker configs, 'buildspec.yml' is for CodeBuild, and '.ebextensions' is for Beanstalk—none of these control CodeDeploy deployments.
    tags: 
    difficulty: 
    points: 

  - id: q284
    type: multiple_choice
    question: A Developer is working on an application that handles 10MB documents that contain highly-sensitive data. The application will use AWS KMS to perform clientside encryption. What steps must be followed?
    options:
     - text: Invoke the Encrypt API passing the plaintext data that must be encrypted, then reference the customer managed key ARN in the 'KeyId' parameter.
       is_correct: false
     - text: Invoke the 'GenerateRandom' API to get a data encryption key, then use the data encryption key to encrypt the data.
       is_correct: false
     - text: Invoke the 'GenerateDataKey' API to retrieve the encrypted version of the data encryption key to encrypt the data.
       is_correct: false
     - text: Invoke the 'GenerateDataKey' API to retrieve the plaintext version of the data encryption key to encrypt the data.
       is_correct: true
    explanation: |
      Correct: The recommended pattern is to call 'GenerateDataKey' to receive both the plaintext data encryption key (for local encryption) and the encrypted data key for storage; this enables client-side encryption of large objects without sending plaintext to KMS.

      Incorrect: Calling Encrypt with the entire document is not suitable for large files; GenerateRandom is not the standard DEK workflow, and obtaining only the encrypted DEK without the plaintext key would not allow local encryption.
    tags: 
    difficulty: 
    points: 

  - id: q285
    type: multiple_choice
    question: A Developer is building a web application that uses Amazon API Gateway to expose an AWS Lambda function to process requests from clients. During testing, the Developer notices that the API Gateway times out even though the Lambda function finishes under the set time limit. Which of the following API Gateway metrics in Amazon CloudWatch can help the Developer troubleshoot the issue? (Choose TWO)
    options:
     - text: CacheHitCount.
       is_correct: false
     - text: IntegrationLatency.
       is_correct: true
     - text: CacheMissCount.
       is_correct: false
     - text: Latency.
       is_correct: true
     - text: Count.
       is_correct: false
    explanation: |
      Correct: 'IntegrationLatency' measures the time taken by the backend integration (for example, Lambda) to respond to API Gateway, and 'Latency' measures the total client-perceived latency; both help diagnose timeouts.

      Incorrect: CacheHitCount, CacheMissCount and Count do not directly indicate backend integration latency or end-to-end request latency relevant to timeouts.
    tags: 
    difficulty: 
    points: 

  - id: q286
    type: multiple_choice
    question: A company needs to distribute firmware updates to its customers around the world. Which service will allow easy and secure control of the access to the downloads at the lowest cost?
    options:
     - text: Use Amazon CloudFront with signed URLs for Amazon S3.
       is_correct: true
     - text: Create a dedicated Amazon CloudFront Distribution for each customer.
       is_correct: false
     - text: Use Amazon CloudFront with AWS Lambda@Edge.
       is_correct: false
     - text: Use Amazon API Gateway and AWS Lambda to control access to an S3 bucket.
       is_correct: false
    explanation: |
      Correct: CloudFront with signed URLs in front of S3 provides global low-latency delivery and controlled access to downloads at low cost, making it an efficient choice for firmware distribution.

      Incorrect: Creating a distribution per customer is impractical and costly; Lambda@Edge or API Gateway add complexity and extra cost compared to signed URLs with CloudFront.
    tags: 
    difficulty: 
    points: 

  - id: q287
    type: multiple_choice
    question: An application writes items to an Amazon DynamoDB table. As the application scales to thousands of instances, calls to the DynamoDB API generate occasional 'ThrottlingException' errors. The application is coded in a language incompatible with the AWS SDK. How should the error be handled?
    options:
     - text: Add exponential backoff to the application logic.
       is_correct: true
     - text: Use Amazon SQS as an API message bus.
       is_correct: false
     - text: Pass API calls through Amazon API Gateway.
       is_correct: false
     - text: Send the items to DynamoDB through Amazon Kinesis Data Firehose.
       is_correct: false
    explanation: |
      Correct: Implementing exponential backoff for retries is the recommended approach to handle DynamoDB throttling errors, reducing retry pressure and allowing operations to succeed as capacity becomes available.

      Incorrect: While SQS, API Gateway, or Firehose could be part of alternative architectures, the direct and recommended method to handle throttling errors is exponential backoff in the client retry logic.
    tags: 
    difficulty: 
    points: 

  - id: q288
    type: multiple_choice
    question: An e-commerce web application that shares session state on-premises is being migrated to AWS. The application must be fault tolerant, natively highly scalable, and any service interruption should not affect the user experience. What is the best option to store the session state?
    options:
     - text: Store the session state in Amazon ElastiCache.
       is_correct: true
     - text: Store the session state in Amazon CloudFront.
       is_correct: false
     - text: Store the session state in Amazon S3.
       is_correct: false
     - text: Enable session stickiness using elastic load balancers.
       is_correct: false
    explanation: |
      Correct: Amazon ElastiCache (Redis or Memcached) offers in-memory session storage with low latency and options for replication and high availability, making it well-suited for session state in scalable applications.

      Incorrect: CloudFront is not a session store, S3 has higher latency and is not ideal for session access, and ELB stickiness does not provide the resilience and scalability required to protect sessions from instance failures.
    tags: 
    difficulty: 
    points: 

  - id: q289
    type: multiple_choice
    question: A Developer is creating a template that uses AWS CloudFormation to deploy an application. This application is serverless and uses Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. Which tool should the Developer use to define simplified syntax for expressing serverless resources?
    options:
     - text: CloudFormation serverless intrinsic functions.
       is_correct: false
     - text: AWS serverless express.
       is_correct: false
     - text: An AWS serverless application model.
       is_correct: true
     - text: A CloudFormation serverless plugin.
       is_correct: false
    explanation: |
      Correct: The AWS Serverless Application Model (SAM) provides a simplified syntax for declaring serverless resources (API Gateway, Lambda, DynamoDB) in CloudFormation templates.

      Incorrect: The other options are not the standard tool for simplified serverless syntax; SAM is the established model for this purpose.
    tags: 
    difficulty: 
    points: 

  - id: q290
    type: multiple_choice
    question: A Developer has a stateful web server on-premises that is being migrated to AWS. The Developer must have greater elasticity in the new design. How should the Developer re-factor the application to make it more elastic? (Choose TWO)
    options:
     - text: Use pessimistic concurrency on Amazon DynamoDB.
       is_correct: false
     - text: Use Amazon CloudFront with an Auto Scaling group.
       is_correct: false
     - text: Use Amazon CloudFront with an AWS Web Application Firewall.
       is_correct: false
     - text: Store session state data in an Amazon DynamoDB table.
       is_correct: true
     - text: Use an ELB with an Auto Scaling group.
       is_correct: true
    explanation: |
      Correct: Moving session state to a scalable managed store like DynamoDB and using an ELB with an Auto Scaling group for the web tier are effective refactors to increase elasticity.

      Incorrect: Pessimistic concurrency in DynamoDB, CloudFront or WAF alone do not provide the primary elasticity improvements required; the two selected changes directly address session sharing and instance scaling.
    tags: 
    difficulty: 
    points: 

  - id: q291
    type: multiple_choice
    question: A Developer must analyze performance issues with production-distributed applications written as AWS Lambda functions. These distributed Lambda applications invoke other components that make up the applications. How should the Developer identify and troubleshoot the root cause of the performance issues in production?
    options:
     - text: Add logging statements to the Lambda functions, then use Amazon CloudWatch to view the logs.
       is_correct: false
     - text: Use AWS Cloud Trail and then examine the logs.
       is_correct: false
     - text: Use AWS X-Ray, then examine the segments and errors.
       is_correct: true
     - text: Run Amazon Inspector agents and then analyze performance.
       is_correct: false
    explanation: |
      Correct: AWS X-Ray provides distributed tracing that helps correlate calls across components, identify latencies and errors, and pinpoint root causes in production systems.

      Incorrect: CloudWatch Logs are useful but do not provide distributed tracing easily; CloudTrail is for API auditing, and Inspector is for security assessments, not distributed performance tracing.
    tags: 
    difficulty: 
    points: 

  - id: q292
    type: multiple_choice
    question: A Developer wants to debug an application by searching and filtering log data. The application logs are stored in Amazon CloudWatch Logs. The Developer creates a new metric filter to count exceptions in the application logs. However, no results are returned from the logs. What is the reason that no filtered results are being returned?
    options:
     - text: A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC.
       is_correct: false
     - text: CloudWatch Logs only publishes metric data for events that happen after the filter is created.
       is_correct: true
     - text: The log group for CloudWatch Logs should be first streamed to Amazon Elasticsearch Service before metric filtering returns the results.
       is_correct: false
     - text: Metric data points for logs groups can be filtered only after they are exported to an Amazon S3 bucket.
       is_correct: false
    explanation: |
      Correct: CloudWatch metric filters start producing metrics only for log events that occur after the filter is created; they do not retroactively create metrics for past log entries.

      Incorrect: You do not need a VPC endpoint for basic metric filtering, and exporting to Elasticsearch or S3 is not required for metric filters to work.
    tags: 
    difficulty: 
    points: 

  - id: q293
    type: multiple_choice
    question: To include objects defined by the AWS Serverless Application Model (SAM) in an AWS CloudFormation template, in addition to 'Resources', what section MUST be included in the document root?
    options:
     - text: 'Conditions'
       is_correct: false
     - text: 'Globals'
       is_correct: false
     - text: 'Transform'
       is_correct: true
     - text: 'Properties'
       is_correct: false
    explanation: |
      Correct: The 'Transform' section must be present in the template root to indicate the SAM transform, which enables the use of SAM shorthand resources.

      Incorrect: 'Conditions', 'Globals', and 'Properties' are optional or unrelated to enabling SAM; only 'Transform' is required for SAM constructs.
    tags: 
    difficulty: 
    points: 

  - id: q294
    type: multiple_choice
    question: A company is using Amazon RDS MySQL instances for its application database tier and Apache Tomcat servers for its web tier. Most of the database queries from web applications are repeated read requests. Use of which AWS service would increase in performance by adding in-memory store for repeated read queries?
    options:
     - text: Amazon RDS Multi-AZ.
       is_correct: false
     - text: Amazon SQS.
       is_correct: false
     - text: Amazon ElastiCache.
       is_correct: true
     - text: Amazon RDS read replica.
       is_correct: false
    explanation: |
      Correct: Amazon ElastiCache provides an in-memory cache (Redis or Memcached) which reduces latency and database load for frequently-read data.

      Incorrect: Multi-AZ improves availability, read replicas help scale reads but do not provide the same low-latency cache benefits, and SQS is not a cache.
    tags: 
    difficulty: 
    points: 

  - id: q295
    type: multiple_choice
    question: A Developer is investigating an issue whereby certain requests are passing through an Amazon API Gateway endpoint /MyAPI, but the requests do not reach the AWS Lambda function backing /MyAPI. The Developer found that a second Lambda function sometimes runs at maximum concurrency allowed for the given AWS account. How can the Developer address this issue?
    options:
     - text: Manually reduce the concurrent execution limit at the account level.
       is_correct: false
     - text: Add another API Gateway stage for /MyAPI, and shard the requests.
       is_correct: false
     - text: Configure the second Lambda function's concurrency execution limit.
       is_correct: true
     - text: Reduce the throttling limits in the API Gateway /MyAPI endpoint
       is_correct: false
    explanation: |
      Correct: Setting a reserved concurrency limit on the second Lambda function prevents it from consuming the entire account concurrency pool and ensures capacity remains for other functions.

      Incorrect: Reducing account-level limits or adding stages does not properly isolate concurrency, and changing API Gateway throttling does not address another function consuming concurrency.
    tags: 
    difficulty: 
    points: 

  - id: q296
    type: multiple_choice
    question: A company is migrating a single-server, on-premises web application to AWS. The company intends to use multiple servers behind an Elastic Load Balancer (ELB) to balance the load, and will also store session data in memory on the web server. The company does not want to lose that session data if a server fails or goes offline, and it wants to minimize user's downtime. Where should the company move session data to MOST effectively reduce downtime and make users' session data more fault tolerant?
    options:
     - text: An Amazon ElastiCache for Redis cluster.
       is_correct: true
     - text: A second Amazon EBS volume.
       is_correct: false
     - text: The web server's primary disk.
       is_correct: false
     - text: An Amazon EC2 instance dedicated to session data.
       is_correct: false
    explanation: |
      Correct: ElastiCache for Redis provides in-memory session storage with options for replication and high availability, making sessions fault tolerant and reducing downtime.

      Incorrect: EBS, local disk, or a dedicated EC2 instance present single points of failure or higher latency compared to a managed cache with replication.
    tags: 
    difficulty: 
    points: 

  - id: q297
    type: multiple_choice
    question: A Developer created configuration specifications for an AWS Elastic Beanstalk application in a file named healthcheckurl.yaml in the '.ebextensions/directory' of their application source bundle. The file contains the following - After the application launches, the health check is not being run on the correct path, even though it is valid. What can be done to correct this configuration file?
    img: images/question297.jpeg
    options:
     - text: Convert the file to JSON format.
       is_correct: false
     - text: Rename the file to a '.config' extension.
       is_correct: true
     - text: Change the configuration section from 'options_settings' to resources.
       is_correct: false
     - text: Change the namespace of the option settings to a custom namespace.
       is_correct: false
    explanation: |
      Correct: Files inside '.ebextensions' must have a '.config' extension to be processed by Elastic Beanstalk; renaming 'healthcheckurl.yaml' to 'healthcheckurl.config' will allow the settings to be applied.

      Incorrect: Converting to JSON or changing sections/namespace will not fix the issue if the file extension is incorrect; the key problem is that the file was not named with the '.config' extension.
    tags: 
    difficulty: 
    points: 

  - id: q298
    type: multiple_choice
    question: A Developer has created a Lambda function and is finding that the function is taking longer to complete than expected. After some debugging, the Developer has discovered that increasing compute capacity would improve performance. How can the Developer increase the Lambda compute resources?
    options:
     - text: Run on a larger instance size with more compute capacity.
       is_correct: false
     - text: Increase the maximum execution time.
       is_correct: false
     - text: Specify a larger compute capacity when calling the Lambda function.
       is_correct: false
     - text: Increase the allocated memory for the Lambda function.
       is_correct: true
    explanation: |
      Correct: Increasing the memory allocation for a Lambda function also increases the proportionate CPU available to the function, which can improve compute performance.

      Incorrect: Lambda does not expose instance sizes to choose from, increasing timeout does not add CPU, and you cannot specify compute separately at invocation time.
    tags: 
    difficulty: 
    points: 

  - id: q299
    type: multiple_choice
    question: An e-commerce site allows returning users to log in to display customized web pages. The workflow is shown in the image below. An application is running on EC2 instances. Amazon RDS is used for the database that stores user accounts and preferences. The website freezes or is slow to load while waiting for the login step to complete. The remaining components of the site are well-optimized. Which of the following techniques will resolve this issue? (Select TWO)
    img: images/question299.jpeg
    options:
     - text: Implement the user login page as an asynchronous Lambda function.
       is_correct: false
     - text: Use Amazon ElastiCache for MemCached to cache user data.
       is_correct: true
     - text: Use Amazon Application Load Balancer to load balance the traffic to the website.
       is_correct: false
     - text: Call the database asynchronously so the code can continue executing.
       is_correct: true
     - text: Batch login requests from hundreds of users together as a single read request to the database.
       is_correct: false
    explanation: |
      Correct: Caching user data in ElastiCache reduces database read latency, and making database calls asynchronous allows the application to continue processing without blocking while waiting for the DB response.

      Incorrect: Moving login to Lambda or changing the load balancer does not directly address database latency; batching hundreds of login requests into one read is impractical and insecure.
    tags: 
    difficulty: 
    points: 

  - id: q300
    type: multiple_choice
    question: A Developer is building a mobile application and needs any update to user profile data to be pushed to all devices accessing the specific identity. The Developer does not want to manage a back end to maintain the user profile data. What is the MOST efficient way for the Developer to achieve these requirements using Amazon Cognito?
    options:
     - text: Use Cognito federated identities.
       is_correct: false
     - text: Use a Cognito user pool.
       is_correct: false
     - text: Use Cognito Sync.
       is_correct: true
     - text: Use Cognito events.
       is_correct: false
    explanation: |
      Correct: Cognito Sync (or the equivalent data sync features) allows synchronizing user profile data across devices associated with an identity without requiring a custom backend, enabling updates to be propagated to all devices.

      Incorrect: Federated identities and user pools handle authentication/authorization but do not by themselves provide cross-device data sync; events are not a substitute for the sync service.
    tags: 
    difficulty: 
    points: 
