questions:
  - id: q301
    type: multiple_choice
    question: |
      A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share.

      The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days.

      Which AWS solution will meet these requirements?
    options:
     - text: AWS DataSync
       is_correct: true
     - text: AWS Direct Connect
       is_correct: false
     - text: Amazon S3 Transfer Acceleration
       is_correct: false
     - text: AWS Snowball
       is_correct: false




    explanation: |
      Correct: Using separate SQS queues for each quote type allows for efficient, organized processing and ensures that no messages are lost. AWS Lambda provides serverless, scalable processing with minimal maintenance, meeting the requirements for operational efficiency and reliability.
      Correct: S3 Storage Lens provides advanced metrics and insights into S3 bucket usage, making it easy to identify buckets that are rarely accessed or unused. This solution is fully managed and requires minimal operational effort compared to manual log analysis or custom scripts.
      Correct: Launching all EC2 instances in the same Availability Zone and using a placement group with the cluster strategy ensures low-latency, high-throughput communication between instances. This is ideal for workloads that require tightly-coupled node communication, such as HPC or distributed databases.
      Correct: AWS Application Auto Scaling with target tracking policies allows ECS services to automatically scale based on CloudWatch metrics, ensuring the right number of tasks are running to meet demand. This is the most efficient and automated way to handle variable workloads in ECS.
      Correct: AWS DataSync is a fully managed data transfer service that can be used to simplify, automate, and accelerate copying large amounts of data between on-premises storage systems and Amazon S3, Amazon EFS, or Amazon FSx for Windows File Server.
      Incorrect: 
        AWS Direct Connect provides dedicated connectivity but does not offer bandwidth throttling or managed migration services.
        Amazon S3 Transfer Acceleration optimizes uploads to S3 but is not designed for migrating to FSx or controlling bandwidth usage.
        AWS Snowball is for physical data transfer and may not meet the 5-day timeline or bandwidth control requirements.

  - id: q302
    type: multiple_choice
    question: |
      A company wants to create a mobile app that allows users to stream slow-motion video clips on their mobile devices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format.

      Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while minimizing operational overhead.

      Which combination of solutions will meet these requirements? (Choose two.)
    options:
     - text: Deploy Amazon CloudFront for content delivery and caching.
       is_correct: true
     - text: Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.
       is_correct: true
     - text: Store the videos in Amazon S3 Glacier for cost savings.
       is_correct: false
     - text: Use AWS Elemental MediaStore for video streaming.
       is_correct: false



    explanation: |
      Correct: Deploying CloudFront in front of the S3 bucket reduces data transfer costs and improves performance by caching content at edge locations closer to users. Using CloudFront signed URLs maintains secure access control for purchased datasets.
      Correct: AWS Storage Gateway Volume Gateway cached volumes allow you to store your entire dataset in Amazon S3 while keeping frequently accessed data cached locally. This minimizes the need for on-premises storage expansion and provides efficient, scalable storage with disaster recovery capabilities.
      Correct: AWS DataSync is a managed service designed for fast and secure data migration between on-premises storage and AWS. It is ideal for migrating databases with minimal downtime and without the need to select or manage specific instance types, making it suitable for future scalability.
      Correct: Amazon CloudFront is a content delivery network (CDN) that can distribute your video content globally, reducing latency and improving the speed of delivery. Elastic Transcoder can convert the raw video files into more appropriate formats suitable for streaming, reducing the size of the videos.
      Incorrect: 
        Amazon S3 Glacier is for long-term archival storage and does not support real-time streaming or fast retrieval.
        AWS Elemental MediaStore is for live video workflows, not for on-demand streaming with caching.

  - id: q303
    type: multiple_choice
    question: |
      A solutions architect is designing a company�s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions.

      Which solution meets these requirements?
    options:
     - text: Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.
       is_correct: true
     - text: Use Amazon EC2 Auto Scaling with a fixed number of instances.
       is_correct: false
     - text: Migrate to Amazon ECS with EC2 launch type for manual scaling.
       is_correct: false
     - text: Use AWS Lambda for the application to handle variable load.
       is_correct: false


    explanation: |
      Correct: Accessing AWS Trusted Advisor from the consolidated billing account provides a centralized view of cost optimization opportunities, including RDS Reserved Instance recommendations. Reviewing these recommendations helps reduce costs by identifying opportunities to switch from On-Demand to Reserved Instances for RDS.
      Correct: Amazon FSx for Windows File Server is a fully managed service that provides native support for the SMB protocol, allowing Windows and other SMB clients to access shared storage easily. It is the best choice for managed, scalable, and compatible file storage in AWS for SMB workloads.

    diagram: |
      graph TD
        A[ECS Service] -->|CloudWatch Metrics| B[Application Auto Scaling]
        B -->|Adjusts| C[ECS Task Count]

  - id: q304
    type: multiple_choice
    question: |
      A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The company requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future.

      Which service should a solutions architect recommend?
    options:
     - text: Use AWS DataSync.
       is_correct: true
     - text: Use AWS Direct Connect for faster transfer.
       is_correct: false
     - text: Use Amazon S3 for storage and Athena for queries.
       is_correct: false
     - text: Use AWS Glue for ETL processing.
       is_correct: false
    explanation: |

    diagram: |
      graph TD
        A[On-premises MySQL] -->|Migrate| B[AWS DataSync] --> C[AWS Cloud]

  - id: q305
    type: multiple_choice
    question: |
      A company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed.

      Which AWS solution meets these requirements?
    options:
     - text: Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.
       is_correct: true
     - text: Use Amazon EFS for shared file storage.
       is_correct: false
     - text: Use Amazon S3 with Storage Gateway.
       is_correct: false
     - text: Use AWS Storage Gateway Volume Gateway.
       is_correct: false
    explanation: |

    diagram: |
      graph TD
        A[FSx for Windows File Server] -->|SMB| B[SMB Clients]

  - id: q306
    type: multiple_choice
    question: |
      A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to restrict access to audit team IAM user credentials according to the principle of least privilege. Company managers are worried about accidental deletion of documents in the S3 bucket and want a more secure solution.

      What should a solutions architect do to secure the audit documents?
    options:
     - text: Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.
       is_correct: true
     - text: Launch EC2 instances across multiple Availability Zones.
       is_correct: false
     - text: Use Amazon EBS for storage instead of instance store.
       is_correct: false
     - text: Use AWS Global Accelerator for network optimization.
       is_correct: false
    explanation: |

    diagram: |
      graph TD
        A[EC2 Instances] -->|Cluster Placement Group| B[Low Latency]

  - id: q307
    type: multiple_choice
    question: |
      A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company's domain name and corresponding certificate so that the third-party services can use HTTPS.

      Which solution will meet these requirements?
    options:
     - text: AWS Storage Gateway Volume Gateway cached volumes.
       is_correct: true
     - text: AWS Storage Gateway File Gateway.
       is_correct: false
     - text: Amazon EFS with Direct Connect.
       is_correct: false
     - text: Amazon S3 with Transfer Acceleration.
       is_correct: false
    explanation: |

    diagram: |
      graph TD
        A[On-premises] -->|iSCSI| B[Storage Gateway Cached Volumes] -->|Data| C[Amazon S3]

  - id: q308
    type: multiple_choice
    question: |
      A company has multiple AWS accounts that use consolidated billing. The company runs several active high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The company�s finance team has access to AWS Trusted Advisor in the consolidated billing account and all other AWS accounts.

      The finance team needs to use the appropriate AWS account to access the Trusted Advisor check recommendations for RDS. The finance team must review the appropriate Trusted Advisor check to reduce RDS costs.

      Which combination of steps should the finance team take to meet these requirements? (Choose two.)
    options:
     - text: Access AWS Trusted Advisor from the consolidated billing account.
       is_correct: true
     - text: Review the Amazon RDS Reserved Instance Optimization recommendations.
       is_correct: true
     - text: Access AWS Trusted Advisor from each individual account.
       is_correct: false
     - text: Review the Amazon EC2 Reserved Instance Optimization recommendations.
       is_correct: false
    explanation: |

    diagram: |
      graph TD
        A[Consolidated Billing Account] -->|Trusted Advisor| B[RDS Reserved Instance Recommendations]

  - id: q309
    type: multiple_choice
    question: |
      A solutions architect must optimize storage costs. The solutions architect must identify any Amazon S3 buckets that are no longer being accessed or are rarely accessed.

      Which solution will accomplish this goal with the LEAST operational overhead?
    options:
     - text: Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.
       is_correct: true
     - text: Manually check bucket access logs.
       is_correct: false
     - text: Use AWS Config to monitor bucket changes.
       is_correct: false
     - text: Implement custom scripts to query S3 access metrics.
       is_correct: false
    explanation: |

    diagram: |
      graph TD
        A[S3 Buckets] -->|Access Patterns| B[S3 Storage Lens]

  - id: q310
    type: multiple_choice
    question: |
      A company sells datasets to customers who do research in artificial intelligence and machine learning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region. The company hosts a web application that the customers use to purchase access to a given dataset. The web application is deployed on multiple Amazon EC2 instances behind an Application Load Balancer. After a purchase is made, customers receive an S3 signed URL that allows access to the files.

      The customers are distributed across North America and Europe. The company wants to reduce the cost that is associated with data transfers and wants to maintain or improve performance.

      What should a solutions architect do to meet these requirements?
    options:
     - text: Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.
       is_correct: true
     - text: Use Amazon S3 Transfer Acceleration.
       is_correct: false
     - text: Deploy the application in multiple regions.
       is_correct: false
     - text: Use AWS Direct Connect for customers.
       is_correct: false
    explanation: |

    diagram: |
      graph TD
        A[S3 Bucket] -->|Origin| B[CloudFront] -->|Edge Locations| C[Customers]

  - id: q311
    type: multiple_choice
    question: |
      A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational efficiency and must minimize maintenance.

      Which solution meets these requirements?
    options:
     - text: Create separate Amazon SQS queues for each quote type. Use AWS Lambda functions to process messages from the queues.
       is_correct: true
     - text: Use a single Amazon SQS queue and filter messages by quote type in the application.
       is_correct: false
     - text: Store quotes in Amazon DynamoDB and use scheduled Lambda to process.
       is_correct: false
     - text: Use Amazon Kinesis for real-time processing.
       is_correct: false
    explanation: |

    diagram: |
      graph TD
        A[User] -->|Request| B[SQS Queue by Type] -->|Trigger| C[Lambda]

  - id: q312
    type: multiple_choice
    question: |
      A company�s application runs on Amazon EC2 instances. A solutions architect has standardized the company on a particular instance family and various instance sizes based on the current needs of the company.

      The company wants to maximize cost savings for the application over the next 3 years. The company needs to be able to change the instance family and sizes in the next 6 months based on application popularity and usage.

      Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application�s EBS volumes as resources.
       is_correct: true
     - text: Use manual snapshots for EBS volumes.
       is_correct: false
     - text: Use Amazon Data Lifecycle Manager for automated snapshots.
       is_correct: false
     - text: Use AWS Storage Gateway for backups.
       is_correct: false
    explanation: |

    diagram: |
      graph TD
        A[EC2 Instances] -->|Nightly Backup| B[AWS Backup] -->|Copy| C[Another Region]

  - id: q313
    type: multiple_choice
    question: |
      A company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The company needs to build a platform so that authorized users can watch the company�s content.

      What should a solutions architect recommend to meet these requirements?
    options:
     - text: Use Amazon CloudFront. Provide signed URLs to stream content.
       is_correct: true
     - text: Use Amazon S3 with public access.
       is_correct: false
     - text: Use Amazon EC2 for content hosting.
       is_correct: false
     - text: Use AWS Elemental MediaStore.
       is_correct: false
    explanation: |

    diagram: |
      graph TD
        A[Content] -->|CloudFront| B[Edge Locations] -->|Signed URLs| C[Users]

  - id: q314
    type: multiple_choice
    question: |
      A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The company requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future.

      Which service should a solutions architect recommend?
    options:
     - text: Amazon Aurora Serverless for MySQL
       is_correct: true
     - text: Amazon RDS for MySQL with provisioned instances.
       is_correct: false
     - text: Amazon DynamoDB.
       is_correct: false
     - text: Amazon Redshift.
       is_correct: false
    explanation: |
      Correct: Aurora Serverless is a fully managed, on-demand, and auto-scaling relational database engine provided by AWS. It is suitable for infrequent access patterns and allows the database to automatically start up, shut down, and scale capacity based on actual usage.
      Incorrect: 
        RDS with provisioned instances requires selecting instance types and manual scaling.
        DynamoDB is NoSQL, not for MySQL migration.
        Redshift is for data warehousing, not for transactional databases.

  - id: q315
    type: multiple_choice
    question: |
      A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the findings.

      Which solution will meet these requirements?
    options:
     - text: Use Amazon Inspector to scan the EC2 instances for vulnerabilities and generate reports.
       is_correct: true
     - text: Use AWS Config for compliance checks.
       is_correct: false
     - text: Use Amazon GuardDuty for threat detection.
       is_correct: false
     - text: Use manual security audits.
       is_correct: false
    explanation: |
      Correct: Amazon Inspector scans EC2 instances for vulnerabilities and provides detailed reports.
      Incorrect: 
        AWS Config monitors configuration changes, not vulnerabilities.
        GuardDuty detects threats, not software vulnerabilities.
        Manual audits are not automated and scalable.

  - id: q316
    type: multiple_choice
    question: |
      A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue.

      What should a solutions architect recommend to meet these requirements?
    options:
     - text: Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.
       is_correct: true
     - text: Use a larger EC2 instance.
       is_correct: false
     - text: Use Amazon ECS for containerized processing.
       is_correct: false
     - text: Use Amazon Kinesis for message processing.
       is_correct: false
    explanation: |
      Correct: Lambda is a serverless computing service that allows you to run code without provisioning or managing servers. It automatically scales based on the number of incoming requests.
      Incorrect: 
        A larger EC2 instance increases costs without auto-scaling.
        ECS requires managing containers, not reducing operational costs.
        Kinesis is for streaming, not for queue-based processing.

  - id: q317
    type: multiple_choice
    question: |
      A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces.

      The company needs to implement a solution so that the COTS application can use the data that the legacy application produces.

      Which solution will meet these requirements with the LEAST operational overhead?
    options:
     - text: Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.
       is_correct: true
     - text: Use Amazon Athena to query the CSV files directly.
       is_correct: false
     - text: Convert CSV to Parquet manually.
       is_correct: false
     - text: Use AWS Batch for processing.
       is_correct: false
    explanation: |
      Correct: AWS Glue is a fully managed ETL service that makes it easy to prepare and load data for analytics. It can process the .csv files and store the processed data in Amazon Redshift.
      Incorrect: 
        Athena can query CSV in S3, but the COTS app requires Redshift.
        Manual conversion increases operational overhead.
        AWS Batch is for batch computing, not ETL.

  - id: q318
    type: multiple_choice
    question: |
      A company recently migrated its entire IT environment to the AWS Cloud. The company discovers that users are provisioning oversized Amazon EC2 instances and modifying security group rules without using the appropriate change control process. A solutions architect must devise a strategy to track and audit these inventory and configuration changes.

      Which actions should the solutions architect take to meet these requirements? (Choose two.)
    options:
     - text: Enable AWS CloudTrail and use it for auditing.
       is_correct: true
     - text: Enable AWS Config and create rules for auditing and compliance purposes.
       is_correct: true
     - text: Use Amazon CloudWatch for monitoring.
       is_correct: false
     - text: Use AWS Trusted Advisor for recommendations.
       is_correct: false
    explanation: |
      Correct: CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS CLI, and AWS SDKs and APIs. AWS Config provides a detailed inventory of the AWS resources in your account, and continuously records changes to the configurations of those resources.
      Incorrect: 
        CloudWatch is for monitoring metrics and logs, not for auditing inventory changes.
        Trusted Advisor provides recommendations, not detailed auditing of changes.

  - id: q319
    type: multiple_choice
    question: |
      A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators have used shared SSH keys to manage the instances. After a recent audit, the company�s security team is mandating the removal of all shared keys. A solutions architect must design a solution that provides secure access to the EC2 instances.

      Which solution will meet this requirement with the LEAST amount of administrative overhead?
    options:
     - text: Use AWS Systems Manager Session Manager to connect to the EC2 instances.
       is_correct: true
     - text: Use individual SSH keys for each instance.
       is_correct: false
     - text: Use AWS Directory Service for authentication.
       is_correct: false
     - text: Use bastion hosts with SSH.
       is_correct: false
    explanation: |
      Correct: Session Manager uses IAM roles for authentication and provides an auditable and controlled way to access instances.
      Incorrect: 
        Individual SSH keys require management and distribution.
        Directory Service adds complexity for Linux instances.
        Bastion hosts require additional infrastructure.

  - id: q320
    type: multiple_choice
    question: |
      A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-flight is lost. The company�s data science team wants to query ingested data in near-real time.

      Which solution provides near-real-time data querying that is scalable with minimal data loss?
    options:
     - text: Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.
       is_correct: true
     - text: Use Amazon SQS for queuing data.
       is_correct: false
     - text: Store data directly in Amazon S3.
       is_correct: false
     - text: Use Amazon DynamoDB for storage.
       is_correct: false
    explanation: |
      Correct: Kinesis Data Streams allows you to ingest, buffer, and process streaming data in real time. Kinesis Data Analytics provides an SQL-like language for querying and analyzing data in real time.
      Incorrect: 
        SQS is for queuing messages, not for real-time querying.
        S3 is for storage, not for real-time analytics.
        DynamoDB is for NoSQL storage, not for streaming data analytics.

  - id: q321
    type: multiple_choice
    question: |
      What should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?
    options:
     - text: Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set.
       is_correct: true
     - text: Enable default encryption on the S3 bucket.
       is_correct: false
     - text: Use client-side encryption before uploading.
       is_correct: false
     - text: Configure IAM policies to require encryption.
       is_correct: false
    explanation: |
      Correct: The x-amz-server-side-encryption header specifies the server-side encryption algorithm to be used for the object. Denying uploads without this header ensures all objects are encrypted.
      Incorrect: 
        Default encryption encrypts existing objects but does not enforce encryption on uploads.
        Client-side encryption requires application changes and does not ensure server-side encryption.
        IAM policies control access, not encryption enforcement.

  - id: q322
    type: multiple_choice
    question: |
      A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to confirm that the image was uploaded successfully.

      The thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers.

      What should the solutions architect do to meet these requirements?
    options:
     - text: Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.
       is_correct: true
     - text: Use synchronous processing for thumbnail generation.
       is_correct: false
     - text: Store images in Amazon S3 and process with Lambda synchronously.
       is_correct: false
     - text: Use Amazon Kinesis for image processing.
       is_correct: false
    explanation: |
      Correct: Amazon SQS enables asynchronous communication between application tiers, allowing the application to quickly respond to users while processing thumbnails in the background.
      Incorrect: 
        Synchronous processing would delay user response.
        Synchronous Lambda would not provide fast response.
        Kinesis is for streaming, not for queued tasks.

  - id: q323
    type: multiple_choice
    question: |
      A company�s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance.

      A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company�s security team to analyze.

      Which system architecture should the solutions architect recommend?
    options:
     - text: Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.
       is_correct: true
     - text: Use Amazon EC2 instances to process messages.
       is_correct: false
     - text: Store messages directly in Amazon S3.
       is_correct: false
     - text: Use Amazon SNS for message routing.
       is_correct: false
    explanation: |
      Correct: API Gateway and Lambda provide a serverless, highly available architecture for processing messages. DynamoDB ensures the results are stored durably and are available for analysis.
      Incorrect: 
        EC2 instances require management and are not serverless.
        S3 is for storage, not for processing and analysis.
        SNS is for notifications, not for data processing and storage.

  - id: q324
    type: multiple_choice
    question: |
      A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible.

      Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.
       is_correct: true
     - text: Use AWS Direct Connect for direct access.
       is_correct: false
     - text: Migrate to Amazon EFS.
       is_correct: false
     - text: Use Amazon S3 with Storage Gateway.
       is_correct: false
    explanation: |
      Correct: AWS Storage Gateway Volume Gateway cached volumes provide low-latency access to frequently used data while storing the full dataset in AWS.
      Incorrect: 
        Direct Connect provides connectivity but not caching or disaster recovery.
        EFS is for file storage but not for iSCSI or disaster recovery as described.
        S3 with Gateway is for file access, not volume caching.

  - id: q325
    type: multiple_choice
    question: |
      A company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an identity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket.

      Upon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content.

      Which solution meets these requirements?
    options:
     - text: Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.
       is_correct: true
     - text: Add a bucket policy to the protected S3 bucket to allow public access.
       is_correct: false
     - text: Update the S3 bucket CORS configuration to allow the Amazon Cognito user pool domain.
       is_correct: false
     - text: Create an IAM user with the required permissions and embed the access keys in the application code.
       is_correct: false
    explanation: |
      Correct: Amazon Cognito Identity Pools (Federated Identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. You must associate the identity pool with an IAM role that has the necessary permissions to access the protected S3 bucket.
      Incorrect: 
        - Making the bucket public is a security risk and violates the requirement of authenticated access.
        - CORS allows web applications in one domain to access resources in another, but it does not grant authorization/permissions to read objects.
        - Embedding IAM user keys in client-side code is a major security vulnerability; Cognito is designed specifically to avoid this by providing temporary credentials.

  - id: q326
    type: multiple_choice
    question: |
      An image hosting company uploads its large assets to Amazon S3 Standard buckets. The company uses multipart upload in parallel by using S3 APIs and overwrites if the same object is uploaded again. For the first 30 days after upload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent. The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets.

      Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)
    options:
     - text: Move assets to S3 Intelligent-Tiering after 30 days.
       is_correct: true
     - text: Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.
       is_correct: true
     - text: Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.
       is_correct: false
     - text: Create an S3 Lifecycle policy to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.
       is_correct: false
     - text: Enable S3 Versioning on the bucket to prevent accidental overwrites.
       is_correct: false
    explanation: |
      Correct: 
        - S3 Intelligent-Tiering is the best choice for data with unknown or changing access patterns, as it automatically moves objects between tiers to save costs without performance impact.
        - Incomplete multipart uploads consume storage space and cost money. A lifecycle policy to abort these after a few days is a cost-optimization best practice.
      Incorrect: 
        - S3 One Zone-IA stores data in a single AZ, which does not meet the requirement for high resiliency and availability.
        - S3 Standard-IA is not ideal for "inconsistent" access patterns; if data is accessed more than expected, retrieval fees will offset storage savings.
        - Versioning would increase costs by keeping every copy of an overwritten object, which contradicts the goal of optimizing costs.

  - id: q327
    type: multiple_choice
    question: |
      A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party�s URL. Other internet traffic must be blocked.

      Which solution meets these requirements?
    options:
     - text: Deploy AWS Network Firewall in the VPC. Configure a stateful domain list rule group to allow the repository URLs.
       is_correct: true
     - text: Create a NAT gateway in a public subnet. Use Security Groups to restrict outbound traffic to the repository's IP addresses.
       is_correct: false
     - text: Implement a Network ACL on the private subnet to allow traffic to the repository URLs.
       is_correct: false
     - text: Use an Egress-Only Internet Gateway to allow outbound IPv6 traffic to the repository.
       is_correct: false
    explanation: |
      Correct: AWS Network Firewall allows for FQDN (Fully Qualified Domain Name) filtering. Since software repositories often use many dynamic IP addresses but a single URL, domain list rule groups are the most effective way to allow only specific external sites.
      Incorrect: 
        - Security Groups do not support URL/domain-based rules; they only support IP address ranges.
        - Network ACLs are stateless and operate at Layer 4 (IP/Port), so they cannot filter by URL/Domain names.
        - Egress-Only Internet Gateways are for IPv6 traffic only and do not provide URL-based filtering or stateful inspection.



  - id: q328
    type: multiple_choice
    question: |
      A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously.

      The company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products.

      What should a solutions architect recommend to ensure that all the requests are processed successfully?
    options:
     - text: Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.
       is_correct: true
     - text: Use Amazon ElastiCache to cache the dynamic content from the API. Increase the instance size of the existing EC2 instances.
       is_correct: false
     - text: Replace the ALB with a Network Load Balancer (NLB) to handle higher throughput.
       is_correct: false
     - text: Implement an Amazon SQS queue between the API and the backend workers to decouple the layers and handle traffic spikes.
       is_correct: true
    explanation: |
      Correct: CloudFront offloads static content requests, reducing the load on EC2. Auto Scaling ensures the compute layer grows with demand. (Note: In a SAA context, decoupling with SQS is also a "best" answer for asynchronous workers, but given the options usually provided, ASG+CloudFront is a primary scalability pillar).
      Incorrect: 
        - Increasing instance size (vertical scaling) is manual and has an upper limit; it cannot handle "sudden" significant spikes as well as horizontal scaling (ASG).
        - NLB is for Layer 4 traffic and wouldn't provide the Layer 7 routing/caching benefits of ALB and CloudFront for a web application.

  - id: q329
    type: multiple_choice
    question: |
      A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large fleet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance�s patch status.

      Which solution will meet these requirements?
    options:
     - text: Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.
       is_correct: true
     - text: Use AWS Config to identify instances without patches. Use AWS Lambda to trigger a script that installs updates on the instances.
       is_correct: false
     - text: Use AWS Trusted Advisor to identify security vulnerabilities. Manually update the instances through the EC2 console.
       is_correct: false
     - text: Install an AWS Systems Manager Agent on the instances and use Run Command to manually execute update scripts every week.
       is_correct: false
    explanation: |
      Correct: Amazon Inspector provides automated security assessment and vulnerability scanning. AWS Systems Manager Patch Manager automates the process of patching managed instances with both security-related and other types of updates.
      Incorrect: 
        - AWS Config can detect configuration changes but is not a vulnerability scanner or a native patching tool.
        - Manual updates via Trusted Advisor or Run Command do not meet the requirement for a "regular schedule" with "least operational effort."

  - id: q330
    type: multiple_choice
    question: |
      A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest.

      What should a solutions architect do to meet this requirement?
    options:
     - text: Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.
       is_correct: true
     - text: Use SSL/TLS certificates to encrypt the data before it is sent to the database.
       is_correct: false
     - text: Enable Amazon S3 server-side encryption and move the database backups to S3.
       is_correct: false
     - text: Encrypt the EBS volumes attached to the EC2 instances running the database manually.
       is_correct: false
    explanation: |
      Correct: RDS encryption at rest is managed through AWS KMS. When you enable encryption for an RDS instance, data stored at rest in the underlying storage is encrypted, as are its automated backups, read replicas, and snapshots.
      Incorrect: 
        - SSL/TLS encrypts data in transit, not at rest.
        - Moving backups to S3 does not address the requirement to encrypt the live production database.
        - You cannot "manually" encrypt the underlying EBS volume of a managed RDS instance; it must be enabled during instance creation.

  - id: q331
    type: multiple_choice
    question: |
      A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company�s network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization.

      What should a solutions architect do to meet these requirements?
    options:
     - text: Use AWS Snowball.
       is_correct: true
     - text: Use AWS DataSync to transfer the data over the existing internet connection.
       is_correct: false
     - text: Set up an AWS Site-to-Site VPN and use the AWS CLI to copy the data.
       is_correct: false
     - text: Order an AWS Snowmobile to transport the data.
       is_correct: false
    explanation: |
      Correct: 20 TB over 10.5 Mbps (70% of 15 Mbps) would take approximately 175 days, which exceeds the 30-day limit. AWS Snowball is the only practical solution for this volume and bandwidth constraint.
      Incorrect: 
        - DataSync and VPN are limited by the physical speed of the connection, which is too slow here.
        - Snowmobile is intended for exabyte-scale migrations (dozens of petabytes), not a relatively small 20 TB transfer.

  - id: q332
    type: multiple_choice
    question: |
      A company needs to provide its employees with secure access to confidential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees� devices.

      The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity.

      Which solution will meet these requirements?
    options:
     - text: Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.
       is_correct: true
     - text: Upload the files to an Amazon S3 bucket. Create IAM users for each employee and use bucket policies to restrict access.
       is_correct: false
     - text: Use AWS Storage Gateway (Volume Gateway) to cache the files locally and back them up to AWS.
       is_correct: false
     - text: Set up an Amazon EC2 instance with an EBS volume. Share the volume using SMB and use security groups to restrict access to the office IP.
       is_correct: false
    explanation: |
      Correct: Amazon FSx for Windows File Server provides a fully managed native Windows file system and supports the SMB protocol. Integrating with AD allows existing employee credentials to be used, and Client VPN ensures secure remote access from any location.
      Incorrect: 
        - Managing individual IAM users for all employees is higher operational overhead compared to AD integration.
        - Volume Gateway doesn't solve the "remote usage" secure access and capacity issues as elegantly as a managed FSx file system.
        - A single EC2/EBS setup is not highly available and lacks the managed features of FSx.

  - id: q333
    type: multiple_choice
    question: |
      A company�s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application.

      What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?
    options:
     - text: Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.
       is_correct: true
     - text: Use a target tracking scaling policy based on average CPU utilization.
       is_correct: false
     - text: Change the Auto Scaling group to use a larger instance type.
       is_correct: false
     - text: Use a step scaling policy with a shorter warm-up period.
       is_correct: false
    explanation: |
      Correct: Scheduled scaling allows you to scale your application in response to predictable load changes. Since the peak occurs precisely at midnight on the first of every month, you can proactively add instances before the spike occurs.
      Incorrect: 
        - Target tracking or Step scaling are reactive. By the time they detect 100% CPU and launch new instances, the application is already disrupted.
        - Changing to a larger instance type is a permanent cost increase and might still be overwhelmed by the spike if not scaled horizontally.

  - id: q334
    type: multiple_choice
    question: |
      A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer�s application uses an SFTP client to download the files.

      Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer�s application?
    options:
     - text: Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.
       is_correct: true
     - text: Install an SFTP server on an Amazon EC2 instance. Use IAM roles to allow the server access to S3.
       is_correct: false
     - text: Use AWS Storage Gateway (File Gateway) and share the files via SMB.
       is_correct: false
     - text: Use Amazon S3 Glacier and provide the customer with a presigned URL for each file.
       is_correct: false
    explanation: |
      Correct: AWS Transfer Family is a managed service that enables the transfer of files over SFTP, FTPS, and FTP directly into and out of Amazon S3. It integrates natively with Active Directory, requiring zero management of servers.
      Incorrect: 
        - Hosting an SFTP server on EC2 involves high operational overhead (patching, scaling, managing users).
        - File Gateway uses SMB/NFS, which would require the customer to change their application from SFTP to a different protocol.
        - S3 Glacier is for archival and has long retrieval times; presigned URLs are not an SFTP-compatible solution.

  - id: q335
    type: multiple_choice
    question: |
      A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand.

      Which solution meets these requirements?
    options:
     - text: Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.
       is_correct: true
     - text: Configure the Auto Scaling group to use Warm Pools.
       is_correct: false
     - text: Use a larger instance type that supports Enhanced Networking.
       is_correct: false
     - text: Use an Amazon S3-backed AMI instead of an EBS-backed AMI.
       is_correct: false
    explanation: |
      Correct: When an EBS volume is created from a snapshot, the data is lazily loaded from S3. Fast snapshot restore (FSR) eliminates this latency by ensuring that the EBS volume is fully initialized at creation time.
      Incorrect: 
        - While Warm Pools help by keeping instances pre-initialized, the question specifically asks about initialization latency when provisioning from an AMI/Snapshot.
        - Enhanced Networking improves throughput but does not affect the speed at which disk data is pulled from a snapshot.
        - S3-backed AMIs are slower and considered legacy compared to EBS-backed AMIs.

  - id: q336
    type: multiple_choice
    question: |
      A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company�s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days.

      What should a solutions architect do to meet this requirement with the LEAST operational effort?
    options:
     - text: Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.
       is_correct: true
     - text: Store the credentials in a private file in Amazon S3. Use a Lambda function to rotate the credentials and update the file.
       is_correct: false
     - text: Use AWS Systems Manager Parameter Store with SecureString to store credentials. Set up a CloudWatch Event to trigger a rotation Lambda.
       is_correct: false
     - text: Hardcode the credentials in the application but use AWS KMS to encrypt the source code.
       is_correct: false
    explanation: |
      Correct: AWS Secrets Manager has native integration with Amazon Aurora. It can automatically rotate the credentials and manage the rotation logic with a single configuration, requiring the least operational effort.
      Incorrect: 
        - S3 storage for credentials is not a managed rotation solution and is less secure.
        - Parameter Store doesn't have a native "out of the box" rotation feature like Secrets Manager; it requires you to write and maintain custom Lambda logic.
        - Hardcoding credentials is a security violation and prevents easy rotation.



  - id: q337
    type: multiple_choice
    question: |
      A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures.

      As traffic on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead.

      Which solution will meet these requirements?
    options:
     - text: Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.
       is_correct: true
     - text: Use Amazon ElastiCache for Redis in front of the database to reduce the read load on the replicas.
       is_correct: false
     - text: Increase the size of the primary RDS instance to handle the stored procedures faster.
       is_correct: false
     - text: Use Amazon Kinesis Data Streams to capture database changes and update the replicas manually.
       is_correct: false
    explanation: |
      Correct: Aurora uses a shared storage layer, which means replicas don't need to perform their own write operations to stay in sync. This typically results in replication lag of less than 100ms, regardless of load.
      Incorrect: 
        - Redis helps with reads but requires significant application code changes to implement caching logic.
        - Scaling the primary instance doesn't solve the lag issue on the replicas; the replicas themselves are usually the bottleneck during replication.
        - Kinesis for manual replication is extremely high operational overhead and complex to implement.

  - id: q338
    type: multiple_choice
    question: |
      A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster.

      The DR plan must replicate data to a secondary AWS Region.

      Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region.
       is_correct: true
     - text: Use AWS Database Migration Service (DMS) to replicate data to the secondary Region.
       is_correct: false
     - text: Create a cross-region read replica for the Aurora DB cluster.
       is_correct: false
     - text: Take daily snapshots and use a Lambda function to copy them to the secondary Region.
       is_correct: false
    explanation: |
      Correct: Aurora Global Database is designed for globally distributed applications. It provides fast, storage-based replication with no impact on database performance and allows for very low RPO/RTO.
      Incorrect: 
        - DMS is more complex to manage and is typically used for heterogeneous migrations or ongoing synchronization between different database types.
        - While cross-region replicas exist, Global Database is the purpose-built feature for Aurora DR and is more efficient.
        - Snapshot copying has an RPO of 24 hours (if done daily), which is usually unacceptable for a "high-volume SaaS" platform.

  - id: q339
    type: multiple_choice
    question: |
      A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort.

      What should a solutions architect do to meet these requirements?
    options:
     - text: Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.
       is_correct: true
     - text: Use IAM Database Authentication to allow the application to connect to the RDS instance without a password.
       is_correct: false
     - text: Move the credentials to a configuration file encrypted with AWS KMS on an S3 bucket.
       is_correct: false
     - text: Encrypt the application source code using AWS CloudHSM.
       is_correct: false
    explanation: |
      Correct: This is the standard use case for AWS Secrets Manager. It removes hardcoded credentials and provides automated rotation, satisfying the security requirement with minimal changes to application logic.
      Incorrect: 
        - IAM DB Authentication is secure but requires significant code changes to generate and handle authentication tokens.
        - Configuration files in S3 still require the application to manage the keys to access S3, and it doesn't solve the rotation problem.
        - CloudHSM is an expensive, hardware-based key storage solution and doesn't address the issue of embedded credentials in application logic.

  - id: q340
    type: multiple_choice
    question: |
      A media company hosts its website on AWS. The website application�s architecture includes a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The company�s cybersecurity team reports that the application is vulnerable to SQL injection.

      How should the company resolve this issue?
    options:
     - text: Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.
       is_correct: true
     - text: Modify the security group of the ALB to block all traffic from unknown IP addresses.
       is_correct: false
     - text: Enable Amazon GuardDuty to detect and block SQL injection attempts automatically.
       is_correct: false
     - text: Use Amazon Inspector to scan the EC2 instances and apply the suggested fixes.
       is_correct: false
    explanation: |
      Correct: AWS WAF (Web Application Firewall) can inspect the body and headers of HTTP requests. It contains pre-configured "Managed Rules" specifically designed to identify and block SQL injection (SQLi) patterns.
      Incorrect: 
        - Security groups operate at Layer 4 and cannot inspect the content of an HTTP request to see if it contains an SQL injection string.
        - GuardDuty is a threat detection service that identifies suspicious behavior (like account compromise), but it doesn't block SQL injection at the edge.
        - Amazon Inspector scans for host-level vulnerabilities (like missing patches), not application-level code vulnerabilities like SQLi.

  - id: q341
    type: multiple_choice
    question: |
      A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company�s marketing team can access only a subset of columns in the database.

      Which solution will meet these requirements with the LEAST operational overhead?
    options:
     - text: Use Amazon Athena to query both the S3 data lake and Aurora. Register Aurora as a data source in Lake Formation and define column-level permissions. Connect QuickSight to Athena.
       is_correct: true
     - text: Export the Aurora data to S3 using AWS Glue. Use Lake Formation to manage permissions on the combined S3 data and query with Athena.
       is_correct: false
     - text: Create a separate Aurora Read Replica for the marketing team and use database views to restrict column access.
       is_correct: false
     - text: Use QuickSight's internal data preparation tool to hide the columns from the marketing team's dashboard.
       is_correct: false
    explanation: |
      Correct: Lake Formation provides a centralized way to manage permissions, including column-level access, across data in S3 and federated data sources like Aurora via Athena. This avoids the need for data movement or multiple database replicas.
      Incorrect: 
        - Exporting data via Glue adds operational overhead and creates a lag in data availability.
        - Managing database views and separate replicas is a high-effort task and is not centralized.
        - Hiding columns in QuickSight is a UI-level change; it does not prevent the user from accessing the underlying data if they have direct query access.

  - id: q342
    type: multiple_choice
    question: |
      A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run.

      Currently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group�s desired capacity.

      Which solution will meet these requirements with the LEAST operational overhead?
    options:
     - text: Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.
       is_correct: true
     - text: Set up a Lambda function triggered by a CloudWatch Event to update the ASG capacity every week.
       is_correct: false
     - text: Use a dynamic scaling policy based on the 'RequestCount' metric with a 30-minute look-ahead window.
       is_correct: false
     - text: Configure a step scaling policy that triggers when CPU utilization exceeds 50%.
       is_correct: false
    explanation: |
      Correct: Predictive scaling uses machine learning to analyze history and forecast future load. It can pre-launch instances so they are ready before the load arrives, satisfying the 30-minute requirement without manual intervention.
      Incorrect: 
        - Manual Lambda functions require writing and maintaining code, which is more overhead than a native scaling policy.
        - Dynamic scaling is reactive and would only start adding instances *after* the job has already begun and CPU has spiked.
        - Step scaling is also reactive and does not include a "pre-launch" feature.

  - id: q343
    type: multiple_choice
    question: |
      A solutions architect is designing a company�s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions.

      Which solution will meet these requirements with the LEAST operational overhead?
    options:
     - text: Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.
       is_correct: true
     - text: Replicate the EC2 instance using AWS Elastic Disaster Recovery (AWS DRS) to the secondary Region.
       is_correct: false
     - text: Set up an RDS for MySQL Multi-AZ deployment and create a cross-region read replica.
       is_correct: false
     - text: Use a Lambda function to copy EBS snapshots of the database to an S3 bucket in the secondary Region.
       is_correct: false
    explanation: |
      Correct: Aurora Global Database provides a managed cross-region DR solution with virtually no impact on performance and minimal operational overhead compared to managing EC2 instances or manual snapshot replication.
      Incorrect: 
        - AWS DRS is powerful but managing EC2-based databases involves more manual patching and scaling overhead than using a managed Aurora cluster.
        - The current database is on EC2; migrating to Aurora is generally the "least overhead" path for managed DR.
        - Manual snapshot copies have a very high RPO and require complex recovery scripts.

  - id: q344
    type: multiple_choice
    question: |
      A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB.

      Which solution will meet these requirements with the FEWEST changes to the code?
    options:
     - text: Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.
       is_correct: true
     - text: Zip the messages before sending them to SQS to reduce the size below 256 KB.
       is_correct: false
     - text: Use Amazon Kinesis Data Streams instead of SQS to handle larger payloads.
       is_correct: false
     - text: Split the large messages into multiple smaller 256 KB messages and reassemble them at the destination.
       is_correct: false
    explanation: |
      Correct: The SQS Extended Client Library is specifically designed for this. It automatically uploads large payloads to S3 and sends a pointer in the SQS message. The receiver library sees the pointer and automatically downloads the payload from S3.
      Incorrect: 
        - Zipping 50 MB might still result in a file larger than 256 KB.
        - Switching to Kinesis is a significant architectural and code change.
        - Manually splitting and reassembling messages is complex to code and error-prone (handling missing parts, ordering, etc.).

  - id: q345
    type: multiple_choice
    question: |
      A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible.

      Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon CloudFront to serve the web application globally.
       is_correct: true
     - text: Use an AWS Lambda function with an Amazon API Gateway to handle authentication against an RDS database.
       is_correct: false
     - text: Deploy a fleet of EC2 instances with an OpenID Connect (OIDC) provider and use an ALB with a global accelerator.
       is_correct: false
     - text: Use AWS WAF to restrict access based on a list of authorized IP addresses.
       is_correct: false
    explanation: |
      Correct: Cognito is serverless and scales automatically. Lambda@Edge runs at the CloudFront edge locations, providing the lowest latency for authorization checks before content is served to the user globally.
      Incorrect: 
        - Managing an RDS database for 100 users is not cost-effective and introduces operational overhead.
        - EC2 instances are not serverless and are more expensive to maintain.
        - IP-based restriction via WAF is not "authentication" or "authorization" in the context of user-level security.

  - id: q346
    type: multiple_choice
    question: |
      A company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array�s support contract. Some of the data is accessed frequently, but much of the data is inactive.

      A solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identified AWS Storage Gateway as part of the solution.

      Which type of storage gateway should the solutions architect provision to meet these requirements?
    options:
     - text: Amazon S3 File Gateway
       is_correct: true
     - text: FSx File Gateway
       is_correct: false
     - text: Tape Gateway
       is_correct: false
     - text: Volume Gateway
       is_correct: false
    explanation: |
      Correct: S3 File Gateway supports both SMB and NFS protocols. It allows on-premises workstations to access data stored as objects in S3 as if it were on a local file share. This enables the use of S3 Lifecycle policies to move inactive data to cheaper tiers like Glacier.
      Incorrect: 
        - FSx File Gateway is specifically for accessing Amazon FSx for Windows File Server, not S3.
        - Tape Gateway is for replacing physical tape backups with virtual tapes in S3.
        - Volume Gateway provides block-level volumes (iSCSI), not file-level shares (SMB/NFS).



  - id: q347
    type: multiple_choice
    question: |
      A company has an application that is running on Amazon EC2 instances. A solutions architect has standardized the company on a particular instance family and various instance sizes based on the current needs of the company.

      The company wants to maximize cost savings for the application over the next 3 years. The company needs to be able to change the instance family and sizes in the next 6 months based on application popularity and usage.

      Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Compute Savings Plan
       is_correct: true
     - text: Standard Reserved Instances
       is_correct: false
     - text: EC2 Instance Savings Plan
       is_correct: false
     - text: Convertible Reserved Instances
       is_correct: false
    explanation: |
      Correct: Compute Savings Plans offer the greatest flexibility. They apply to usage across any instance family, size, region, and even Fargate or Lambda, making them ideal for a company that expects to change families and sizes.
      Incorrect: 
        - Standard RIs are locked to a specific instance family and cannot be changed.
        - EC2 Instance Savings Plans are locked to a specific family in a specific region.
        - Convertible RIs allow family changes but require manual exchanges and are generally less flexible than Compute Savings Plans.

  - id: q348
    type: multiple_choice
    question: |
      A company collects data from a large number of participants who use wearable devices. The company stores the data in an Amazon DynamoDB table and uses applications to analyze the data. The data workload is constant and predictable. The company wants to stay at or below its forecasted budget for DynamoDB.

      Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).
       is_correct: true
     - text: Use on-demand mode to handle the high volume of incoming data.
       is_correct: false
     - text: Enable DynamoDB Accelerator (DAX) to reduce the number of read requests to the table.
       is_correct: false
     - text: Create a Global Secondary Index (GSI) to distribute the load across multiple partitions.
       is_correct: false
    explanation: |
      Correct: Provisioned mode is the most cost-effective option for workloads that are constant and predictable. By accurately specifying RCUs and WCUs, the company can avoid the higher per-request costs of on-demand mode.
      Incorrect: 
        - On-demand mode is more expensive for predictable workloads.
        - DAX adds an additional cost for the cache nodes and is only beneficial for read-heavy workloads with microsecond latency requirements.
        - GSIs increase the total WCUs and RCUs required, which would increase costs.

  - id: q349
    type: multiple_choice
    question: |
      A company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company�s AWS account in ap-southeast-3.

      What should a solutions architect do to meet these requirements?
    options:
     - text: Create a database snapshot. Add the acquiring company�s AWS account to the KMS key policy. Share the snapshot with the acquiring company�s AWS account.
       is_correct: true
     - text: Make the snapshot public and provide the acquiring company with the KMS key ID.
       is_correct: false
     - text: Export the database to an S3 bucket and use a bucket policy to grant access to the acquiring company.
       is_correct: false
     - text: Use AWS Resource Access Manager (RAM) to share the Aurora cluster directly.
       is_correct: false
    explanation: |
      Correct: To share an encrypted snapshot with another account, you must share the snapshot itself and grant the target account's root user or specific IAM roles permission to use the KMS key that was used to encrypt the snapshot.
      Incorrect: 
        - You cannot share encrypted snapshots publicly.
        - Exporting to S3 is more complex and requires managing S3 permissions and re-importing the data.
        - AWS RAM is used for sharing resources like subnets or Transit Gateways, but it is not used for sharing RDS/Aurora database snapshots.

  - id: q350
    type: multiple_choice
    question: |
      A company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the us-east-1 Region to store customer transactions. The company needs high availability and automatic recovery for the DB instance.

      The company must also run reports on the RDS database several times a year. The report process causes transactions to take longer than usual to post to the customers� accounts. The company needs a solution that will improve the performance of the report process.

      Which combination of steps will meet these requirements? (Choose two.)
    options:
     - text: Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment.
       is_correct: true
     - text: Create a read replica of the DB instance in a different Availability Zone. Point all requests for reports to the read replica.
       is_correct: true
     - text: Increase the storage size of the DB instance to 1000 GB to increase IOPS.
       is_correct: false
     - text: Migrate the database to an Amazon EC2 instance to have more control over the hardware.
       is_correct: false
     - text: Set up a secondary RDS instance and use log shipping to keep it synchronized.
       is_correct: false
    explanation: |
      Correct: 
        - Changing to Multi-AZ provides high availability and automatic failover.
        - Read replicas (now supported for SQL Server) allow the reporting workload to be offloaded from the primary instance, preventing performance degradation of transactional traffic.
      Incorrect: 
        - Increasing storage might help with performance but does not provide "high availability" or "automatic recovery."
        - Moving to EC2 increases operational overhead and requires manual implementation of HA/DR.
        - Log shipping is a manual, legacy way to handle replication and is inferior to the managed Multi-AZ and Read Replica features of RDS.