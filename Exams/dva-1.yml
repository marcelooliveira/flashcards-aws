questions:
  - id: q1
    type: multiple_choice
    question: Which of the following are good use cases for how Amazon ElastiCache can help an application? (Select TWO)
    options:
     - text: Improve the performance of S3 PUT operations.
       is_correct: false
     - text: Improve the latency of deployments performed by AWS CodeDeploy.
       is_correct: false
     - text: Improve latency and throughput for read-heavy application workloads.
       is_correct: true
     - text: Reduce the time required to merge AWS CodeCommit branches.
       is_correct: false
     - text: Improve performance of compute-intensive applications.
       is_correct: true
    explanation: |
      Correct: ElastiCache speeds up frequent reads by storing data in memory, reducing latency and increasing throughput for read-heavy workloads. It also helps applications that benefit from in-memory caching to avoid repeated computational work.
      Incorrect: The incorrect options describe responsibilities that are not appropriate for an in-memory cache (S3 PUT operations, deployments, or code management), so they are not valid ElastiCache use cases.
    tags: 
    difficulty: 
    points: 

  - id: q2
    type: multiple_choice
    question: Which of the following services are key/value stores? (Choose 3 answers)
    options:
     - text: Amazon ElastiCache.
       is_correct: true
     - text: Simple Notification Service.
       is_correct: false
     - text: DynamoDB.
       is_correct: true
     - text: Simple Workflow Service.
       is_correct: false
     - text: Simple Storage Service.
       is_correct: true
    explanation: |
      Correct: The correct options (ElastiCache, DynamoDB, and S3) can be used as key/value stores: ElastiCache stores key/value pairs in memory, DynamoDB is a NoSQL key/value database, and S3 objects are accessed by a key.
      Incorrect: The incorrect options (SNS, SWF) are messaging or orchestration services and do not function as key/value stores.
    tags: 
    difficulty: 
    points: 

  - id: q3
    type: multiple_choice
    question: A developer wants to send multi-value headers to an AWS Lambda function that is registered as a target with an Application Load Balancer (ALB). What should the developer do to achieve this?
    options:
     - text: Place the Lambda function and target group in the same account.
       is_correct: false
     - text: Send the request body to the Lambda function with a size less than 1 MB 0.
       is_correct: false
     - text: Include the Base64 encoding status status code, status description, and headers in the Lambda function.
       is_correct: false
     - text: Enable the multi-value headers on the ALB.
       is_correct: true
    explanation: |
      Correct: You must enable the multi-value headers feature on the ALB so that headers with multiple values are delivered to the Lambda function exactly as sent by the client.
      The following example request has two query parameters with the same key:
      ```
      http://www.example.com?&myKey=val1&myKey=val2
      ```
      With the default format, the load balancer uses the last value sent by the client and sends you an event that includes query string parameters using queryStringParameters. For example:
      
      ```json
      "queryStringParameters": { "myKey": "val2"},
      ```
      If you enable multi-value headers, the load balancer uses both key values sent by the client and sends you an event that includes query string parameters using multiValueQueryStringParameters. For example:
      
      ```json
      "multiValueQueryStringParameters": { "myKey": ["val1", "val2"] },
      ```

      Incorrect: The other options (account placement, request body size, or handling Base64 status) do not enable delivery of multi-value headers from the ALB to Lambda.
    tags: 
    difficulty: 
    points: 

  - id: q4
    type: multiple_choice
    question: A company's ecommerce website is experiencing massive traffic spikes, which are causing performance problems in the company database. Users are reporting that accessing the website takes a long time. A developer wants to implement a caching layer using Amazon ElastiCache. The website is required to be responsive no matter which product a user views, and the updates to product information and prices must be strongly consistent. Which cache writing policy will satisfy these requirements?
    options:
     - text: Write to the cache directly and sync the backend at a later time.
       is_correct: false
     - text: Write to the backend first and wait for the cache to expire.
       is_correct: false
     - text: Write to the cache and the backend at the same time.
       is_correct: false
     - text: Write to the backend first and invalidate the cache.
       is_correct: true
    explanation: |
      Correct: Writing to the backend first and then invalidating the cache ensures strong consistency because the database remains the source of truth and the cache is invalidated so subsequent reads get the updated data.
      Incorrect: The other strategies risk stale reads or inconsistency (writing only to cache, delayed sync, or simultaneous writes without proper invalidation) and therefore do not meet the strong consistency requirement.
    tags: 
    difficulty: 
    points: 

  - id: q5
    type: multiple_choice
    question: A Developer wants to upload data to Amazon S3 and must encrypt the data in transit. Which of the following solutions will accomplish this task? (Choose TWO)
    options:
     - text: Set up hardware VPN tunnels to a VPC and access S3 through a VPC endpoint.
       is_correct: false
     - text: Set up Client-Side Encryption with an AWS KMS-Managed Customer Master Key.
       is_correct: true
     - text: Set up Server-Side Encryption with AWS KMS-Managed Keys.
       is_correct: false
     - text: Transfer the data over an SSL connection.
       is_correct: true
     - text: Set up Server-Side Encryption with S3-Managed Keys.
       is_correct: false
    explanation: |
      Correct: Transferring data over SSL/TLS protects the channel (encryption in transit), and client-side encryption ensures data is encrypted before it leaves the application, satisfying requirements for data in transit.
      Incorrect: The server-side encryption options are primarily about encryption at rest, and hardware VPN/VPC endpoint is not a direct guarantee of in-transit encryption in this context.
    tags: 
    difficulty: 
    points: 

  - id: q6
    type: multiple_choice
    question: A Developer wants to encrypt new objects that are being uploaded to an Amazon S3 bucket by an application. There must be an audit trail of who has used the key during this process. There should be no change to the performance of the application. Which type of encryption meets these requirements?
    options:
     - text: Server-side encryption using S3-managed keys.
       is_correct: false
     - text: Server-side encryption with AWS KMS-managed keys.
       is_correct: true
     - text: Client-side encryption with a client-side symmetric master key.
       is_correct: false
     - text: Client-side encryption with AWS KMS-managed keys.
       is_correct: false
    explanation: |
      Correct: SSE-KMS provides server-side encryption integrated with AWS KMS and records a key usage audit trail without requiring application changes that would impact performance.
      Incorrect: S3-managed keys do not provide the same key usage auditability, and client-side encryption would require application changes and could impact performance.
    tags: 
    difficulty: 
    points: 

  - id: q7
    type: multiple_choice
    question: An application is being developed to audit several AWS accounts. The application will run in Account A and must access AWS services in Accounts B and C. What is the MOST secure way to allow the application to call AWS services in each audited account?
    options:
     - text: Configure cross-account roles in each audited account. Write code in Account A that assumes those roles.
       is_correct: true
     - text: Use S3 cross-region replication to communicate among accounts, with Amazon S3 event notifications to trigger Lambda functions.
       is_correct: false
     - text: Deploy an application in each audited account with its own role. Have Account A authenticate with the application.
       is_correct: false
     - text: Create an IAM user with an access key in each audited account. Write code in Account A that uses those access keys.
       is_correct: false
    explanation: |
      Correct: Configuring cross-account IAM roles in the audited accounts and having the application in Account A assume those roles is the most secure approach, because each account retains control and temporary credentials are used.
      Incorrect: The other options are less secure or more complex (sharing static access keys, replicating via S3, or deploying agents in each account) and increase attack surface or operational burden.
    tags: 
    difficulty: 
    points: 

  - id: q8
    type: multiple_choice
    question: A company uses a third-party tool to build, bundle, and package rts applications on-premises and store them locally. The company uses Amazon EC2 instances to run its front-end applications. How can an application be deployed from the source control system onto the EC2 instances?
    options:
     - text: Use AWS CodeDeploy and point it to the local storage to directly deploy a bundle m a zip. tar. or tar.gz format.
       is_correct: false
     - text: Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy.
       is_correct: true
     - text: Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances.
       is_correct: false
     - text: Use AWS CodeBuild to automatically deploy the latest build to the latest EC2 instances.
       is_correct: false
    explanation: |
      Correct: The supported flow is to upload the deployment bundle to S3 and instruct CodeDeploy to use that artifact for deploying to EC2 instances.
      Incorrect: Pointing CodeDeploy directly to local storage is not supported in this way, and CodeCommit/CodeBuild do not replace the need to provide the deployment artifact to CodeDeploy in this scenario.
    tags: 
    difficulty: 
    points: 

  - id: q9
    type: multiple_choice
    question: A company is building a compute-intensive application that will run on a fleet of Amazon EC2 instances. The application uses attached Amazon EBS disks for storing data. The application will process sensitive information and all the data must be encrypted. What should a developer do to ensure the data is encrypted on disk without impacting performance?
    options:
     - text: Configure the Amazon EC2 instance fleet to use encrypted EBS volumes for storing data.
       is_correct: true
     - text: Add logic to write all data to an encrypted Amazon S3 bucket.
       is_correct: false
     - text: Add a custom encryption algorithm to the application that will encrypt and decrypt all data.
       is_correct: false
     - text: Create a new Amazon Machine Image (AMI) with an encrypted root volume and store the data to ephemeral disks.
       is_correct: false
    explanation: |
      Correct: Using encrypted EBS volumes provides at-rest encryption managed by the service without changing the application and with minimal performance impact.
      Incorrect: The other options require architectural changes or custom encryption logic or use ephemeral storage that may not be suitable for persistent data.
    tags: 
    difficulty: 
    points: 

  - id: q10
    type: multiple_choice
    question: A global company has an application running on Amazon EC2 instances that serves image files from Amazon S3. User requests from the browser are causing high traffic, which results in degraded performance. Which optimization solution should a Developer implement to increase application performance?
    options:
     - text: Create multiple prefix in the S3 bucket to increase the request rate.
       is_correct: false
     - text: Create an Amazon ElastiCache cluster to cache and serve frequently accessed items.
       is_correct: false
     - text: Use Amazon CloudFront to serve the content of images stored in Amazon S3.
       is_correct: true
     - text: Submit a ticket to AWS support to request a rate limit increase for the S3 bucket.
       is_correct: false
    explanation: |
      Correct: Using CloudFront reduces global latency and offloads traffic from S3 by delivering images via a CDN.
      Incorrect: The other options do not address global content delivery effectively (S3 prefixes, ElastiCache without integration, or requesting rate increases) and are not the best fit.
    tags: 
    difficulty: 
    points: 

  - id: q11
    type: multiple_choice
    question: An AWS Lambda function generates a 3MB JSON file and then uploads it to an Amazon S3 bucket daily. The file contains sensitive information, so the Developer must ensure that it is encrypted before uploading to the bucket. Which of the following modifications should the Developer make to ensure that the data is encrypted before uploading it to the bucket?
    options:
     - text: Use the default AWS KMS customer master key for S3 in the Lambda function code.
       is_correct: false
     - text: Use the S3 managed key and call the 'GenerateDataKey' API to encrypt the file.
       is_correct: false
     - text: Use the 'GenerateDataKey' API, then use that data key to encrypt the file in the Lambda function code.
       is_correct: true
     - text: Use a custom KMS customer master key created for S3 in the Lambda function code.
       is_correct: false
    explanation: |
      Correct: Calling 'GenerateDataKey' returns a plaintext data key and an encrypted copy; using the plaintext data key to encrypt the object locally before upload ensures the content is encrypted in transit and at rest.
      Incorrect: The other options either misunderstand how to use KMS for client-side encryption or rely on default server-side keys without explicitly encrypting the file prior to upload.
    tags: 
    difficulty: 
    points: 

  - id: q12
    type: multiple_choice
    question: Company D is running their corporate website on Amazon S3 accessed from 'http://www.companyd.com'. Their marketing team has published new web fonts to a separate S3 bucket accessed by the S3 endpoint 'https://s3-us-west-1.amazonaws.com/cdfonts'. While testing the new web fonts, Company D recognized the web fonts are being blocked by the browser. What should Company D do to prevent the web fonts from being blocked by the browser?
    options:
     - text: Enable versioning on the cdfonts bucket for each web font.
       is_correct: false
     - text: Create a policy on the cdfonts bucket to enable access to everyone.
       is_correct: false
     - text: Add the 'Content-MD5' header to the request for webfonts in the cdfonts bucket from the website.
       is_correct: false
     - text: Configure the cdfonts bucket to allow cross-origin requests by creating a CORS configuration.
       is_correct: true
    explanation: |
      Correct: Configuring a CORS policy on the cdfonts bucket allows resources hosted on another domain to be loaded by the browser, resolving font blocking caused by same-origin policies.
      Incorrect: Versioning, making the bucket public, or adding a Content-MD5 header do not address the cross-origin request blocking issue.
    tags: 
    difficulty: 
    points: 

  - id: q13
    type: multiple_choice
    question: A developer must extend an existing application that is based on the AWS Serverless Application Model (AWS SAM). The developer has used the AWS SAM CLI to create the project. The project contains different AWS Lambda functions. Which combination of commands must the developer use to redeploy the AWS SAM application? (Select TWO)
    options:
     - text: 'sam init'
       is_correct: false
     - text: 'sam validate'
       is_correct: false
     - text: 'sam build'
       is_correct: true
     - text: 'sam deploy'
       is_correct: true
     - text: 'sam publish'
       is_correct: false
    explanation: |
      Correct: 'sam build' and 'sam deploy' are the standard workflow to build SAM artifacts and deploy the updated application to AWS.
      Incorrect: The other commands are for initializing a project, validating templates, or publishing packages and do not by themselves perform the build+deploy redeploy flow.
    tags: 
    difficulty: 
    points: 

  - id: q14
    type: multiple_choice
    question: An application deployed on AWS Elastic Beanstalk experiences increased error rates during deployments of new application versions, resulting in service degradation for users. The Development team believes that this is because of the reduction in capacity during the deployment steps. The team would like to change the deployment policy configuration of the environment to an option that maintains full capacity during deployment while using the existing instances. Which deployment policy will meet these requirements while using the existing instances?
    options:
     - text: All at once.
       is_correct: false
     - text: Rolling.
       is_correct: false
     - text: Rolling with additional batch.
       is_correct: true
     - text: Immutable.
       is_correct: false
    explanation: |
      Correct: 'Rolling with additional batch' adds temporary instances to maintain capacity during deployment, preventing visible capacity reduction while using the existing instances.
      Incorrect: The other options either reduce capacity during deployment (All at once or Rolling) or create new immutable instances rather than using existing instances per the requirement.
    tags: 
    difficulty: 
    points: 

  - id: q15
    type: multiple_choice
    question: A Developer is creating an application that needs to locate the public IPv4 address of the Amazon EC2 instance on which it runs. How can the application locate this information?
    options:
     - text: Get the instance metadata by retrieving 'http://169.254.169.254/latest/metadata/'

       is_correct: true
     - text: Get the instance user data by retrieving 'http://169.254.169.254/latest/userdata/'
       is_correct: false
     - text: Get the application to run 'IFCONFIG' to get the public IP address.
       is_correct: false
     - text: Get the application to run 'IPCONFIG' to get the public IP address.
       is_correct: false
    explanation: |
      Correct: The instance metadata endpoint (169.254.169.254) provides the public IP address programmatically and reliably.
      Incorrect: User data is not the right source for public IP, and OS commands like ifconfig/ipconfig may not reliably provide the public IP or be portable across OSes.
    tags: 
    difficulty: 
    points: 

  - id: q16
    type: multiple_choice
    question: The development team is working on an API that will be served from Amazon API gateway. The API will be served from three environments; development, test, and production. The API Gateway is configured to use 237 GB of cache in all three stages. Which is the MOST cost-efficient deployment strategy?
    options:
     - text: Create a single API Gateway with all three stages.
       is_correct: false
     - text: Create three API Gateways, one for each stage in a single AWS account.
       is_correct: false
     - text: Create an API Gateway in three separate AWS accounts.
       is_correct: false
     - text: Enable the cache for development and test environments only when needed.
       is_correct: true
    explanation: |
      Correct: Enabling cache only when required for development and test avoids continuous provisioned cache costs while keeping production appropriately configured.
      Incorrect: Creating multiple gateways or always enabling large caches in all stages would incur unnecessary fixed costs.
    tags: 
    difficulty: 
    points: 

  - id: q17
    type: multiple_choice
    question: A company is migrating its on-premises database to Amazon RDS for MySQL. The company has read-heavy workloads, and wants to make sure it re-factors its code to achieve optimum read performance for its queries. How can this objective be met?
    options:
     - text: Add database retries to effectively use RDS with vertical scaling.
       is_correct: false
     - text: Use RDS with multi-AZ deployment.
       is_correct: false
     - text: Add a connection string to use an RDS read replica for read queries.
       is_correct: true
     - text: Add a connection string to use a read replica on an EC2 instance.
       is_correct: false
    explanation: |
      Correct: Directing read queries to read replicas offloads the primary database and improves read performance for read-heavy workloads.
      Incorrect: Retries or multi-AZ address availability rather than scaling reads, and using a read replica on EC2 is not a standard approach.
    tags: 
    difficulty: 
    points: 

  - id: q18
    type: multiple_choice
    question: A developer needs to modify an application architecture to meet new functional requirements. Application data is stored in Amazon DynamoDB and processed for analysis in a nightly batch. The system analysts do not want to wait unit the next day to view the processed data and have asked to have it available in near-real time. Which application architect pattern would enables the data to be processed as it is received?
    options:
     - text: Event driven.
       is_correct: true
     - text: Client served driven.
       is_correct: false
     - text: Fan-out driven.
       is_correct: false
     - text: Schedule driven.
       is_correct: false
    explanation: |
      Correct: An event-driven architecture processes data as it arrives using events and services that support near-real-time processing instead of nightly batches.
      Incorrect: The other patterns are schedule-based or otherwise do not guarantee immediate processing upon data arrival.
    tags: 
    difficulty: 
    points: 

  - id: q19
    type: multiple_choice
    question: A developer has built an application that inserts data into an Amazon DynamoDB table. The table is configured to use provisioned capacity. The application is deployed on a burstable nano Amazon EC2 Instance. The application logs show that the application has been failing because of a 'ProvisionedThroughputExceedException' error. Which actions should the developer take to resolve this issue? (Choose two.)
    options:
     - text: Move the application to a larger EC instance.
       is_correct: false
     - text: Increase the number or read capacity units (RCUs) that are provisioned for the DynamoDB table.
       is_correct: false
     - text: Reduce the frequency of requests to DynamoDB by implement ng exponential backoff.
       is_correct: true
     - text: Increase the frequency of requests to DynamoDB by decreasing the retry delay.
       is_correct: false
     - text: Change the capacity mode of the DynamoDB table from provisioned to on-demand.
       is_correct: true
    explanation: |
      Correct: Implementing exponential backoff reduces request collisions with provisioned throughput, and switching to on-demand mode lets the table scale automatically to handle spikes.
      Incorrect: Moving the instance or changing retry behavior to increase requests does not address DynamoDB throughput limits; although increasing RCUs could help, it was not selected in this question set.
    tags: 
    difficulty: 
    points: 

  - id: q20
    type: multiple_choice
    question: A software company needs to make sure user-uploaded documents are securely stored in Amazon S3. The documents must be encrypted at rest in Amazon S3. The company does not want to manage the security infrastructure in-house, but the company still needs extra protection to ensure it has control over its encryption keys due to industry regulations. Which encryption strategy should a developer use to meet these requirements?
    options:
     - text: Server-side encryption with Amazon S3 managed keys (SSE-S3).
       is_correct: false
     - text: Server-side encryption with customer-provided encryption keys (SSE-C).
       is_correct: false
     - text: Server-side encryption with AWS KMS managed keys (SSE-KMS).
       is_correct: true
     - text: Client-side encryption.
       is_correct: false
    explanation: |
      Correct: SSE-KMS offers KMS-managed keys with additional control and auditing while offloading key management to AWS, meeting regulatory needs without fully managing keys in-house.
      Incorrect: SSE-S3 gives less control/auditability, SSE-C requires the customer to manage keys, and client-side encryption shifts responsibility to the customer.
    tags: 
    difficulty: 
    points: 

  - id: q21
    type: multiple_choice
    question: An application uses Amazon Kinesis Data Streams to ingest and process large streams of data records in real time. Amazon EC2 instances consume and process the data from the shards of the Kinesis data stream by using Amazon Kinesis Client Library (KCL). The application handles the failure scenarios and does not require standby workers. The application reports that a specific shard is receiving more data than expected. To adapt to the changes in the rate of data flow, the 'hot' shard is resharded. Assuming that the initial number of shards in the Kinesis data stream is 4, and after resharding the number of shards increased to 6, what is the maximum number of EC2 instances that can be deployed to process data from all the shards?
    options:
     - text: 12.
       is_correct: false
     - text: 6.
       is_correct: true
     - text: 4.
       is_correct: false
     - text: 1.
       is_correct: false
    explanation: |
      Correct: After resharding there are 6 shards; each shard can be processed by one worker, so the maximum number of instances needed to process all shards is equal to the shard count (6).
      Incorrect: The other numbers do not match the shard count after resharding and are therefore incorrect.
    tags: 
    difficulty: 
    points: 

  - id: q22
    type: multiple_choice
    question: A gaming company is developing a mobile game application for iOS's and Android's platforms. This mobile game securely stores user data locally on the device. The company wants to allow users to use multiple device for the game, which requires user data synchronization across device.Which service should be used to synchronize user data across devices without the need to create a backend application?
    options:
     - text: AWS Lambda.
       is_correct: false
     - text: Amazon S3.
       is_correct: false
     - text: Amazon DynamoDB.
       is_correct: false
     - text: Amazon Cognito.
       is_correct: true
    explanation: |
      Correct: Amazon Cognito provides synchronization features for user data across devices without requiring a full backend, enabling preferences and user data sync.
      Incorrect: The other services do not provide the built-in device data synchronization feature without additional backend development.
    tags: 
    difficulty: 
    points: 

  - id: q23
    type: multiple_choice
    question: A Developer is making changes to a custom application that is currently using AWS Elastic Beanstalk. After the Developer completes the changes, what solutions will update the Elastic Beanstalk environment with the new application version? (Choose TWO)
    options:
     - text: Package the application code into a '.zip' file, and upload, then deploy the packaged application from the AWS Management Console.
       is_correct: true
     - text: Package the application code into a '.tar' file, create a new application version from the AWS Management Console, then update the environment by using AWS CLI.
       is_correct: false
     - text: Package the application code into a '.tar' file, and upload and deploy the packaged application from the AWS Management Console.
       is_correct: false
     - text: Package the application code into a '.zip' file, create a new application version from the packaged application by using AWS CLI, then update the environment by using AWS CLI.
       is_correct: true
     - text: Package the application code into a '.zip' file, create a new application version from the AWS Management Console, then rebuild the environment by using AWS CLI.
       is_correct: false
    explanation: |
      Correct: The correct options describe supported methods to create an application version (zip and upload via console or create via CLI) and then update the Elastic Beanstalk environment.
      Incorrect: The incorrect options use unsupported formats or unnecessary steps (tar in the console flow, or rebuilding the environment) and are not the standard procedures.
    tags: 
    difficulty: 
    points: 

  - id: q24
    type: multiple_choice
    question: A company is running an application built on AWS Lambda functions. One Lambda function has performance issues when it has to download a 50MB file from the Internet in every execution. This function is called multiple times a second. What solution would give the BEST performance increase?
    options:
     - text: Cache the file in the '/tmp' directory.
       is_correct: true
     - text: Increase the Lambda maximum execution time.
       is_correct: false
     - text: Put an Elastic Load Balancer in front of the Lambda function.
       is_correct: false
     - text: Cache the file in Amazon S3.
       is_correct: false
    explanation: |
      Correct: Caching the file in '/tmp' allows reusing the file between invocations within the same Lambda container and reduces repeated downloads, improving performance.
      Incorrect: Increasing timeout, placing an ELB in front of Lambda, or caching in S3 do not avoid repeated downloads in the same cost-effective way.
    tags: 
    difficulty: 
    points: 

  - id: q25
    type: multiple_choice
    question: Queries to an Amazon DynamoDB table are consuming a large amount of read capacity. The table has a significant number of large attributes. The application does not need all of the attribute data. How can DynamoDB costs be minimized while maximizing application performance?
    options:
     - text: Batch all the writes, and perform the write operations when no or few reads are being performed.
       is_correct: false
     - text: Create a global secondary index with a minimum set of projected attributes.
       is_correct: true
     - text: Implement exponential backoffs in the application.
       is_correct: false
     - text: Load balance the reads to the table using an Application Load Balancer.
       is_correct: false
    explanation: |
      Correct: Creating a GSI with a minimal set of projected attributes returns only the required data for queries and reduces RCU consumption while improving performance.
      Incorrect: The other options do not directly reduce the amount of data returned per query or are not appropriate solutions for minimizing read capacity usage.
    tags: 
    difficulty: 
    points: 

  - id: q26
    type: multiple_choice
    question: A Developer is writing a REST service that will add items to a shopping list. The service is built on Amazon API Gateway with AWS Lambda integrations. The shopping list items are send as query string parameters in the method request. How should the Developer convert the query string parameters to arguments for the Lambda function?
    options:
     - text: Enable request validation.
       is_correct: false
     - text: Include the Amazon Resource Name (ARN) of the Lambda function.
       is_correct: false
     - text: Change the integration type.
       is_correct: false
     - text: Create a mapping template.
       is_correct: true
    explanation: |
      Correct: Creating a mapping template transforms query string parameters into the JSON payload or format expected by the Lambda handler.
      Incorrect: Validation, ARNs, or changing integration type do not perform the transformation of query string parameters into handler arguments.
    tags: 
    difficulty: 
    points: 

  - id: q27
    type: multiple_choice
    question: A development team is creating a new application designed to run on AWS. While the test and production environments will run on Amazon EC2 instances, developers will each run their own environment on their laptops. Which of the following is the simplest and MOST secure way to access AWS services from the local development machines?
    options:
     - text: Use an IAM role to assume a role and execute API calls using the role.
       is_correct: false
     - text: Create an IAM user to be shared with the entire development team, provide the development team with the access key.
       is_correct: false
     - text: Create an IAM user for each developer on the team; provide each developer with a unique access key.
       is_correct: true
     - text: Set up a federation through an Amazon Cognito user pool.
       is_correct: false
    explanation: |
      Correct: Creating an IAM user per developer gives each person unique credentials and auditability while keeping the approach simple and secure for local development.
      Incorrect: Sharing a single user is insecure, and assuming roles or setting up federation are more complex approaches not necessary for simple developer access scenarios.
    tags: 
    difficulty: 
    points: 

  - id: q28
    type: multiple_choice
    question: How is provisioned throughput affected by the chosen consistency model when reading data from a DynamoDB table?
    options:
     - text: Strongly consistent reads use the same amount of throughput as eventually consistent reads.
       is_correct: false
     - text: Strongly consistent reads use more throughput than eventually consistent reads.
       is_correct: true
     - text: Strongly consistent reads use less throughput than eventually consistent reads.
       is_correct: false
     - text: Strongly consistent reads use variable throughput depending on read activity.
       is_correct: false
    explanation: |
      Correct: Strongly consistent reads consume more RCUs than eventually consistent reads because they require immediate confirmation of the latest data.
      Incorrect: The other statements either deny this difference or imply variable behavior that does not describe the throughput cost of strongly consistent reads.
    tags: 
    difficulty: 
    points: 

  - id: q29
    type: multiple_choice
    question: A developer needs to deploy a new version to an AWS Elastic Beanstalk application. How can the developer accomplish this task?
    options:
     - text: Upload and deploy the new application version in the Elastic Beanstalk console.
       is_correct: true
     - text: Use the eb init CLI command to deploy a new version.
       is_correct: false
     - text: Terminate the current Elastic Beanstalk environment and create a new one.
       is_correct: false
     - text: Modify the ebextensions folder to add a source option to services.
       is_correct: false
    explanation: |
      Correct: Uploading and deploying a new application version via the Elastic Beanstalk console is the supported flow for updating an application.
      Incorrect: The other options involve initialization, destructive environment replacement, or configuration changes that do not directly deploy a new version.
    tags: 
    difficulty: 
    points: 

  - id: q30
    type: multiple_choice
    question: A gaming application stores scores for players in an Amazon DynamoDB table that has four attributes; 'user_id', 'user_name', 'user_score', and 'user_rank'. The users are allowed to update their names only if a user is authenticated by web identity federation. Which set of conditions should be added in the policy attached to the role for the 'dynamodb:PutItem' API call?
    options:
     - text: Option A.
       is_correct: true
       img: images/question30_A.jpg
     - text: Option B.
       is_correct: false
       img: images/question30_B.jpg
     - text: Option C.
       is_correct: false
       img: images/question30_C.jpg
     - text: Option D.
       is_correct: false
       img: images/question30_D.jpg
    explanation: |
      Correct: Option A includes conditions that allow the action only when the user is authenticated via web identity and enforces the correct condition to limit updates to the 'user_name' attribute.
      Incorrect: The other options do not correctly implement web identity conditions or enforce attribute-level restrictions as required.
    tags: 
    difficulty: 
    points: 

  - id: q31
    type: multiple_choice
    question: A developer wants the ability to roll back to a previous version of an AWS Lambda function in the event of errors caused by a new deployment. How can the developer achieve this with MINIMAL impact on users?
    options:
     - text: Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to use the newly deployed version. If too many errors are encountered, point the alias back to the previous version.
       is_correct: false
     - text: Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to direct 10% of users to the newly deployed version. If too many errors are encountered, send 100% of traffic to the previous version.
       is_correct: true
     - text: Do not make any changes to the application Deploy the new version of the code. If too many errors are encountered, point the application back to the previous version using the version number in the Amazon Resource Name (ARN).
       is_correct: false
     - text: Create three aliases; new, existing, and router. Point the existing alias to the current version. Have the router alias direct 100% of users to the existing alias. Update the application to use the router alias. Deploy the new version of the code. Point the new alias to this version. Update the router alias to direct 10% of users to the new alias. If too many errors are encountered, send 100% of traffic to the existing alias.
       is_correct: false
    explanation: |
      Correct: The canary deployment strategy using aliases and shifting 10% of traffic allows a gradual rollout and quick rollback with minimal user impact if errors occur.
      Incorrect: The other options either do not implement a gradual rollout or are unnecessarily complex and do not provide the same low-impact rollback behavior.
    tags: 
    difficulty: 
    points: 

  - id: q32
    type: multiple_choice
    question: An application contains two components; one component to handle HTTP requests, and another component to handle background processing tasks. Each component must scale independently. The developer wants to deploy this application using AWS Elastic Beanstalk. How should this application be deployed, based on these requirements?
    options:
     - text: Deploy the application in a single Elastic Beanstalk environment.
       is_correct: false
     - text: Deploy each component in a separate Elastic Beanstalk environment.
       is_correct: true
     - text: Use multiple Elastic Beanstalk environments for the HTTP component but one environment for the background task component.
       is_correct: false
     - text: Use multiple Elastic Beanstalk environments for the background task component but one environment for the HTTP component.
       is_correct: false
    explanation: |
      Correct: Deploying each component in its own Elastic Beanstalk environment allows independent scaling according to each component's needs.
      Incorrect: Combining components in a single environment or unbalanced environment allocation does not provide independent scaling as required.
    tags: 
    difficulty: 
    points: 

  - id: q33
    type: multiple_choice
    question: A company is using AWS CloudFormation templates to deploy AWS resources. The company needs to update one of its AWS CloudFormation stacks. What can the company do to find out how the changes will impact the resources that are running?
    options:
     - text: Investigate the change sets.
       is_correct: true
     - text: Investigate the stack policies.
       is_correct: false
     - text: Investigate the 'Metadata' section.
       is_correct: false
     - text: Investigate the 'Resources' section.
       is_correct: false
    explanation: |
      Correct: Change sets show the differences and the impact of changes before applying an update to the stack.
      Incorrect: Stack policies, Metadata, or Resources sections do not provide a pre-update preview of resource changes like change sets do.
    tags: 
    difficulty: 
    points: 

  - id: q34
    type: multiple_choice
    question: A developer is creating a serverless web application and maintains different branches of code. The developer wants to avoid updating the Amazon API Gateway target endpoint each time a new code push is performed. What solution would allow the developer to perform a code push efficiently, without the need to update the API Gateway?
    options:
     - text: Associate different AWS Lambda functions to an API Gateway target endpoint.
       is_correct: false
     - text: Create different stages in API Gateway, then associate API Gateway with AWS Lambda.
       is_correct: false
     - text: Create aliases and versions in AWS Lambda.
       is_correct: true
     - text: Tag the AWS Lambda functions with different names.
       is_correct: false
    explanation: |
      Correct: Using Lambda aliases and versions lets the API Gateway point to an alias and you can update the alias to point to a new version without changing the gateway endpoint.
      Incorrect: Associating multiple functions or tagging functions does not provide the same transparent version routing without updating the gateway.
    tags: 
    difficulty: 
    points: 

  - id: q35
    type: multiple_choice
    question: An application running on EC2 instances is storing data in an S3 bucket. Security policy mandates that all data must be encrypted in transit. How can the Developer ensure that all traffic to the S3 bucket is encrypted?
    options:
     - text: Install certificates on the EC2 instances.
       is_correct: false
     - text: Create a bucket policy that allows traffic where 'SecureTransport' is 'true'
       is_correct: false
     - text: Create an HTTPS redirect on the EC2 instances.
       is_correct: false
     - text: Create a bucket policy that denies traffic where 'SecureTransport' is 'false'
       is_correct: true
    explanation: |
      Correct: A bucket policy that explicitly denies requests where 'SecureTransport' is false enforces that only HTTPS/TLS requests are accepted by the bucket.
      Incorrect: Installing certificates on instances or redirects do not enforce bucket-level transport security; allowing SecureTransport without denial does not prevent non-secure requests.
    tags: 
    difficulty: 
    points: 

  - id: q36
    type: multiple_choice
    question: A supplier is writing a new RESTful API for customers to query the status of orders. The customers requested the following API endpoint 'http://www.supplierdomain.com/status/customerID'. Which of the following application designs meet the requirements? (Select TWO)
    options:
     - text: Amazon SQS; Amazon SNS.
       is_correct: false
     - text: Elastic Load Balancing; Amazon EC2.
       is_correct: true
     - text: Amazon ElastiCache; Amazon Elacticsearch Service.
       is_correct: false
     - text: Amazon API Gateway; AWS Lambda.
       is_correct: true
     - text: Amazon S3; Amazon CloudFront.
       is_correct: false
    explanation: |
      Correct: ELB+EC2 and API Gateway+Lambda are valid architectures for providing a real-time REST endpoint, either with managed servers or serverless functions.
      Incorrect: The other options are more suited to messaging, caching/search, or static content and are not appropriate for a dynamic REST endpoint.
    tags: 
    difficulty: 
    points: 

  - id: q37
    type: multiple_choice
    question: A developer Is designing an AWS Lambda function that create temporary files that are less than 10 MB during execution. The temporary files will be accessed and modified multiple times during execution. The developer has no need to save or retrieve these files in the future. Where should the temporary file be stored?
    options:
     - text: the '/tmp' directory.
       is_correct: true
     - text: Amazon EFS.
       is_correct: false
     - text: Amazon EBS.
       is_correct: false
     - text: Amazon S3.
       is_correct: false
    explanation: |
      Correct: The '/tmp' directory is the local temporary storage available to Lambda functions and is appropriate for small files that are used during execution and do not need persistence.
      Incorrect: EFS, EBS, and S3 are persistent or external storage solutions that are unnecessary and more complex for this temporary use case.
    tags: 
    difficulty: 
    points: 

  - id: q38
    type: multiple_choice
    question: A website's page load times are gradually increasing as more users access the system at the same time. Analysis indicates that a user profile is being loaded from a database in all the web pages being visited by each user and this is increasing the database load and the page load latency. To address this issue the Developer decides to cache the user profile data. Which caching strategy will address this situation MOST efficiently?
    options:
     - text: Create a new Amazon EC2 Instance and run a NoSQL database on it. Cache the profile data within this database using the write-through caching strategy.
       is_correct: false
     - text: Create an Amazon ElastiCache cluster to cache the user profile data. Use a cache-aside caching strategy.
       is_correct: true
     - text: Use a dedicated Amazon RDS instance for caching profile data. Use a write-through caching strategy.
       is_correct: false
     - text: Create an ElastiCache cluster to cache the user profile data. Use a write-through caching strategy.
       is_correct: false
    explanation: |
      Correct: Using ElastiCache with a cache-aside strategy allows the application to read from cache when available and update the cache when needed, reducing database load and page latency.
      Incorrect: The other options propose heavier or less appropriate solutions (running NoSQL on EC2, using RDS as a cache, or write-through caching that can increase write latency) and are not as efficient for this case.
    tags: 
    difficulty: 
    points: 

  - id: q39
    type: multiple_choice
    question: An advertising company has a dynamic website with heavy traffic. The company wants to migrate the website infrastructure to AWS to handle everything except website development. Which solution BEST meets these requirements?
    options:
     - text: Use AWS VM Import to migrate a web server image to AWS Launch the image on a compute-optimized Amazon EC2 instance.
       is_correct: false
     - text: Launch multiple Amazon Lightsail instance behind a load balancer. Set up the website on those instances.
       is_correct: false
     - text: Deploy the website code in an AWS Elastic Beanstalk environment. Use Auto Scaling to scale the numbers of instance.
       is_correct: true
     - text: Use Amazon S3 to host the website. Use Amazon CloudFornt to deliver the content at scale.
       is_correct: false
    explanation: |
      Correct: Elastic Beanstalk with Auto Scaling lets the company offload infrastructure management, scaling, and deployments to the platform while focusing on website development.
      Incorrect: VM import, Lightsail, or S3/CloudFront (for static sites) do not meet the requirement to handle a dynamic, managed web application as effectively.
    tags: 
    difficulty: 
    points: 

  - id: q40
    type: multiple_choice
    question: A developer is writing an AWS Lambda function. The developer wants to log key events that occur during the Lambda function and include a unique identifier to associate the events with a specific function invocation. Which of the following will help the developer accomplish this objective?
    options:
     - text: Obtain the request identifier from the Lambda context object. Architect the application to write logs to the console.
       is_correct: true
     - text: Obtain the request identifier from the Lambda event object. Architect the application to write logs to a file.
       is_correct: false
     - text: Obtain the request identifier from the Lambda event object. Architect the application to write logs to the console.
       is_correct: false
     - text: Obtain the request identifier from the Lambda context object. Architect the application to write logs to a file.
       is_correct: false
    explanation: |
      Correct: The Lambda context object contains a request ID for the invocation; writing logs to the console lets CloudWatch capture them and associate logs with that request ID.
      Incorrect: The event object may not contain a unique request ID and writing logs to a file is not the recommended integration pattern for CloudWatch Logs.
    tags: 
    difficulty: 
    points: 

  - id: q41
    type: multiple_choice
    question: A company stores all personally identifiable information (PII) in an Amazon DynamoDB table named PII in Account A. An application running on Amazon EC2 instances in Account B requires access to the PII table. An administrators in Account A created an IAM role named AccessPII with privileges to access the PII table, and made account B a trusted entity. Which combination of actional steps should Developers take to access the table? (Select TWO)
    options:
     - text: Allow the EC2 IAM role the permission to assume the AccessPII role.
       is_correct: true
     - text: Allow the EC2 IAM role the permission to access the PII table.
       is_correct: false
     - text: Include the AWS API in the application code logic to obtain temporary credentials from the EC2 IAM role to access the PII table.
       is_correct: false
     - text: Include the 'AssumeRole' API operation in the application code logic to obtain temporary credentials to access the PII table.
       is_correct: true
     - text: Include the GetSessionToken API operation in the application code logic to obtain temporary credentials to access the PII table.
       is_correct: false
    explanation: |
      Correct: The EC2 role must be permitted to assume the AccessPII role, and the application should call 'AssumeRole' to obtain temporary credentials that allow access to the PII table in the other account.
      Incorrect: Granting direct access to the EC2 role in the other account or using GetSessionToken is not the correct cross-account delegation approach in this scenario.
    tags: 
    difficulty: 
    points: 

  - id: q42
    type: multiple_choice
    question: An AWS Lambda function accesses two Amazon DynamoDB tables. A developer wants to improve the performance of the Lambda function by identifying bottlenecks in the function. How can the developer inspect the timing of the DynamoDB API calls?
    options:
     - text: Add DynamoDB as an event source to the Lambda function. View the performance with Amazon CloudWatch metrics.
       is_correct: false
     - text: Place an Application Load Balancer (ALB) in front of the two DynamoDB tables. Inspect the ALB logs.
       is_correct: false
     - text: Limit Lambda to no more than five concurrent invocations Monitor from the Lambda console.
       is_correct: false
     - text: Enable AWS X-Ray tracing for the function. View the traces from the X-Ray service.
       is_correct: true
    explanation: |
      Correct: Enabling AWS X-Ray tracing for the Lambda function provides distributed traces that show the time spent in DynamoDB API calls and helps identify bottlenecks.
      Incorrect: The other options do not provide detailed tracing of DynamoDB API call timings.
    tags: 
    difficulty: 
    points: 

  - id: q43
    type: multiple_choice
    question: An Amazon RDS database instance is used by many applications to look up historical data. The query rate is relatively constant. When the historical data is updated each day, the resulting write traffic slows the read query performance and affects all application users. What can be done to eliminate the performance impact on application users?
    options:
     - text: Make sure Amazon RDS is Multi-AZ so it can better absorb increased traffic.
       is_correct: false
     - text: Create an RDS Read Replica and direct all read traffic to the replica.
       is_correct: true
     - text: Implement Amazon ElastiCache in front of Amazon RDS to buffer the write traffic.
       is_correct: false
     - text: Use Amazon DynamoDB instead of Amazon RDS to buffer the read traffic.
       is_correct: false
    explanation: |
      Correct: Creating a read replica and directing read traffic to it isolates read load from write activity on the primary, eliminating the write-induced slowdown for readers.
      Incorrect: Multi-AZ improves availability, ElastiCache does not address write contention without a cache invalidation strategy, and migrating to DynamoDB is a large architectural change.
    tags: 
    difficulty: 
    points: 

  - id: q44
    type: multiple_choice
    question: A company is developing a serverless ecommerce web application. The application needs to make coordinated, all-or-nothing changes to multiple items in the company's inventory table in Amazon DynamoDB. Which solution will meet these requirements?
    options:
     - text: Enable transactions for the DynamoDB table. Use the 'BatchWriteItem' operation to update the items.
       is_correct: false
     - text: Use the 'TransactWriteitems' operation to group the changes. Update the items in the table.
       is_correct: true
     - text: Set up a FIFO queue using Amazon SQS. Group the changes in the queue. Update the table based on the grouped changes.
       is_correct: false
     - text: Create a transaction table in an Amazon Aurora DB cluster to manage the transactions. Write a backend process to sync the Aurora DB table and the DynamoDB table.
       is_correct: false
    explanation: |
      Correct: 'TransactWriteItems' provides atomic, all-or-nothing writes across multiple items in DynamoDB.
      Incorrect: 'BatchWriteItem' is not transactional, and the other solutions add complexity without providing native atomic transactions.
    tags: 
    difficulty: 
    points: 

  - id: q45
    type: multiple_choice
    question: An application is running on an EC2 instance. The Developer wants to store an application metric in Amazon CloudWatch. What is the best practice for implementing this requirement?
    options:
     - text: Use the PUT Object API call to send data to an S3 bucket. Use an event notification to invoke a Lambda function to publish data to CloudWatch.
       is_correct: false
     - text: Publish the metric data to an Amazon Kinesis Stream using a 'PutRecord' API call. Subscribe a Lambda function that publishes data to CloudWatch.
       is_correct: false
     - text: Use the CloudWatch 'PutMetricData' API call to submit a custom metric to CloudWatch. Provide the required credentials to enable the API call.
       is_correct: false
     - text: Use the CloudWatch 'PutMetricData' API call to submit a custom metric to CloudWatch. Launch the EC2 instance with the required IAM role to enable the API call.
       is_correct: true
    explanation: |
      Correct: Use 'PutMetricData' with the EC2 instance launched with an IAM role that grants the necessary permissions so the instance can securely send metrics without embedded credentials.
      Incorrect: The other approaches are indirect or add unnecessary complexity compared to the recommended role+PutMetricData pattern.
    tags: 
    difficulty: 
    points: 

  - id: q46
    type: multiple_choice
    question: A Developer needs to design an application running on AWS that will be used to consume Amazon SQS messages that range from 1 KB up to 1GB in size. How should the Amazon SQS messages be managed?
    options:
     - text: Use Amazon S3 and the Amazon SQS CLI.
       is_correct: false
     - text: Use Amazon S3 and the Amazon SQS Extended Client Library for Java.
       is_correct: true
     - text: Use Amazon EBS and the Amazon SQS CLI.
       is_correct: false
     - text: Use Amazon EFS and the Amazon SQS CLI.
       is_correct: false
    explanation: |
      Correct: The SQS Extended Client Library stores large payloads in S3 and places a reference in the SQS message, which is the recommended approach for very large messages.
      Incorrect: EBS/EFS or using the SQS CLI do not provide the integrated solution for handling messages up to 1GB.
    tags: 
    difficulty: 
    points: 

  - id: q47
    type: multiple_choice
    question: A developer has written a multi-threaded application that is running on a fleet of Amazon EC2 instances. The operations team has requested a graphical method to monitor the number of running threads over time. What is the MOST efficient way to fulfill this request?
    options:
     - text: Periodically send the thread count to AWS X-Ray segments, then generate a service graph on demand.
       is_correct: false
     - text: Create a custom Amazon CloudWatch metric and periodically perform a 'PutMetricData' call with the current thread count.
       is_correct: true
     - text: Periodically log thread count data to Amazon S3. Use Amazon Kinesis to process the data into a graph.
       is_correct: false
     - text: Periodically write the current thread count to a table using Amazon DynarnoDB and use Amazon CloudFront to create a graph.
       is_correct: false
    explanation: |
      Correct: Publishing a custom CloudWatch metric via 'PutMetricData' lets the team visualize the thread count over time in CloudWatch dashboards.
      Incorrect: The other options are overly complex or not designed to provide direct time-series visualization of a metric.
    tags: 
    difficulty: 
    points: 

  - id: q48
    type: multiple_choice
    question: The Lambda function below is being called through an API using Amazon API Gateway. The average execution time for the Lambda function is about 1 second. The pseudocode for the Lambda function is as shown in the exhibit. What two actions can be taken to improve the performance of this Lambda function without increasing the cost of the solution? (Select TWO)
    img: images/question48.jpg
    options:
     - text: Package only the modules the Lambda function requires.
       is_correct: true
     - text: Use Amazon DynamoDB instead of Amazon RDS.
       is_correct: false
     - text: Move the initialization of the variable Amazon RDS connection outside of the handler function.
       is_correct: true
     - text: Implement custom database connection pooling with the Lambda function.
       is_correct: false
     - text: Implement local caching of Amazon RDS data so Lambda can re-use the cache.
       is_correct: false
    explanation: |
      Correct: Reducing package size and moving RDS connection initialization outside the handler reduce cold start time and avoid recreating connections on each invocation, improving latency without extra cost.
      Incorrect: Replacing RDS with DynamoDB, custom pooling, or local caching introduce architectural changes or complexity and are not the most direct low-cost improvements.
    tags: 
    difficulty: 
    points: 

  - id: q49
    type: multiple_choice
    question: An application on AWS is using third-party APIs. The Developer needs to monitor API errors in the code, and wants to receive notifications if failures go above a set threshold value. How can the Developer achieve these requirements?
    options:
     - text: Publish a custom metric on Amazon CloudWatch and use Amazon Simple Email Service (SES) for notification.
       is_correct: false
     - text: Use an Amazon CloudWatch API-error metric and use Amazon Simple Notification Service (SNS) for notification.
       is_correct: false
     - text: Use an Amazon CloudWatch API-error metric and use Amazon SES for notification.
       is_correct: false
     - text: Publish a custom metric on Amazon CloudWatch and use Amazon SNS for notification.
       is_correct: true
    explanation: |
      Correct: Publishing a custom CloudWatch metric for API errors and using SNS for notifications allows creating alarms and delivering alerts to subscribers (email, SMS, etc.).
      Incorrect: The other combinations either use SES inappropriately for alarms or assume a built-in API-error metric that may not meet the custom monitoring requirement.
    tags: 
    difficulty: 
    points: 

  - id: q50
    type: multiple_choice
    question: The release process workflow of an application requires a manual approval before the code is deployed into the production environment. What is the BEST way to achieve this using AWS CodePipeline?
    options:
     - text: Use multiple pipelines to allow approval.
       is_correct: false
     - text: Use an approval action in a stage.
       is_correct: true
     - text: Disable the stage transition to allow manual approval.
       is_correct: false
     - text: Disable a stage just prior the deployment stage.
       is_correct: false
    explanation: |
      Correct: Adding an approval action in a pipeline stage is the native CodePipeline mechanism to require human approval before progressing to deployment.
      Incorrect: The other approaches are workarounds or impractical and do not provide the controlled approval workflow offered by an approval action.
    tags: 
    difficulty: 
    points: 
