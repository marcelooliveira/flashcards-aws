questions:
  - id: q451
    type: multiple_choice
    question: A company is migrating its applications and databases to the AWS Cloud. The company will use Amazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS. Which activities will be managed by the company's operational team? (Choose three.)
    options:
     - text: Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection
       is_correct: true
     - text: Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window
       is_correct: true
     - text: Encryption of the data that moves in transit through Direct Connect
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: The company's operational team is responsible for configuring additional software components on Amazon ECS, such as monitoring tools, patch management tools, log management systems, and host intrusion detection systems. These components are often specific to the company's requirements and policies. The operational team is also responsible for creating Amazon RDS DB instances, configuring parameters, and setting up maintenance windows based on the company's operational needs. Additionally, while AWS manages the physical infrastructure of Direct Connect, the company's operational team is responsible for configuring encryption for the data in transit over Direct Connect.
      Incorrect: 
        "***replace later***"

  - id: q452
    type: multiple_choice
    question: A company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and takes 10 seconds to run. The job runs on a scheduled interval and consumes 1 GB of memory. The CPU utilization of the instance is low except for short surges during which the job uses the maximum CPU available. The company wants to optimize the costs to run the job. Which solution will meet these requirements?
    options:
     - text: Copy the code into an AWS Lambda function that has 1 GB of memory. Create an Amazon EventBridge scheduled rule to run the code each hour.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. It automatically scales based on the number of requests, making it cost-effective for sporadic workloads. Amazon EventBridge allows you to schedule events at specified intervals. By creating a scheduled rule, you can trigger the Lambda function to run the Java-based job every hour.
      Incorrect: 
        "***replace later***"

  - id: q453
    type: multiple_choice
    question: A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 buckets. Because of regulatory requirements, the company must retain backup files for a specific time period. The company must not alter the files for the duration of the retention period. Which solution will meet these requirements?
    options:
     - text: Use AWS Backup to create a backup vault that has a vault lock in compliance mode. Create the required backup plan.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Backup provides a centralized solution for managing backups across various AWS services, including Amazon EC2. By creating a backup vault with a vault lock in compliance mode, the company ensures that the backup files are retained and cannot be altered for the duration of the retention period. Compliance mode is designed to meet regulatory requirements for data retention.
      Incorrect: 
        "***replace later***"

  - id: q454
    type: multiple_choice
    question: A company has resources across multiple AWS Regions and accounts. A newly hired solutions architect discovers a previous employee did not provide details about the resources inventory. The solutions architect needs to build and map the relationship details of the various workloads across all accounts. Which solution will meet these requirements in the MOST operationally efficient way?
    options:
     - text: Use Workload Discovery on AWS to generate architecture diagrams of the workloads.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS has a service called AWS Well-Architected Tool, which includes Workload Discovery. Workload Discovery automatically discovers and visualizes the architecture of your workloads. It provides architecture diagrams, best practice recommendations, and insights into your workloads.
      Incorrect: 
        "***replace later***"

  - id: q455
    type: multiple_choice
    question: A company uses AWS Organizations. The company wants to operate some of its AWS accounts with different budgets. The company wants to receive alerts and automatically prevent provisioning of additional resources on AWS accounts when the allocated budget threshold is met during a specific period. Which combination of solutions will meet these requirements? (Choose three.)
    options:
     - text: Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the required AWS accounts.
       is_correct: true
     - text: Create an IAM role for AWS Budgets to run budget actions with the required permissions.
       is_correct: true
     - text: Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Budgets allows you to create budgets and set thresholds for spending. By creating an IAM role for AWS Budgets, you can enable budget actions with the required permissions. Adding alerts and budget actions ensures that the company is notified when thresholds are met and can prevent additional resource provisioning.
      Incorrect: 
        "***replace later***"

  - id: q456
    type: multiple_choice
    question: A company runs applications on Amazon EC2 instances in one AWS Region. The company wants to back up the EC2 instances to a second Region. The company also wants to provision EC2 resources in the second Region and manage the EC2 instances centrally from one AWS account. Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Create a backup plan by using AWS Backup. Configure cross-Region backup to the second Region for the EC2 instances.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Backup is a centralized backup service that allows you to create backup plans for various AWS resources, including EC2 instances. With AWS Backup, you can configure cross-Region backups, meaning you can replicate backups from one AWS Region to another. This provides a cost-effective and centralized solution for backup.
      Incorrect: 
        "***replace later***"

  - id: q457
    type: multiple_choice
    question: A company that uses AWS is building an application to transfer data to a product manufacturer. The company has its own identity provider (IdP). The company wants the IdP to authenticate application users while the users use the application to transfer data. The company must use Applicability Statement 2 (AS2) protocol. Which solution will meet these requirements?
    options:
     - text: Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP authentication.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Transfer Family is a fully managed service that allows you to transfer files over the internet using a range of protocols, including AS2. You can integrate AWS Transfer Family with your IdP for user authentication. By using a Lambda function, you can customize the authentication process and integrate it with your own IdP.
      Incorrect: 
        "***replace later***"

  - id: q458
    type: multiple_choice
    question: A solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback service. The application requires 1 GB of memory and 2 GB of storage for its computation resources. The application will require that the data is in a relational format. Which additional combination ofAWS services will meet these requirements with the LEAST administrative effort? (Choose two.)
    options:
     - text: AWS Lambda
       is_correct: true
     - text: Amazon RDS
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Lambda is a serverless compute service that automatically scales based on the number of requests and executes your code without requiring you to provision or manage servers. It's event-driven, and you pay only for the compute time consumed. For a REST API, Lambda can be a low-administration solution compared to managing infrastructure directly. Amazon RDS (Relational Database Service) is a fully managed relational database service that simplifies database administration tasks. It provides options for popular database engines like MySQL, PostgreSQL, Oracle, and Microsoft SQL Server. You can easily provision, scale, and manage a relational database without dealing with the underlying infrastructure.
      Incorrect: 
        "***replace later***"

  - id: q459
    type: multiple_choice
    question: A company uses AWS Organizations to run workloads within multiple AWS accounts. A tagging policy adds department tags to AWS resources when the company creates tags. An accounting team needs to determine spending on Amazon EC2 consumption. The accounting team must determine which departments are responsible for the costs regardless ofAWS account. The accounting team has access to AWS Cost Explorer for all AWS accounts within the organization and needs to access all reports from Cost Explorer. Which solution meets these requirements in the MOST operationally efficient way?
    options:
     - text: From the Organizations management account billing console, activate a user-defined cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: While AWS provides AWS-defined tags, the use of a user-defined tag provides flexibility in terms of naming and tagging conventions. Activating the tag at the Organizations management account level ensures that the tag is applied to resources across all member accounts.
      Incorrect: 
        "***replace later***"

  - id: q460
    type: multiple_choice
    question: A company wants to securely exchange data between its software as a service (SaaS) application Salesforce account and Amazon S3. The company must encrypt the data at rest by using AWS Key Management Service (AWS KMS) customer managed keys (CMKs). The company must also encrypt the data in transit. The company has enabled API access for the Salesforce account. Which solution will meet these requirements?
    options:
     - text: Create Amazon AppFlow flows to transfer the data securely from Salesforce to Amazon S3.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon AppFlow is a fully managed integration service that allows you to securely transfer data between AWS services and SaaS applications like Salesforce. It supports data encryption both in transit and at rest. With AppFlow, you can configure the integration flow, including source (Salesforce) and destination (Amazon S3), and set up encryption options. It simplifies the data transfer process and can handle the encryption requirements without the need for custom development.
      Incorrect: 
        "***replace later***"

  - id: q461
    type: multiple_choice
    question: A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple Amazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon DynamoDB. The app communicates by using TCP traffic and UDP traffic between the users and the servers. The application will be used globally. The company wants to ensure the lowest possible latency for all users. Which solution will meet these requirements?
    options:
     - text: Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Global Accelerator is a service that improves the availability and performance of your applications with global users. By creating an accelerator and integrating it with a Network Load Balancer (NLB), you can ensure low latency for TCP and UDP traffic globally.
      Incorrect: 
        "***replace later***"

  - id: q462
    type: multiple_choice
    question: A company has an application that processes customer orders. The company hosts the application on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. Occasionally when traffic is high the workload does not process orders fast enough. What should a solutions architect do to write the orders reliably to the database as quickly as possible?
    options:
     - text: Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon SQS, which is a fully managed message queuing service. Writing orders to an SQS queue allows for decoupling the EC2 instances processing the orders from the application writing the orders. EC2 instances in an Auto Scaling group can then read from the SQS queue, ensuring that the processing scales with demand. Using an Auto Scaling group ensures that you can dynamically adjust the number of EC2 instances based on the workload. This can help handle high traffic efficiently.
      Incorrect: 
        "***replace later***"

  - id: q463
    type: multiple_choice
    question: An IoT company is releasing a mattress that has sensors to collect data about a user’s sleep. The sensors will send data to an Amazon S3 bucket. The sensors collect approximately 2 MB of data every night for each mattress. The company must process and summarize the data for each mattress. The results need to be available as soon as possible. Data processing will require 1 GB of memory and will finish within 30 seconds. Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Use AWS Lambda with a Python script
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. It automatically scales with the number of requests, making it well-suited for event-driven workloads like processing data from IoT devices. Python is a lightweight and efficient language for data processing tasks. Lambda allows you to execute code in response to events, such as data arriving in the S3 bucket.
      Incorrect: 
        "***replace later***"

  - id: q464
    type: multiple_choice
    question: A company hosts an online shopping application that stores all orders in an Amazon RDS for PostgreSQL Single-AZ DB instance. Management wants to eliminate single points of failure and has asked a solutions architect to recommend an approach to minimize database downtime without requiring any changes to the application code. Which solution meets these requirements?
    options:
     - text: Convert the existing database instance to a Multi-AZ deployment by modifying the database instance and specifying the Multi-AZ option.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: By converting the existing RDS instance to a Multi-AZ deployment, you enable high availability with automatic failover. Amazon RDS will automatically replicate the database to a standby instance in a different Availability Zone (AZ). In the event of a failure, Amazon RDS will automatically promote the standby to the primary, minimizing downtime.
      Incorrect: 
        "***replace later***"

  - id: q465
    type: multiple_choice
    question: A company is developing an application to support customer demands. The company wants to deploy the application on multiple Amazon EC2 Nitro-based instances within the same Availability Zone. The company also wants to give the application the ability to write to multiple block storage volumes in multiple EC2 Nitro-based instances simultaneously to achieve higher application availability. Which solution will meet these requirements?
    options:
     - text: Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Provisioned IOPS SSD (io2) volumes do indeed support Multi-Attach, allowing you to attach a single volume to multiple Nitro-based instances in the same Availability Zone. This can be suitable for scenarios where multiple instances need simultaneous access to a shared volume with high performance.
      Incorrect: 
        "***replace later***"

  - id: q466
    type: multiple_choice
    question: A company designed a stateless two-tier application that uses Amazon EC2 in a single Availability Zone and an Amazon RDS Multi-AZ DB instance. New company management wants to ensure the application is highly available. What should a solutions architect do to meet this requirement?
    options:
     - text: Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: By configuring Multi-AZ EC2 Auto Scaling and using an Application Load Balancer, you can ensure high availability for the application. The Auto Scaling group ensures that instances are distributed across multiple Availability Zones, and the load balancer directs traffic to healthy instances.
      Incorrect: 
        "***replace later***"

  - id: q467
    type: multiple_choice
    question: A company uses AWS Organizations. A member account has purchased a Compute Savings Plan. Because of changes in the workloads inside the member account, the account no longer receives the full benefit of the Compute Savings Plan commitment. The company uses less than 50% of its purchased compute power. What should the company do to maximize the utilization of the Compute Savings Plan?
    options:
     - text: Turn on discount sharing from the Billing Preferences section of the account console in the company's Organizations management account.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: By turning on discount sharing in the Billing Preferences section of the management account, the company can share unused Compute Savings Plan benefits with other accounts in the organization, maximizing utilization.
      Incorrect: 
        "***replace later***"

  - id: q468
    type: multiple_choice
    question: A company is developing a microservices application that will provide a search catalog for customers. The company must use REST APIs to present the frontend of the application to users. The REST APIs must access the backend services that the company hosts in containers in private VPC subnets. Which solution will meet these requirements?
    options:
     - text: Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: By using Amazon API Gateway with a private VPC link, you can securely connect the REST API to backend services hosted in Amazon ECS within private subnets. This ensures secure communication without exposing the backend services to the internet.
      Incorrect: 
        "***replace later***"

  - id: q469
    type: multiple_choice
    question: A company stores raw collected data in an Amazon S3 bucket. The data is used for several types of analytics on behalf of the company's customers. The type of analytics requested determines the access pattern on the S3 objects. The company cannot predict or control the access pattern. The company wants to reduce its S3 costs. Which solution will meet these requirements?
    options:
     - text: Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: S3 Intelligent-Tiering is designed to optimize costs by automatically moving objects between two access tiers: frequent and infrequent access. It is well-suited for scenarios where access patterns are unpredictable. Using S3 Lifecycle rules to transition objects to S3 Intelligent-Tiering allows you to take advantage of automatic cost savings based on actual access patterns without the need for manual adjustments.
      Incorrect: 
        "***replace later***"

  - id: q470
    type: multiple_choice
    question: A company has applications hosted on Amazon EC2 instances with IPv6 addresses. The applications must initiate communications with other external applications using the internet. However the company’s security policy states that any external service cannot initiate a connection to the EC2 instances. What should a solutions architect recommend to resolve this issue?
    options:
     - text: Create an egress-only internet gateway and make it the destination of the subnet's route table.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: An egress-only internet gateway is used for IPv6 traffic leaving the VPC to reach the internet. It allows outbound communication initiated by resources inside the VPC but prevents incoming traffic initiated from the internet. Configuring the subnet's route table to use the egress-only internet gateway as the destination ensures that IPv6 traffic initiated from EC2 instances can reach external services while blocking unsolicited incoming traffic.
      Incorrect: 
        "***replace later***"

  - id: q471
    type: multiple_choice
    question: A company is creating an application that runs on containers in a VPC. The application stores and accesses data in an Amazon S3 bucket. During the development phase, the application will store and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs and wants to prevent traffic from traversing the internet whenever possible. Which solution will meet these requirements?
    options:
     - text: Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: A gateway VPC endpoint enables private connectivity between a VPC and Amazon S3. It allows direct access to Amazon S3 without the need for internet gateways, NAT devices, VPN connections, or AWS Direct Connect.
      Incorrect: 
        "***replace later***"

  - id: q472
    type: multiple_choice
    question: A company has a mobile chat application with a data store based in Amazon DynamoDB. Users would like new messages to be read with as little latency as possible. A solutions architect needs to design an optimal solution that requires minimal application changes. Which method should the solutions architect select?
    options:
     - text: Configure Amazon DynamoDB Accelerator (DAX) for the new messages table. Update the code to use the DAX endpoint.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon DynamoDB Accelerator (DAX) is an in-memory caching service for DynamoDB that helps improve the read performance of DynamoDB tables. By configuring DAX for the new messages table and updating the code to use the DAX endpoint, you can achieve low-latency reads with minimal application changes.
      Incorrect: 
        "***replace later***"

  - id: q473
    type: multiple_choice
    question: A company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website serves static content. Website traffic is increasing, and the company is concerned about a potential increase in cost. What should the company do to reduce costs?
    options:
     - text: Create an Amazon CloudFront distribution to cache static files at edge locations
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: By creating a CloudFront distribution and configuring it to cache static files, you can offload the delivery of static content to the CDN, reducing the load on the ALB and potentially lowering data transfer costs. CloudFront helps improve website performance and can be cost-effective due to its caching mechanism.
      Incorrect: 
        "***replace later***"

  - id: q474
    type: multiple_choice
    question: A company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions. Because of a recent application launch requirement, the company’s VPCs must communicate with all other VPCs across all Regions. Which solution will meet these requirements with the LEAST amount of administrative effort?
    options:
     - text: Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Transit Gateway is designed for simplifying the connectivity between multiple VPCs and on-premises networks. It allows for hub-and-spoke connectivity patterns, making it easier to manage communication across multiple VPCs. By using AWS Transit Gateway in a single Region to connect VPCs and enabling Transit Gateway peering across Regions, you can efficiently manage communication between VPCs in different Regions with centralized control and minimal administrative effort.
      Incorrect: 
        "***replace later***"

  - id: q475
    type: multiple_choice
    question: A company is designing a containerized application that will use Amazon Elastic Container Service (Amazon ECS). The application needs to access a shared file system that is highly durable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 hours. The file system needs to provide a mount target in each Availability Zone within a Region. A solutions architect wants to use AWS Backup to manage the replication to another Region. Which solution will meet these requirements?
    options:
     - text: Amazon Elastic File System (Amazon EFS) with the Standard storage class
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon EFS is a scalable file storage service that provides a shared file system with mount targets in multiple Availability Zones. By using the Standard storage class, you can ensure high durability and availability. AWS Backup can be used to manage cross-Region replication, meeting the RPO requirement of 8 hours.
      Incorrect: 
        "***replace later***"

  - id: q476
    type: multiple_choice
    question: A company is expecting rapid growth in the near future. A solutions architect needs to configure existing users and grant permissions to new users on AWS. The solutions architect has decided to create IAM groups. The solutions architect will add the new users to IAM groups based on department. Which additional action is the MOST secure way to grant permissions to the new users?
    options:
     - text: Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Creating an IAM policy that grants the least privilege required for the users' tasks is a security best practice. By attaching this policy to IAM groups, you ensure that new users added to these groups inherit the specific permissions defined in the policy.
      Incorrect: 
        "***replace later***"

  - id: q477
    type: multiple_choice
    question: IAM
    options:
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: "***replace later***"
      Incorrect: 
        "***replace later***"

  - id: q478
    type: multiple_choice
    question: A law firm needs to share information with the public. The information includes hundreds of files that must be publicly readable. Modifications or deletions of the files by anyone before a designated future date are prohibited. Which solution will meet these requirements in the MOST secure way?
    options:
     - text: Create a new Amazon S3 bucket with S3 Versioning enabled. Use S3 Object Lock with a retention period in accordance with the designated date. Configure the S3 bucket for static website hosting. Set an S3 bucket policy to allow read-only access to the objects.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: S3 Versioning helps maintain multiple versions of an object over time. With S3 Object Lock, you can enforce retention periods during which the objects cannot be modified or deleted. This aligns with the requirement to prohibit modifications or deletions before a designated future date.
      Incorrect: 
        "***replace later***"

  - id: q479
    type: multiple_choice
    question: A company is making a prototype of the infrastructure for its new website by manually provisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database. After the configuration has been thoroughly validated, the company wants the capability to immediately deploy the infrastructure for development and production use in two Availability Zones in an automated fashion. What should a solutions architect recommend to meet these requirements?
    options:
     - text: Define the infrastructure as a template by using the prototype infrastructure as a guide. Deploy the infrastructure with AWS CloudFormation.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS CloudFormation is a service specifically designed for defining and deploying AWS infrastructure as code using templates. In this case, you can create a CloudFormation template based on the validated prototype infrastructure, and then use CloudFormation to deploy and manage the infrastructure in an automated and repeatable way.
      Incorrect: 
        "***replace later***"

  - id: q480
    type: multiple_choice
    question: A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage. The chief information security officer has directed that no application traffic between the two services should traverse the public internet. Which capability should the solutions architect use to meet the compliance requirements?
    options:
     - text: VPC endpoint
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS provides VPC endpoints that allow you to privately connect your VPC to supported AWS services, including Amazon S3, without needing to use public IP addresses or traverse the public internet. With an S3 VPC endpoint, the traffic between your Amazon EC2 instances and Amazon S3 remains within the AWS network, providing a secure and private connection.
      Incorrect: 
        "***replace later***"

  - id: q481
    type: multiple_choice
    question: A company hosts a three-tier web application in the AWS Cloud. A Multi-AZ Amazon RDS for MySQL server forms the database layer. Amazon ElastiCache forms the cache layer. The company wants a caching strategy that adds or updates data in the cache when a customer adds an item to the database. The data in the cache must always match the data in the database. Which solution will meet these requirements?
    options:
     - text: Implement the write-through caching strategy
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: In a write-through caching strategy, data is always written or updated in the cache when it is modified in the database. This ensures that the cache is consistently updated with the latest data from the database. When a customer adds an item to the database, the write-through caching strategy ensures that the item is also added or updated in the cache.
      Incorrect: 
        "***replace later***"

  - id: q482
    type: multiple_choice
    question: A company wants to migrate 100 GB of historical data from an on-premises location to an Amazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on premises. The company needs to encrypt the data in transit to the S3 bucket. The company will store new data directly in Amazon S3. Which solution will meet these requirements with the LEAST operational overhead?
    options:
     - text: Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS DataSync is a service designed for efficiently transferring large amounts of data between on-premises storage systems and Amazon S3. It supports encryption of data in transit, ensuring the security of the data during the migration process. AWS DataSync is specifically built for data transfer scenarios and minimizes operational overhead, providing an efficient and straightforward solution.
      Incorrect: 
        "***replace later***"

  - id: q483
    type: multiple_choice
    question: A company containerized a Windows job that runs on .NET 6 Framework under a Windows container. The company wants to run this job in the AWS Cloud. The job runs every 10 minutes. The job’s runtime varies between 1 minute and 3 minutes. Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a scheduled task based on the container image of the job to run every 10 minutes.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon ECS is a fully managed container orchestration service, and AWS Fargate allows you to run containers without managing the underlying infrastructure. ECS on Fargate is a serverless option, which means you only pay for the vCPU and memory that you use, and it scales automatically to meet the needs of the job.
      Incorrect: 
        "***replace later***"

  - id: q484
    type: multiple_choice
    question: A company wants to move from many standalone AWS accounts to a consolidated, multi-account architecture. The company plans to create many new AWS accounts for different business units. The company needs to authenticate access to these AWS accounts by using a centralized corporate directory service. Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)
    options:
     - text: Create a new organization in AWS Organizations with all features turned on. Create the new AWS accounts in the organization.
       is_correct: true
     - text: Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization. Configure IAM Identity Center, and integrate it with the company's corporate directory service.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Creating a new organization in AWS Organizations with all features turned on is a foundational step for managing multiple AWS accounts in a consolidated manner. Setting up AWS IAM Identity Center (AWS Single Sign-On) simplifies and centralizes authentication across multiple AWS accounts, integrating with the company's corporate directory service.
      Incorrect: 
        "***replace later***"

  - id: q485
    type: multiple_choice
    question: A company is looking for a solution that can store video archives in AWS from old news footage. The company needs to minimize costs and will rarely need to restore these files. When the files are needed, they must be available in a maximum of five minutes. What is the MOST cost-effective solution?
    options:
     - text: Store the video archives in Amazon S3 Glacier and use Expedited retrievals.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon S3 Glacier is a cost-effective storage solution for rarely accessed data. Expedited retrievals allow you to access the data within a few minutes, meeting the requirement for availability within five minutes.
      Incorrect: 
        "***replace later***"

  - id: q486
    type: multiple_choice
    question: A company is building a three-tier application on AWS. The presentation tier will serve a static website. The logic tier is a containerized application. This application will store data in a relational database. The company wants to simplify deployment and to reduce operational costs. Which solution will meet these requirements?
    options:
     - text: Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon S3 is a highly scalable and cost-effective storage service that can be used to host static content like a static website. AWS Fargate is a serverless compute engine for containers, simplifying deployment and reducing operational overhead. A managed Amazon RDS cluster provides a fully managed relational database solution.
      Incorrect: 
        "***replace later***"

  - id: q487
    type: multiple_choice
    question: A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system, be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC. Which storage solution meets these requirements?
    options:
     - text: Amazon Elastic File System (Amazon EFS) with multiple mount targets
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon EFS is a scalable file storage service that can be mounted by multiple Amazon EC2 instances and on-premises servers. It provides a shared file system with multiple mount targets in different Availability Zones (AZs) for high availability.
      Incorrect: 
        "***replace later***"

  - id: q488
    type: multiple_choice
    question: A 4-year-old media company is using the AWS Organizations all features feature set to organize its AWS accounts. According to the company's finance team, the billing information on the member accounts must not be accessible to anyone, including the root user of the member accounts. Which solution will meet these requirements?
    options:
     - text: Create a service control policy (SCP) to deny access to the billing information. Attach the SCP to the root organizational unit (OU).
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: SCPs in AWS Organizations allow you to set fine-grained permissions and controls over what actions can be performed in member accounts. By creating an SCP, you can explicitly deny access to billing information for all users, including the root user, under the specified organizational unit (OU).
      Incorrect: 
        "***replace later***"

  - id: q489
    type: multiple_choice
    question: An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders. The local data center team has detected that some of the order messages were not received. A solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days. Which solution will meet these requirements with the LEAST development effort?
    options:
     - text: Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon SNS allows you to set up a dead letter queue to capture and retain messages that cannot be delivered to the intended endpoint. When configuring a DLQ, you can specify an Amazon SQS queue as the target for messages that fail to be delivered. Amazon SQS provides message retention settings, and in this case, you can set the retention period to 14 days.
      Incorrect: 
        "***replace later***"

  - id: q490
    type: multiple_choice
    question: A gaming company uses Amazon DynamoDB to store user information such as geographic location, player data, and leaderboards. The company needs to configure continuous backups to an Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability of the application and must not affect the read capacity units (RCUs) that are defined for the table. Which solution meets these requirements?
    options:
     - text: Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Exporting data directly from DynamoDB to Amazon S3 with continuous backups ensures that the backups do not affect the availability of the application or the read capacity units. Point-in-time recovery provides additional protection by allowing you to restore the table to any point within the retention period.
      Incorrect: 
        "***replace later***"

  - id: q491
    type: multiple_choice
    question: A solutions architect is designing an asynchronous application to process credit card data validation requests for a bank. The application must be secure and be able to process each request at least once. Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Use Amazon SQS with server-side encryption (SSE) enabled to store the requests. Use an AWS Lambda function to process the requests.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon SQS with server-side encryption ensures that the data is secure while in transit and at rest. Using an AWS Lambda function to process the requests provides a cost-effective, serverless solution that ensures each request is processed at least once.
      Incorrect: 
        "***replace later***"

  - id: q492
    type: multiple_choice
    question: A company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts. Which solution will meet these requirements with the LEAST development effort?
    options:
     - text: Use AWS Organizations to organize the accounts into organizational units (OUs). Define and attach a service control policy (SCP) to control the usage of EC2 instance types.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Organizations allows you to consolidate multiple AWS accounts into an organization that you create and centrally manage. Organizational Units (OUs) can be used to group accounts based on different criteria, such as development, production, etc. Service Control Policies (SCPs) are used to set fine-grained permissions on AWS accounts within an organization. By defining an SCP and attaching it to the OUs containing the development accounts, you can restrict the EC2 instance types that can be launched.
      Incorrect: 
        "***replace later***"

  - id: q493
    type: multiple_choice
    question: A company wants to use artificial intelligence (AI) to determine the quality of its customer service calls. The company currently manages calls in four different languages, including English. The company will offer new languages in the future. The company does not have the resources to regularly maintain machine learning (ML) models. The company needs to create written sentiment analysis reports from the customer service call recordings. The customer service call recording text must be translated into English. Which combination of steps will meet these requirements? (Choose three.)
    options:
     - text: Use Amazon Transcribe to convert the audio recordings in any language into text.
       is_correct: true
     - text: Use Amazon Translate to translate text in any language to English.
       is_correct: true
     - text: Use Amazon Comprehend to create the sentiment analysis reports.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon Transcribe converts speech into text, Amazon Translate translates text into English, and Amazon Comprehend performs sentiment analysis. This combination meets the requirements without the need for maintaining custom ML models.
      Incorrect: 
        "***replace later***"

  - id: q494
    type: multiple_choice
    question: The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24 or 203.0.113.0/24.
    options:
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: "***replace later***"
      Incorrect: 
        "***replace later***"

  - id: q495
    type: multiple_choice
    question: A company is conducting an internal audit. The company wants to ensure that the data in an Amazon S3 bucket that is associated with the company’s AWS Lake Formation data lake does not contain sensitive customer or employee data. The company wants to discover personally identifiable information (PII) or financial information, including passport numbers and credit card numbers. Which solution will meet these requirements?
    options:
     - text: Configure Amazon Macie to run a data discovery job that uses managed identifiers for the required data types.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data like PII or financial information. By configuring Amazon Macie to run a data discovery job, you can use managed identifiers to search for specific types of sensitive data within the S3 bucket.
      Incorrect: 
        "***replace later***"

  - id: q496
    type: multiple_choice
    question: A company uses on-premises servers to host its applications. The company is running out of storage capacity. The applications use both block storage and NFS storage. The company needs a high-performing solution that supports local caching without re-architecting its existing applications. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)
    options:
     - text: Deploy an AWS Storage Gateway file gateway to replace NFS storage.
       is_correct: true
     - text: Deploy an AWS Storage Gateway volume gateway to replace the block storage.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS Storage Gateway file gateway provides a solution for NFS storage, while the volume gateway provides a solution for block storage. Both options support local caching and do not require re-architecting existing applications.
      Incorrect: 
        "***replace later***"

  - id: q497
    type: multiple_choice
    question: A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs. Which solution will meet these requirements MOST cost-effectively?
    options:
     - text: Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 traffic.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: A VPC gateway endpoint enables private connectivity between a VPC and Amazon S3, eliminating the need for a NAT gateway and reducing data transfer costs.
      Incorrect: 
        "***replace later***"

  - id: q498
    type: multiple_choice
    question: A company uses Amazon S3 to store high-resolution pictures in an S3 bucket. To minimize application changes, the company stores the pictures as the latest version of an S3 object. The company needs to retain only the two most recent versions of the pictures. The company wants to reduce costs. The company has identified the S3 bucket as a large expense. Which solution will reduce the S3 costs with the LEAST operational overhead?
    options:
     - text: Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: Using S3 Lifecycle policies to delete expired object versions and retain only the two most recent versions reduces storage costs and minimizes operational overhead.
      Incorrect: 
        "***replace later***"

  - id: q499
    type: multiple_choice
    question: A company needs to minimize the cost of its 1 Gbps AWS Direct Connect connection. The company's average connection utilization is less than 10%. A solutions architect must recommend a solution that will reduce the cost without compromising security. Which solution will meet these requirements?
    options:
     - text: Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: By working with an AWS Direct Connect Partner to order a smaller hosted connection, the company can reduce costs while maintaining secure connectivity.
      Incorrect: 
        "***replace later***"

  - id: q500
    type: multiple_choice
    question: A company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change. Which solutions will meet these requirements? (Choose two.)
    options:
     - text: Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.
       is_correct: true
     - text: Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS DataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.
       is_correct: true
     - text: "***dont't touch, replace later***"
       is_correct: false
    explanation: |
      Correct: AWS DataSync provides a secure and efficient way to transfer files while preserving file permissions. Using a Snowcone device with DataSync agents is an alternative for environments with limited network bandwidth.
      Incorrect: 
        "***replace later***"