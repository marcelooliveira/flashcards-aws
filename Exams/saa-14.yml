questions:
- id: q651
  type: multiple_choice
  question: |
    A company needs an S3 storage strategy: 
    - 0-180 days: Readily available.
    - 180-360 days: Infrequently accessed.
    - 360+ days: Archived but must be available INSTANTLY.
    - 5+ years: Audits only (12-hour retrieval ok).
    Which solution meets this?
  options:
    - text: S3 Standard -> S3 One Zone-IA -> S3 Glacier Instant Retrieval -> S3 Glacier Deep Archive.
      is_correct: false
    - text: S3 Standard -> S3 Standard-IA -> S3 Glacier Flexible Retrieval -> S3 Glacier Deep Archive.
      is_correct: false
    - text: S3 Standard -> S3 Standard-IA -> S3 Glacier Instant Retrieval -> S3 Glacier Deep Archive.
      is_correct: true
    - text: S3 Standard -> S3 Intelligent-Tiering -> S3 Glacier Deep Archive.
      is_correct: false
  explanation: |
    Correct: S3 Standard-IA provides high durability across multiple AZs for the first transition. S3 Glacier Instant Retrieval is the ONLY archive tier that provides "instant" (millisecond) access. S3 Glacier Deep Archive is the cheapest tier for the final 5-year audit requirement where 12-hour retrieval is acceptable.
    Incorrect: One Zone-IA lacks durability. Glacier Flexible Retrieval takes minutes to hours, failing the "instant" requirement.
  diagram: |
    graph TD
      A[S3 Standard 0-180 days] --> B[S3 Standard-IA 180-360 days]
      B --> C[S3 Glacier Instant Retrieval 360+ days]
      C --> D[S3 Glacier Deep Archive 5+ years, audit]



- id: q652
  type: multiple_choice
  question: |
    A company has a critical data workload running 6 hours daily on Amazon EMR. Data loss is not allowed. 
    Which EMR configuration is most cost-effective?
  options:
    - text: Persistent cluster with all On-Demand Instances.
      is_correct: false
    - text: Transient cluster with primary/core nodes on On-Demand and task nodes on Spot Instances.
      is_correct: true
    - text: Transient cluster with all Spot Instances.
      is_correct: false
    - text: Persistent cluster using Reserved Instances.
      is_correct: false
  explanation: |
    Correct: A "Transient" cluster shuts down after the job, saving money. Keeping Primary and Core nodes on On-Demand ensures the cluster doesn't fail (data integrity), while using Spot Instances for "Task" nodes (which don't store HDFS data) provides significant savings.
  diagram: |
    graph TD
      A[EMR Job Start] --> B[Transient Cluster]
      B --> C[Primary/Core Nodes On-Demand]
      B --> D[Task Nodes Spot Instances]
      B --> E[Cluster Terminates After Job]

- id: q653
  type: multiple_choice
  question: |
    A company needs to automatically tag all resources in a specific account with a "Cost Center ID" based on an RDS database mapping of the user who created them.
    Which solution meets this?
  options:
    - text: Use a Service Control Policy (SCP) to deny resource creation without tags.
      is_correct: false
    - text: Create a Lambda function triggered by EventBridge (via CloudTrail) to tag resources based on the RDS DB.
      is_correct: true
    - text: Use AWS Config rules to flag untagged resources.
      is_correct: false
    - text: Create an IAM policy that forces tagging.
      is_correct: false
  explanation: |
    Correct: This is a classic automation pattern. CloudTrail captures the "Create" event, EventBridge triggers Lambda, and Lambda lookups the Cost Center in RDS and applies the tag to the resource ARN.
  diagram: |
    graph TD
      A[Resource Creation Event] --> B[CloudTrail]
      B --> C[EventBridge Rule]
      C --> D[Lambda Function]
      D --> E[RDS Lookup - Cost Center]
      D --> F[Tag Resource]

- id: q654
  type: multiple_choice
  question: |
    A PHP app uses Apache for static content and local Redis for sessions. 
    How should the company redesign for high availability and managed services?
  options:
    - text: Use Elastic Beanstalk for the entire stack.
      is_correct: false
    - text: Migrate to Lambda and DynamoDB.
      is_correct: false
    - text: Use CloudFront + S3 for static content, ALB + ECS Fargate for PHP, and Multi-AZ ElastiCache for Redis.
      is_correct: true
    - text: Run everything on EC2 with a Load Balancer.
      is_correct: false
  explanation: |
    Correct: This decouples the layers. S3/CloudFront handles static assets cheaply. ECS Fargate provides managed container orchestration for PHP. ElastiCache for Redis provides a managed, highly available session store.
  diagram: |
    graph TD
      A[User] --> B[CloudFront]
      B --> C[S3 Static Content]
      B --> D[ALB]
      D --> E[ECS Fargate PHP App]
      E --> F[ElastiCache Redis - Sessions]



- id: q655
  type: multiple_choice
  question: |
    A web app needs sticky sessions, public access, and WAF security. 
    Which TWO steps meet these requirements? (Choose two.)
  options:
    - text: Create a Network Load Balancer (NLB).
      is_correct: false
    - text: Create a public Application Load Balancer (ALB).
      is_correct: true
    - text: Associate a WAF web ACL with the ALB.
      is_correct: true
    - text: Assign Elastic IP addresses to the EC2 instances.
      is_correct: false
    - text: Enable sticky sessions on the EC2 instances.
      is_correct: false
  explanation: |
    Correct: ALBs natively support HTTP sticky sessions and integrate directly with AWS WAF. NLBs do not support WAF or standard HTTP cookie-based stickiness.

  diagram: |
    graph TD
      A[User] --accesses--> B[Public ALB]
      B --routes to--> C[EC2 Instances with Sticky Sessions]
      B --protected by--> D[AWS WAF Web ACL]

- id: q656
  type: multiple_choice
  question: |
    Historical images are accessed once or twice a year. The company needs a highly available, cost-effective storage/delivery solution.
    Which solution is best?
  options:
    - text: Use Amazon EBS volumes attached to EC2.
      is_correct: false
    - text: Use Amazon EFS.
      is_correct: false
    - text: Store images in S3 Standard.
      is_correct: false
    - text: Store images in S3 Standard-IA and deliver via a static website.
      is_correct: true
  explanation: |
    Correct: S3 Standard-Infrequent Access (IA) has a lower storage price, perfect for images accessed only twice a year. Delivering via "Static Website Hosting" removes the need for expensive compute (EC2).

  diagram: |
    graph TD
      A[Historical Images] --stored in--> B[S3 Standard-IA]
      B --delivered by--> C[Static Website Hosting]

- id: q657
  type: multiple_choice
  question: |
    A company needs to centralize management of security group CIDR ranges (office IPs) across multiple accounts.
    Which solution is most efficient?
  options:
    - text: Use a Service Control Policy.
      is_correct: false
    - text: Create a managed prefix list and share it via AWS Resource Access Manager (RAM).
      is_correct: true
    - text: Manually update security groups in every account.
      is_correct: false
    - text: Use AWS Firewall Manager.
      is_correct: false
  explanation: |
    Correct: Managed Prefix Lists allow you to group multiple CIDR blocks into a single object. By sharing this object via RAM, you can reference it in Security Groups in any account. When the list is updated once, all security groups are updated automatically.

  diagram: |
    graph TD
      A[Office CIDR Ranges] --grouped in--> B[Managed Prefix List]
      B --shared via--> C[AWS RAM]
      C --referenced by--> D[Security Groups in Multiple Accounts]

- id: q658
  type: multiple_choice
  question: |
    An HPC workload requires NFS and SMB multi-protocol access with the LEAST latency. 
    Which TWO steps should be taken? (Choose two.)
  options:
    - text: Use a cluster placement group for EC2 instances.
      is_correct: true
    - text: Use FSx for Lustre.
      is_correct: false
    - text: Use FSx for Windows File Server.
      is_correct: false
    - text: Use Amazon S3 with S3 File Gateway.
      is_correct: false
    - text: Use Amazon FSx for NetApp ONTAP.
      is_correct: true
  explanation: |
    Correct: Cluster placement groups provide the lowest possible network latency between instances. FSx for ONTAP is the only managed service that supports both NFS and SMB protocols simultaneously on the same file system.

  diagram: |
    graph TD
      A[EC2 Instances] --placed in--> B[Cluster Placement Group]
      B --connects to--> C[FSx for NetApp ONTAP]
      C --provides--> D[NFS & SMB Access]

- id: q659
  type: multiple_choice
  question: |
    50 TB of data must be moved to AWS in 2 weeks. The VPN is 90% utilized. 
    Which service should be used?
  options:
    - text: AWS DataSync.
      is_correct: false
    - text: AWS Direct Connect.
      is_correct: false
    - text: AWS Snowball Edge Storage Optimized.
      is_correct: true
    - text: S3 Transfer Acceleration.
      is_correct: false
  explanation: |
    Correct: 50 TB over a nearly saturated VPN would take too long and impact production. Snowball Edge is a physical device that allows offline transfer of large data volumes within the 2-week window.

  diagram: |
    graph TD
      A[On-premises Data] --copied to--> B[AWS Snowball Edge]
      B --shipped to--> C[AWS]
      C --data imported to--> D[S3]

- id: q660
  type: multiple_choice
  question: |
    Application performance is slow exactly at the start of peak hours every day because scaling takes time. 
    Which solution is best?
  options:
    - text: Use Target Tracking scaling.
      is_correct: false
    - text: Use Step Scaling.
      is_correct: false
    - text: Use Predictive Scaling.
      is_correct: false
    - text: Configure Scheduled Scaling to launch instances before peak hours.
      is_correct: true
  explanation: |
    Correct: Since peak hours occur at the same time every day, "Scheduled Scaling" allows you to pre-provision capacity so it's ready before the rush begins, avoiding the "warm-up" delay.

  diagram: |
    graph TD
      A[Scheduled Scaling] --launches--> B[EC2 Instances]
      B --ready for--> C[Peak Hours]

- id: q661
  type: multiple_choice
  question: |
    A company needs to scale database connections effectively during peak times with LEAST operational overhead.
    Which solution is best?
  options:
    - text: Migrate to DynamoDB.
      is_correct: false
    - text: Use Amazon RDS Proxy.
      is_correct: true
    - text: Increase the RDS instance size.
      is_correct: false
    - text: Use an ElastiCache cluster.
      is_correct: false
  explanation: |
    Correct: RDS Proxy handles connection pooling, allowing many application connections to share a small number of database connections. This prevents "Too Many Connections" errors during traffic spikes.

  diagram: |
    graph TD
      A[App Connections] --connect via--> B[RDS Proxy]
      B --pools connections to--> C[RDS Database]



- id: q662
  type: multiple_choice
  question: |
    EBS snapshot costs are increasing every month. The company wants to optimize costs with LEAST overhead. 
    Which solution is best?
  options:
    - text: Manually delete snapshots every month.
      is_correct: false
    - text: Use S3 Lifecycle policies.
      is_correct: false
    - text: Use AWS Backup.
      is_correct: false
    - text: Use Amazon Data Lifecycle Manager (DLM) to manage snapshots.
      is_correct: true
  explanation: |
    Correct: DLM allows you to create policies (e.g., "Keep only the last 7 daily snapshots") to automate the deletion of old EBS snapshots, ensuring costs don't grow infinitely.

  diagram: |
    graph TD
      A[EBS Snapshots] --managed by--> B[Data Lifecycle Manager]
      B --automates--> C[Snapshot Deletion]

- id: q663
  type: multiple_choice
  question: |
    An ECS cluster needs exclusive access to an S3 bucket and an RDS database. 
    Which solution meets this?
  options:
    - text: Use IAM roles for the ECS tasks.
      is_correct: false
    - text: Use S3 and RDS encryption with KMS.
      is_correct: false
    - text: Use VPC endpoints for S3 and restrict RDS security groups to the ECS security group.
      is_correct: true
    - text: Place everything in a public subnet.
      is_correct: false
  explanation: |
    Correct: Networking restrictions ensure that traffic stays private and originates only from the ECS cluster. VPC endpoints for S3 and SG-to-SG rules for RDS provide the required isolation.

  diagram: |
    graph TD
      A[ECS Cluster] --accesses--> B[S3 via VPC Endpoint]
      A --connects to--> C[RDS (restricted by SG)]

- id: q664
  type: multiple_choice
  question: |
    An app has huge CPU spikes (10x) twice a month. The company uses Elastic Beanstalk and wants automatic scaling. 
    Which solution is best?
  options:
    - text: Use burstable instances (T3) in unlimited mode with request-based scaling.
      is_correct: true
    - text: Use Compute Optimized (C5) instances.
      is_correct: false
    - text: Use Scheduled Scaling.
      is_correct: false
    - text: Use a Single Instance environment.
      is_correct: false
  explanation: |
    Correct: Burstable instances (T-series) are cheaper for normal usage. "Unlimited mode" allows them to burst to handle the 10x spikes twice a month without performance degradation.

  diagram: |
    graph TD
      A[Elastic Beanstalk] --uses--> B[T3 Burstable Instances]
      B --scales with--> C[Request-based Scaling]

- id: q665
  type: multiple_choice
  question: |
    A global company needs to track and audit all incremental changes to infrastructure using automation.
    Which solution is best?
  options:
    - text: Use AWS CloudTrail only.
      is_correct: false
    - text: Use AWS CloudFormation for deployment and AWS Config for change tracking.
      is_correct: true
    - text: Use AWS Service Catalog.
      is_correct: false
    - text: Use AWS Trusted Advisor.
      is_correct: false
  explanation: |
    Correct: CloudFormation ensures "Infrastructure as Code" (automation), while AWS Config provides a detailed history of configuration changes for auditing purposes.

  diagram: |
    graph TD
      A[CloudFormation] --deploys--> B[AWS Resources]
      B --tracked by--> C[AWS Config]
      C --audits--> D[Configuration Changes]

- id: q666
  type: multiple_choice
  question: |
    A stateless Python app and MySQL DB need high availability. The code cannot be modified. 
    Which TWO actions are required? (Choose two.)
  options:
    - text: Migrate the database to DynamoDB.
      is_correct: false
    - text: Migrate the database to Amazon RDS with Multi-AZ.
      is_correct: true
    - text: Use AWS DataSync for the application files.
      is_correct: false
    - text: Deploy EC2 instances in an Auto Scaling group behind an Application Load Balancer.
      is_correct: true
  explanation: |
    Correct: RDS Multi-AZ provides database HA without code changes. ALB + ASG ensures the application layer is highly available across multiple AZs.

  diagram: |
    graph TD
      A[App EC2 Instances] --in--> B[Auto Scaling Group]
      B --behind--> C[Application Load Balancer]
      D[MySQL DB] --migrated to--> E[RDS Multi-AZ]

- id: q667
  type: multiple_choice
  question: |
    A company needs to access S3 from on-premises (via Direct Connect) and VPC without traversing the internet. 
    Which solution is best?
  options:
    - text: Create an S3 Gateway Endpoint.
      is_correct: true
    - text: Use a NAT Gateway.
      is_correct: false
    - text: Create an S3 Interface Endpoint.
      is_correct: false
    - text: Use a Public VIF on Direct Connect.
      is_correct: false
  explanation: |
    Correct: S3 Gateway Endpoints are free and allow private communication. While Interface Endpoints also work for on-premises, Gateway Endpoints are the standard solution for routing internal S3 traffic within the AWS backbone.

  diagram: |
    graph TD
      A[On-premises & VPC] --connects via--> B[S3 Gateway Endpoint]
      B --routes to--> C[S3 Bucket]

- id: q668
  type: multiple_choice
  question: |
    Developers must use a predefined "Application Name" tag to create resources. 
    Which solution enforces this value?
  options:
    - text: IAM policies with condition keys.
      is_correct: false
    - text: AWS Config.
      is_correct: false
    - text: Service Control Policies (SCPs).
      is_correct: false
    - text: Tag Policies in AWS Organizations.
      is_correct: true
  explanation: |
    Correct: Tag Policies allow you to define allowed values for specific tags across an entire organization, ensuring standardization.

  diagram: |
    graph TD
      A[AWS Organizations] --enforces--> B[Tag Policy]
      B --requires--> C[Application Name Tag]

- id: q669
  type: multiple_choice
  question: |
    A company needs to rotate an RDS PostgreSQL master password every 30 days with LEAST effort. 
    Which solution is best?
  options:
    - text: Use IAM Database Authentication.
      is_correct: false
    - text: Manually change the password in the RDS console.
      is_correct: false
    - text: Use AWS Secrets Manager with built-in rotation for RDS.
      is_correct: true
    - text: Use AWS Systems Manager Parameter Store.
      is_correct: false
  explanation: |
    Correct: Secrets Manager has native integration with RDS to automate password rotation using a Lambda function that AWS provides and manages for you.

  diagram: |
    graph TD
      A[Secrets Manager] --rotates--> B[RDS PostgreSQL Password]
      B --uses--> C[AWS Lambda Automation]

- id: q670
  type: multiple_choice
  question: |
    A DynamoDB table is used for 4 hours once a week with a known number of operations. 
    How can costs be optimized?
  options:
    - text: Use On-Demand mode.
      is_correct: false
    - text: Use Provisioned mode with calculated RCU/WCU.
      is_correct: true
    - text: Use DynamoDB Accelerator (DAX).
      is_correct: false
    - text: Use Reserved Capacity with On-Demand mode.
      is_correct: false
  explanation: |
    Correct: Since the workload is predictable and infrequent, Provisioned mode is cheaper than On-Demand (which charges more per request). You can even scale the provisioned capacity down to 1 when the tests aren't running.

  diagram: |
    graph TD
      A[DynamoDB Table] --configured with--> B[Provisioned RCU/WCU]
      B --scaled for--> C[Known Workload]

- id: q671
  type: multiple_choice
  question: |
    A company needs to prevent unusual spending by monitoring costs and notifying stakeholders in the event of anomalies. 
    Which solution meets this?
  options:
    - text: Create a Cost Explorer report and schedule an email.
      is_correct: false
    - text: Create a Cost Anomaly Detection monitor.
      is_correct: true
    - text: Use AWS Budgets to alert when costs exceed a threshold.
      is_correct: false
    - text: Set up a CloudWatch alarm on the EstimatedCharges metric.
      is_correct: false
  explanation: |
    Correct: AWS Cost Anomaly Detection uses machine learning to identify spend that is outside of normal patterns, even if it hasn't hit a specific budget limit. It is the most precise tool for "unusual spending" versus "exceeding a limit."
    Incorrect: Budgets are for fixed thresholds, not pattern-based anomalies.

  diagram: |
    graph TD
      A[AWS Costs] --monitored by--> B[Cost Anomaly Detection]
      B --notifies--> C[Stakeholders]

- id: q672
  type: multiple_choice
  question: |
    A company needs to analyze large amounts of clickstream data in S3 quickly to determine if it should be processed further.
    Which solution has the LEAST operational overhead?
  options:
    - text: Use Amazon EMR to run a Spark job.
      is_correct: false
    - text: Use an AWS Glue crawler to catalog the data and Amazon Athena for ad-hoc SQL queries.
      is_correct: true
    - text: Load the data into Amazon Redshift.
      is_correct: false
    - text: Use AWS Lambda to parse the files and store results in DynamoDB.
      is_correct: false
  explanation: |
    Correct: This is a totally serverless "Data Lake" approach. Glue crawls the S3 bucket to infer the schema, and Athena allows you to run SQL immediately against the raw S3 data without moving it.

  diagram: |
    graph TD
      A[S3 Clickstream Data] --cataloged by--> B[AWS Glue Crawler]
      B --queried by--> C[Amazon Athena]

- id: q673
  type: multiple_choice
  question: |
    A company has an SMB file server. Files are frequent-access for 7 days, then must be archived with a maximum 24-hour retrieval time.
    Which solution meets this?
  options:
    - text: Use AWS DataSync to move files to S3 Glacier.
      is_correct: false
    - text: Use AWS Storage Gateway (S3 File Gateway) with an S3 Lifecycle policy to move objects to S3 Glacier Deep Archive after 7 days.
      is_correct: true
    - text: Use Amazon FSx for Windows File Server.
      is_correct: false
    - text: Use AWS Snowball Edge.
      is_correct: false
  explanation: |
    Correct: S3 File Gateway provides a local SMB mount point. The 7-day lifecycle policy manages the move to Glacier Deep Archive (which has a 12-48 hour retrieval window), satisfying the 24-hour requirement cost-effectively.

  diagram: |
    graph TD
      A[SMB File Server] --mounted by--> B[S3 File Gateway]
      B --stores files in--> C[S3 Bucket]
      C --lifecycle moves to--> D[Glacier Deep Archive]



- id: q674
  type: multiple_choice
  question: |
    An RDS PostgreSQL database experiences a heavy read load during high traffic periods.
    Which TWO actions resolve this? (Choose two.)
  options:
    - text: Configure RDS Auto Scaling.
      is_correct: false
    - text: Create an RDS Read Replica.
      is_correct: true
    - text: Enable Multi-AZ deployment.
      is_correct: false
    - text: Use Amazon ElastiCache to cache frequent queries.
      is_correct: true
    - text: Move the database to a larger EC2 instance.
      is_correct: false
  explanation: |
    Correct: Read Replicas offload the read-only query volume from the primary instance. ElastiCache (Redis/Memcached) stores frequent query results in-memory, preventing the queries from hitting the database at all.

  diagram: |
    graph TD
      A[RDS PostgreSQL] --replicates to--> B[RDS Read Replica]
      A --frequent queries cached by--> C[ElastiCache]

- id: q675
  type: multiple_choice
  question: |
    A company wants to prevent accidental deletion of EBS snapshots without changing administrative rights of the storage admin.
    Which solution has the LEAST effort?
  options:
    - text: Use an SCP to deny ec2:DeleteSnapshot.
      is_correct: false
    - text: Use AWS Backup with a Vault Lock.
      is_correct: false
    - text: Configure the Recycle Bin for snapshots.
      is_correct: false
    - text: Use the Snapshot Lock feature.
      is_correct: true
  explanation: |
    Correct: EBS Snapshot Lock allows you to lock snapshots to prevent deletion by anyone (including admins) for a specified period. This is a direct feature of EBS.
    Incorrect: Recycle Bin allows recovery after deletion but doesn't "prevent" the deletion action itself in the same way.

  diagram: |
    graph TD
      A[EBS Snapshots] --protected by--> B[Snapshot Lock]
      B --prevents--> C[Accidental Deletion]

- id: q676
  type: multiple_choice
  question: |
    A company needs to capture VPC traffic info in near real time and send it to Amazon OpenSearch for analysis.
    Which solution meets this?
  options:
    - text: Enable CloudTrail and use a Lambda to parse logs.
      is_correct: false
    - text: Enable VPC Flow Logs, send them to CloudWatch Logs, and use a Kinesis Data Firehose to stream them to OpenSearch.
      is_correct: true
    - text: Use VPC Traffic Mirroring to a dedicated EC2 instance.
      is_correct: false
    - text: Export VPC Flow Logs to S3 and run a Glue job.
      is_correct: false
  explanation: |
    Correct: VPC Flow Logs capture IP traffic. Sending them through CloudWatch and Firehose provides a managed, near real-time pipeline into OpenSearch (formerly Elasticsearch).

  diagram: |
    graph TD
      A[VPC Flow Logs] --sent to--> B[CloudWatch Logs]
      B --streamed by--> C[Kinesis Data Firehose]
      C --delivers to--> D[Amazon OpenSearch]



- id: q677
  type: multiple_choice
  question: |
    A company needs an infrequent EKS development cluster to test resiliency cost-effectively.
    Which solution is best?
  options:
    - text: Use Fargate for all pods.
      is_correct: false
    - text: Use a managed node group with a mix of On-Demand and Spot Instances.
      is_correct: true
    - text: Use self-managed node groups with Reserved Instances.
      is_correct: false
    - text: Use a managed node group with only On-Demand Instances.
      is_correct: false
  explanation: |
    Correct: Using Spot Instances in EKS managed node groups can save up to 90% in costs. Mixing them with On-Demand ensures the cluster remains functional for tests even if some Spot instances are reclaimed.

  diagram: |
    graph TD
      A[EKS Cluster] --uses--> B[Managed Node Group]
      B --mixes--> C[On-Demand Instances]
      B --mixes--> D[Spot Instances]

- id: q678
  type: multiple_choice
  question: |
    A company needs full control to create, rotate, and disable encryption keys for S3 data with minimal effort.
    Which solution meets this?
  options:
    - text: Use SSE-S3 encryption.
      is_correct: false
    - text: Use customer-managed AWS KMS keys (SSE-KMS).
      is_correct: true
    - text: Use AWS-managed KMS keys.
      is_correct: false
    - text: Use client-side encryption with a local key manager.
      is_correct: false
  explanation: |
    Correct: SSE-KMS with "Customer Managed Keys" (CMK) gives the user full administrative control over the lifecycle (rotation, disabling, policies) of the keys.
    Incorrect: SSE-S3 is managed by AWS; you cannot rotate or disable those keys yourself.

  diagram: |
    graph TD
      A[S3 Data] --encrypted by--> B[SSE-KMS]
      B --controlled by--> C[Customer Managed Keys]

- id: q679
  type: multiple_choice
  question: |
    VM backups are stored in S3. They must be retained for 30 days and automatically deleted after.
    Which THREE steps are needed? (Choose three.)
  options:
    - text: Enable S3 Object Lock.
      is_correct: false
    - text: Enable S3 Versioning on the bucket.
      is_correct: true
    - text: Configure S3 Object Lock in Governance mode with a 30-day retention.
      is_correct: true
    - text: Use an S3 Lifecycle policy to transition to Glacier.
      is_correct: false
    - text: Use an S3 Lifecycle policy to expire current versions after 30 days.
      is_correct: true
    - text: Tag objects with a "DeleteAt" tag.
      is_correct: false
  explanation: |
    Correct: Versioning allows Object Lock to work. Object Lock (Governance) ensures the data can't be deleted for 30 days. The Lifecycle policy (Expiration) ensures that once the 30 days are up, the objects are automatically removed.

  diagram: |
    graph TD
      A[VM Backups] --stored in--> B[S3 Bucket]
      B --versioning enables--> C[Object Lock Governance]
      C --retains for--> D[30 Days]
      D --expired by--> E[S3 Lifecycle Policy]

- id: q680
  type: multiple_choice
  question: |
    Files must be copied continuously from one S3 bucket to EFS and another S3 bucket, only overwriting if source files change.
    Which solution has the LEAST overhead?
  options:
    - text: Use AWS DataSync with a scheduled task in "changed data only" mode.
      is_correct: true
    - text: Use an S3 Event Notification to trigger a Lambda function.
      is_correct: false
    - text: Use the AWS CLI `s3 sync` command on a cron job.
      is_correct: false
    - text: Use S3 Replication.
      is_correct: false
  explanation: |
    Correct: DataSync is designed for large-scale data movement and natively supports "changed data only" syncing between S3 and EFS.
    Incorrect: S3 Replication doesn't support EFS as a target.

  diagram: |
    graph TD
      A[S3 Bucket] --synced by--> B[AWS DataSync]
      B --copies to--> C[EFS]
      B --copies to--> D[Other S3 Bucket]

- id: q681
  type: multiple_choice
  question: |
    All EBS data must be encrypted with KMS, and the company must control the key rotation.
    Which solution has the LEAST overhead?
  options:
    - text: Use customer-managed KMS keys for EBS encryption.
      is_correct: true
    - text: Use AWS-managed KMS keys (aws/ebs).
      is_correct: false
    - text: Encrypt data at the software level inside the OS.
      is_correct: false
    - text: Use SSE-C encryption.
      is_correct: false
  explanation: |
    Correct: To "control" rotation (e.g., triggering it manually or setting a specific schedule), you must use Customer Managed Keys. AWS-managed keys rotate on a fixed 3-year schedule that you cannot change.

  diagram: |
    graph TD
      A[EBS Data] --encrypted by--> B[KMS]
      B --uses--> C[Customer Managed Keys]
      C --rotation controlled by--> D[User]

- id: q682
  type: multiple_choice
  question: |
    A solution must automatically identify non-encrypted EBS volumes and enforce compliance policies.
    Which solution has the LEAST administrative overhead?
  options:
    - text: Use AWS Config rules to detect non-compliance and Systems Manager Automation for remediation.
      is_correct: true
    - text: Write a custom Python script to scan the account.
      is_correct: false
    - text: Use Amazon Macie.
      is_correct: false
    - text: Use AWS Trusted Advisor.
      is_correct: false
  explanation: |
    Correct: AWS Config is the standard tool for auditing resource configurations. Coupled with Systems Manager (SSM) Automation, it can automatically remediate (e.g., stop the instance or alert) non-compliant resources.

  diagram: |
    graph TD
      A[EBS Volumes] --audited by--> B[AWS Config Rules]
      B --remediated by--> C[SSM Automation]



- id: q683
  type: multiple_choice
  question: |
    A multi-tier app (single MySQL node, multi-node web tier) needs to migrate to AWS with minimal changes but improved resiliency.
    Which TWO steps meet this? (Choose two.)
  options:
    - text: Migrate the web tier to an Auto Scaling group behind an ALB.
      is_correct: true
    - text: Migrate the web tier to AWS Lambda.
      is_correct: false
    - text: Migrate the MySQL database to an RDS Multi-AZ instance.
      is_correct: true
    - text: Use Amazon DynamoDB for the data tier.
      is_correct: false
    - text: Use an EC2 instance with a larger EBS volume for the database.
      is_correct: false
  explanation: |
    Correct: Moving to ALB + ASG ensures the web tier is redundant and can scale. Moving MySQL to RDS Multi-AZ adds automated failover and backups with zero code changes, which satisfies the "minimal changes" requirement.

  diagram: |
    graph TD
      A[Web Tier] --migrated to--> B[Auto Scaling Group]
      B --behind--> C[Application Load Balancer]
      D[MySQL DB] --migrated to--> E[RDS Multi-AZ]

- id: q684
  type: multiple_choice
  question: |
    A company near eu-central-1 cannot use that region for some apps due to regulations, but needs single-digit millisecond latency.
    Which solution meets this?
  options:
    - text: Use Amazon CloudFront.
      is_correct: false
    - text: Deploy the applications in AWS Local Zones.
      is_correct: true
    - text: Use AWS Outposts.
      is_correct: false
    - text: Use a Site-to-Site VPN.
      is_correct: false
  explanation: |
    Correct: Local Zones place compute, storage, and database services close to large population/industry centers, providing the ultra-low latency required while staying within specific geographic boundaries.

  diagram: |
    graph TD
      A[Applications] --deployed in--> B[AWS Local Zones]
      B --provides--> C[Single-digit ms Latency]
