questions:
  - id: q101
    type: multiple_choice
    question: An application stops working with the following error; 'The specified bucket does not exist'. Where is the BEST place to start the root cause analysis?
    options:
     - text: Check the Elastic Load Balancer logs for 'DeleteBucket' requests.
       is_correct: false
     - text: Check the application logs in Amazon CloudWatch Logs for Amazon S3 'DeleteBucket' errors.
       is_correct: false
     - text: Check AWS X-Ray for Amazon S3 'DeleteBucket' alarms.
       is_correct: false
     - text: Check AWS CloudTrail for a 'DeleteBucket' event.
       is_correct: true
    explanation: |
      Start by checking AWS CloudTrail for a 'DeleteBucket' event because CloudTrail records API calls that change AWS resources and will show who or what deleted the bucket.

      The other options do not reliably record bucket deletion events: ELB and X-Ray do not track S3 management API calls, and CloudWatch application logs may not contain the delete event unless the app explicitly logged it.
    tags: 
    difficulty: 
    points: 

  - id: q102
    type: multiple_choice
    question: A Developer will be using the AWS CLI on a local development server to manage AWS services. What can be done to ensure that the CLI uses the Developer's IAM permissions when making commands?
    options:
     - text: Specify the Developer's IAM access key ID and secret access key as parameters for each CLI command.
       is_correct: false
     - text: Run the 'aws configure' CLI command, and provide the Developer's IAM access key ID and secret access key.
       is_correct: true
     - text: Specify the Developer's IAM user name and password as parameters for each CLI command.
       is_correct: false
     - text: Use the Developer's IAM role when making the CLI command.
       is_correct: false
    explanation: |
      The recommended approach is to run 'aws configure' to store the developer's access key and secret in the CLI configuration so commands run with that IAM identity.

      Passing credentials on every command, using usernames/passwords, or claiming to use a role directly are incorrect: they are insecure, unsupported for CLI authentication, or require additional configuration like assuming a role.
    tags: 
    difficulty: 
    points: 

  - id: q103
    type: multiple_choice
    question: An application stores images in an S3 bucket. Amazon S3 event notifications are used to trigger a Lambda function that resizes the images. Processing each image takes less than a second. How will AWS Lambda handle the additional traffic?
    options:
     - text: Lambda will scale out to execute the requests concurrently.
       is_correct: true
     - text: Lambda will handle the requests sequentially in the order received.
       is_correct: false
     - text: Lambda will process multiple images in a single execution.
       is_correct: false
     - text: Lambda will add more compute to each execution to reduce processing time.
       is_correct: false
    explanation: |
      Lambda automatically scales out by running multiple concurrent executions to handle increased event traffic, allowing many image-resize tasks to run in parallel.

      Lambda does not queue or serialize events by default, nor does it batch unrelated events into a single execution or arbitrarily increase per-invocation compute without configuration; scaling is via concurrency.
    tags: 
    difficulty: 
    points: 

  - id: q104
    type: multiple_choice
    question: A company is building a stock trading application that requires sub-millisecond latency in processing trading requests. Amazon DynamoDB is used to store all the trading data that is used to process each request. After load testing the application, the development team found that due to data retrieval times, the latency requirement is not satisfied. Because of sudden high spikes in the number of requests, DynamoDB read capacity has to be significantly over-provisioned to avoid throttling. What steps should be taken to meet latency requirements and reduce the cost of running the application?
    options:
     - text: Add Global Secondary Indexes for trading data.
       is_correct: false
     - text: Store trading data in Amazon S3 and use Transfer Acceleration.
       is_correct: false
     - text: Add retries with exponential back-off for DynamoDB queries.
       is_correct: false
     - text: Use DynamoDB Accelerator to cache trading data.
       is_correct: true
    explanation: |
      Using DynamoDB Accelerator (DAX) provides an in-memory cache that reduces read latency to microseconds and lowers the need to over-provision read capacity for spiky workloads.

      The other options either do not address microsecond latency (S3, retries) or are indexing changes that do not provide the in-memory caching benefit DAX provides for high-speed reads.
    tags: 
    difficulty: 
    points: 

  - id: q105
    type: multiple_choice
    question: A Developer created a Lambda function for a web application backend. When testing the Lambda function from the AWS Lambda console, the Developer can see that the function is being executed, but there is no log data being generated in Amazon CloudWatch Logs, even after several minutes. What could cause this situation?
    options:
     - text: The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs.
       is_correct: false
     - text: The Lambda function is missing CloudWatch Logs as a source trigger to send log data.
       is_correct: false
     - text: The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs.
       is_correct: true
     - text: The Lambda function is missing a target CloudWatch Log group.
       is_correct: false
    explanation: |
      If the Lambda execution role lacks permissions to create log groups or put log events, then no logs will be written to CloudWatch even though the function runs, so ensuring the role has CloudWatch Logs permissions fixes this.

      The incorrect options misunderstand how Lambda logging works: explicit log statements are not required for basic runtime logs, there is no trigger for sending logs, and Lambda creates log groups automatically when permitted.
    tags: 
    difficulty: 
    points: 

  - id: q106
    type: multiple_choice
    question: A Developer wants to use AWS X-Ray to trace a user request end-to-end throughput the software stack. The Developer made the necessary changes in the application tested it, and found that the application is able to send the traces to AWS X-Ray. However, when the application is deployed to an EC2 instance, the traces are not availableWhich of the following could create this situation? (Choose TWO)
    options:
     - text: The traces are reaching X-Ray, but the Developer does not have access to view the records.
       is_correct: false
     - text: The X-Ray daemon is not installed on the EC2 instance.
       is_correct: true
     - text: The X-Ray endpoint specified in the application configuration is incorrect.
       is_correct: false
     - text: The instance role does not have 'xray:BatchGetTraces' and 'xray:GetTraceGraph' permissions.The instance role does not have 'xray:PutTraceSegments' and 'xray:PutTelemetryRecords' permissions.
       is_correct: false
     - text: The instance role does not have 'xray:PutTraceSegments' and 'xray:PutTelemetryRecords' permissions.
       is_correct: true
    explanation: |
      Two common causes are a missing X-Ray daemon on the EC2 host and an instance role lacking 'xray:PutTraceSegments' and 'xray:PutTelemetryRecords' permissions, both preventing trace data from being sent to or accepted by X-Ray.

      The other options are less likely: missing viewer permissions would not prevent traces from appearing in the console for authorized users, and an incorrect endpoint is uncommon if the same configuration worked elsewhere.
    tags: 
    difficulty: 
    points: 

  - id: q107
    type: multiple_choice
    question: An application has hundreds of users. Each user may use multiple devices to access the application. The Developer wants to assign unique identifiers to these users regardless of the device they use. Which of the following methods should be used to obtain unique identifiers?
    options:
     - text: Create a user table in Amazon DynamoDB as key-value pairs of users and their devices. Use these keys as unique identifiers.
       is_correct: false
     - text: Use IAM-generated access key IDs for the users as the unique identifier, but do not store secret keys.
       is_correct: false
     - text: Implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities.
       is_correct: true
     - text: Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier.
       is_correct: false
    explanation: |
      Developer-authenticated identities in Amazon Cognito let you assign stable, unique identifiers to users across devices while integrating with your own authentication system and issuing credentials as needed.

      The incorrect options misuse IAM for application user IDs or rely on ad-hoc DynamoDB keys; IAM keys are for AWS access, not application identity, and inventing custom ID storage is more error-prone than Cognito.
    tags: 
    difficulty: 
    points: 

  - id: q108
    type: multiple_choice
    question: What are the steps to using the AWS CLI to launch a templatized serverless application?
    options:
     - text: Use AWS CloudFormation get-template then CloudFormation execute-change-set.
       is_correct: false
     - text: Use AWS CloudFormation validate-template then CloudFormation create-change-set.
       is_correct: false
     - text: Use AWS CloudFormation package then CloudFormation deploy.
       is_correct: true
     - text: Use AWS CloudFormation create-stack then CloudFormation update-stack.
       is_correct: false
    explanation: |
      The standard CLI workflow is to run 'aws cloudformation package' to upload local artifacts and produce a modified template, then 'aws cloudformation deploy' to create or update the stack.

      The other options either validate or create change sets without packaging local artifacts, or confuse stack creation/update sequencing; packaging is required for templated serverless artifacts.
    tags: 
    difficulty: 
    points: 

  - id: q109
    type: multiple_choice
    question: A deployment package uses the AWS CLI to copy files into any S3 bucket in the account, using access keys stored in environment variables. The package is running on EC2 instances, and the instances have been modified to run with an assumed IAM role and a more restrictive policy that allows access to only one bucket. After the change, the Developer logs into the host and still has the ability to write into all of the S3 buckets in that account. What is the MOST likely cause of this situation?
    options:
     - text: An IAM inline policy is being used on the IAM role.
       is_correct: false
     - text: An IAM managed policy is being used on the IAM role.
       is_correct: true
     - text: The AWS CLI is corrupt and needs to be reinstalled.
       is_correct: false
     - text: The AWS credential provider looks for instance profile credentials last.
       is_correct: false
    explanation: |
      The likely cause is that long-lived access keys stored in environment variables are still being used by the CLI; using a managed policy on the role is not the issue—existing credentials override instance role credentials until rotated or removed.

      The incorrect answers misidentify uncommon causes: a corrupt CLI or credential provider order are unlikely, and using an inline vs managed policy does not explain the persisted use of old credentials.
    tags: 
    difficulty: 
    points: 

  - id: q110
    type: multiple_choice
    question: An application overwrites an object in Amazon S3, and then immediately reads the same object. Why would the application sometimes retrieve the old version of the object?
    options:
     - text: S3 overwrite PUTS are eventually consistent, so the application may read the old object.
       is_correct: true
     - text: The application needs to add extra metadata to label the latest version when uploading to Amazon S3.
       is_correct: false
     - text: All S3 PUTS are eventually consistent, so the application may read the old object.
       is_correct: false
     - text: The application needs to explicitly specify latest version when retrieving the object.
       is_correct: false
    explanation: |
      S3 provides eventual consistency for overwrite PUTs and deletes, so an immediate read after overwrite can sometimes return the previous version until replication completes.

      The other options are incorrect because metadata or version specification does not change S3's consistency model, and not all PUTs are eventually consistent (new object PUTs are read-after-write consistent for new keys).
    tags: 
    difficulty: 
    points: 

  - id: q111
    type: multiple_choice
    question: An application under development is required to store hundreds of video files. The data must be encrypted within the application prior to storage, with a unique key for each video file. How should the Developer code the application?
    options:
     - text: Use the 'KMS Encrypt' API to encrypt the data. Store the encrypted data key and data.
       is_correct: false
     - text: Use a cryptography library to generate an encryption key for the application. Use the encryption key to encrypt the data. Store the encrypted data.
       is_correct: false
     - text: Use the 'KMS GenerateDataKey' API to get a data key. Encrypt the data with the data key. Store the encrypted data key and data.
       is_correct: true
     - text: Upload the data to an S3 bucket using server side-encryption with an AWS KMS key.
       is_correct: false
    explanation: |
      Best practice is to call 'GenerateDataKey' to obtain a unique data key for each file, use that key to encrypt the file locally, and store the encrypted data key alongside the ciphertext for later decryption via KMS.

      The incorrect options either misuse KMS encrypt directly on large data, rely on insecure custom key management, or use server-side encryption which does not satisfy the requirement to encrypt within the application prior to storage.
    tags: 
    difficulty: 
    points: 

  - id: q112
    type: multiple_choice
    question: A developer is testing an application that invokes an AWS Lambda function asynchronously. During the testing phase, the Lambda function fails to process after two retries. How can the developer troubleshoot the failure?
    options:
     - text: Configure AWS CloudTrail logging to investigate the invocation failures.
       is_correct: false
     - text: Configure Dead Letter Queues by sending events to Amazon SQS for investigatio.
       is_correct: true
     - text: Configure Amazon Simple Workflow Service to process any direct unprocessed events.
       is_correct: false
     - text: Configure AWS Config to process any direct unprocessed events.
       is_correct: false
    explanation: |
      Configuring a Dead Letter Queue (DLQ) for asynchronous Lambda invocations captures failed events in SQS for later inspection and debugging after retries are exhausted.

      The other options do not directly capture failed asynchronous events for troubleshooting: CloudTrail tracks API calls, SWF and Config are not appropriate mechanisms for capturing failed payloads.
    tags: 
    difficulty: 
    points: 

  - id: q113
    type: multiple_choice
    question: A developer is setting up Amazon API Gateway for their company's products. The API will be used by registered developers to query and update their environments. The company wants to limit the amount of requests end users can send for both cost and security reasons. Management wants to offer registered developers the option of buying larger packages that allow for more requests. How can the developer accomplish this with the LEAST amount of overhead management?
    options:
     - text: Enable throttling for the API Gateway stage. Set a value for both the rate and burst capacity. If a registered user chooses a larger package, create a stage for them, adjust the values, and share the new URL with them.
       is_correct: false
     - text: Set up Amazon CloudWatch API logging in API Gateway. Create a filter based on the user and requestTime fields and create an alarm on this filter. Write an AWS Lambda function to analyze the values and requester information, and respond accordingly. Set up the function as the target for the alarm. If a registered user chooses a larger package, update the Lambda code with the values.
       is_correct: false
     - text: Enable Amazon CloudWatch metrics for the API Gateway stage. Set up CloudWatch alarms based off the Count metric and the ApiName, Method, Resource, and Stage dimensions to alerts when request rates pass the threshold. Set the alarm action to 'Deny'. If a registered user chooses a larger package, create a user-specific alarm and adjust the values.
       is_correct: false
     - text: Set up a default usage plan, specify values for the rate and burst capacity, and associate it with a stage. If a registered user chooses a larger package, create a custom plan with the appropriate values and associate the plan with the user.
       is_correct: true
    explanation: |
      Usage plans in API Gateway let you define throttling and quota limits and associate API keys with developers, making it straightforward to offer different request packages with minimal management.

      The other options require heavy manual work or misuse CloudWatch alarms or separate stages per customer, which is far more complex and error-prone than using usage plans and API keys.
    tags: 
    difficulty: 
    points: 

  - id: q114
    type: multiple_choice
    question: A developer is refactoring a monolithic application. The application takes a POST request and performs several operations. Some of the operations are in parallel while others run sequentially. These operations have been refactored into individual AWS Lambda functions. The POST request will be processed by Amazon API Gateway. How should the developer invoke the Lambda functions in the same sequence using API Gateway?
    options:
     - text: Use Amazon SQS to invoke the Lambda functions.
       is_correct: false
     - text: Use an AWS Step Functions activity to run the Lambda functions.
       is_correct: false
     - text: Use Amazon SNS to trigger the Lambda functions.
       is_correct: false
     - text: Use an AWS Step Functions state machine to orchestrate the Lambda functions.
       is_correct: true
    explanation: |
      AWS Step Functions state machines provide durable orchestration for sequencing and parallel execution of Lambda functions and are the appropriate tool to implement the described flow.

      The incorrect options either use messaging services that do not provide coordinated sequencing and branching (SQS, SNS) or confuse activities with the state machine orchestration needed for this pattern.
    tags: 
    difficulty: 
    points: 

  - id: q115
    type: multiple_choice
    question: A company is adding stored value (or gift card) capability to its highly popular casual gaming website. Users need to be able to trade this value for other users' items on the platform. This would require both users' records be updated as a single transaction, or both users' records to be completely rolled back. Which AWS database options can provide the transactional capability required for this new feature? (Choose TWO)
    options:
     - text: Amazon DynamoDB with operations made with the 'ConsistentRead' parameter set to 'true'
       is_correct: false
     - text: Amazon ElastiCache for Memcached with operations made within a transaction block.
       is_correct: false
     - text: Amazon Aurora MySQL with operations made within a transaction block.
       is_correct: true
     - text: Amazon DynamoDB with reads and writes made using 'Transact*' operations.
       is_correct: true
     - text: Amazon Redshift with operations made within a transaction block.
       is_correct: false
    explanation: |
      Amazon Aurora (MySQL) supports traditional ACID transactions, and DynamoDB supports transactional operations via the Transact* APIs, so both can implement all-or-nothing multi-item updates.

      The incorrect options either misunderstand DynamoDB's consistency flag (ConsistentRead is not transactional), use caching layers that lack durable transactions, or use data warehouses like Redshift which are not intended for transactional update patterns.
    tags: 
    difficulty: 
    points: 

  - id: q116
    type: multiple_choice
    question: A developer is creating an AWS Lambda function that generates a new file each time it runs. Each new file must be checked into an AWS CodeCommit repository hosted in the same AWS account. How should the developer accomplish this?
    options:
     - text: When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change.
       is_correct: true
     - text: After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository.
       is_correct: false
     - text: Use an AWS SDK to instantiate a CodeCommit client. Invoke the 'put_file' method to add the file to the repository.
       is_correct: false
     - text: Upload the new to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository.
       is_correct: false
    explanation: |
      Cloning the repository with the Git CLI inside the Lambda execution, committing the new file, and pushing the change replicates the normal Git workflow and is a straightforward way to add files to CodeCommit.

      The incorrect options either misuse the low-level APIs, suggest impractical Step Functions orchestration for each file, or reference a non-existent simple 'put_file' SDK call for typical Git operations.
    tags: 
    difficulty: 
    points: 

  - id: q117
    type: multiple_choice
    question: A developer must ensure that the IAM credentials used by an application in Amazon EC2 are not misused or compromised. What should the developer use to keep user credentials secure?
    options:
     - text: Environment variables.
       is_correct: false
     - text: AWS credentials file.
       is_correct: false
     - text: Instance profile credentials.
       is_correct: true
     - text: Command line options.
       is_correct: false
    explanation: |
      Instance profile credentials (IAM roles attached to EC2) provide temporary, automatically rotated credentials and are the recommended secure method for EC2-based applications.

      The incorrect options involve long-lived credentials stored in environment variables, files, or passed on command line which are insecure and harder to manage or rotate.
    tags: 
    difficulty: 
    points: 

  - id: q118
    type: multiple_choice
    question: A company has an application where reading objects from Amazon S3 is based on the type of user. The user types are registered user and guest user. The company has 25,000 users and is growing. Information is pulled from an S3 bucket depending on the user type. Which approaches are recommended to provide access to both user types? (Choose TWO)
    options:
     - text: Provide a different access key and secret access key in the application code for registered users and guest users to provide read access to the objects.
       is_correct: false
     - text: Use S3 bucket policies to restrict read access to specific IAM users.
       is_correct: false
     - text: Use Amazon Cognito to provide access using authenticated and unauthenticated roles.
       is_correct: true
     - text: Create a new IAM user for each user and grant read access.
       is_correct: false
     - text: Use the AWS IAM service and let the application assume the different roles using the AWS Security Token Service (AWS STS) 'AssumeRole' action depending on the type of user and provide read access to Amazon S3 using the assumed role.
       is_correct: true
    explanation: |
      Use Amazon Cognito to issue temporary credentials for authenticated and unauthenticated users, and use role assumption via STS for fine-grained access control; both approaches scale and avoid embedding static credentials.

      The incorrect options propose creating or embedding long-lived credentials, or assigning per-user IAM accounts, which do not scale and are insecure compared to Cognito and STS-based solutions.
    tags: 
    difficulty: 
    points: 

  - id: q119
    type: multiple_choice
    question: A company has 25,000 employees and is growing. The company is creating an application that will be accessible to its employees only. A developer is using Amazon S3 to store images and Amazon RDS to store application data. The company requires that all employee information remain in the legacy Security Assertion Markup Language (SAML) employee directory only and is not interested in mirroring any employee information on AWS. How can the developer provide authorized access for the employees who will be using this application so each employee can access their own application data only?
    options:
     - text: Use Amazon VPC and keep all resources inside the VPC, and use a VPC link for the S3 bucket with the bucket policy.
       is_correct: false
     - text: Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy.
       is_correct: false
     - text: Use an Amazon Cognito identity pool, federate with the SAML provider, and use an IAM condition key with a value for the 'cognito-identity.amazonaws.com:sub' variable to grant access to the employees.
       is_correct: true
     - text: Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only.
       is_correct: false
    explanation: |
      Federating the SAML directory through a Cognito identity pool allows users to authenticate via SAML without mirroring identities on AWS, and using the 'cognito-identity.amazonaws.com:sub' condition lets you grant per-user access to resources.

      The incorrect options either try to keep everything in the VPC (which doesn't solve identity federation), use user pools which require storing user profiles, or propose creating thousands of IAM roles which is unmanageable.
    tags: 
    difficulty: 
    points: 

  - id: q120
    type: multiple_choice
    question: A company has developed a new serverless application using AWS Lambda functions that will be deployed using the AWS Serverless Application Model (AWS SAM) CLI. Which step should the developer complete prior to deploying the application?
    options:
     - text: Compress the application to a '.zip' file and upload it into AWS Lambda.
       is_correct: false
     - text: Test the new AWS Lambda function by first tracing it in AWS X-Ray.
       is_correct: false
     - text: Bundle the serverless application using a SAM package.
       is_correct: true
     - text: Create the application environment using the 'eb create my-env' command.
       is_correct: false
    explanation: |
      Before deploying with SAM CLI, you must package the application (sam package or sam build/deploy) so local artifacts are prepared and uploaded to S3 for deployment.

      The incorrect options either reference manual zipping, irrelevant tracing steps, or Elastic Beanstalk commands which are not part of the SAM deployment workflow.
    tags: 
    difficulty: 
    points: 

  - id: q121
    type: multiple_choice
    question: An application needs to encrypt data that is written to Amazon S3 where the keys are managed in an on-premises data center, and the encryption is handled by S3. Which type of encryption should be used?
    options:
     - text: Use server-side encryption with Amazon S3-managed keys.
       is_correct: false
     - text: Use server-side encryption with AWS KMS-managed keys.
       is_correct: false
     - text: Use client-side encryption with customer master keys.
       is_correct: false
     - text: Use server-side encryption with customer-provided keys.
       is_correct: true
    explanation: |
      Server-side encryption with customer-provided keys (SSE-C) lets you provide your own encryption keys from on-premises while S3 handles the encryption and storage, matching the requirement.

      The incorrect options either use AWS-managed keys (S3 or KMS) or client-side encryption which does not meet the requirement of S3 handling encryption with on-premises-managed keys.
    tags: 
    difficulty: 
    points: 

  - id: q122
    type: multiple_choice
    question: A development team is working on a mobile app that allows users to upload pictures to Amazon S3. The team expects the app will be used by hundreds of thousands of users during a single event simultaneously. Once the pictures are uploaded, the backend service will scan and parse the pictures for inappropriate content. Which approach is the MOST resilient way to achieve this goal, which also smooths out temporary volume spikes for the backend service?
    options:
     - text: Develop an AWS Lambda function to check the upload folder in the S3 bucket. If new uploaded pictures are detected, the Lambda function will scan and parse them.
       is_correct: false
     - text: Once a picture is uploaded to Amazon S3, publish the event to an Amazon SQS queue. Use the queue as an event source to trigger an AWS Lambda function. In the Lambda function, scan and parse the picture.
       is_correct: true
     - text: When the user uploads a picture, invoke an API hosted in Amazon API Gateway. The API will invoke an AWS Lambda function to scan and parse the picture.
       is_correct: false
     - text: Create a state machine in AWS Step Functions to check the upload folder in the S3 bucket. If a new picture is detected, invoke an AWS Lambda function to scan and parse it.
       is_correct: false
    explanation: |
      Publishing S3 events to SQS decouples uploads from processing and smooths spikes, letting the queue buffer work and Lambda scale consumers at a rate the backend can handle.

      The other options either poll S3 (inefficient), invoke processing synchronously at upload time (which can fail under peak load), or use Step Functions to poll which is not ideal for high-volume buffering.
    tags: 
    difficulty: 
    points: 

  - id: q123
    type: multiple_choice
    question: A development team wants to run their container workloads on Amazon ECS. Each application container needs to share data with another container to collect logs and metrics. What should the developer team do to meet these requirements?
    options:
     - text: Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together.
       is_correct: false
     - text: Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks.
       is_correct: false
     - text: Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers.
       is_correct: true
     - text: Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers.
       is_correct: false
    explanation: |
      In ECS, including both containers in the same task definition and mounting a shared volume allows containers in the same task to share logs and metrics efficiently.

      The incorrect options describe pod concepts (Kubernetes) or attempt to share volumes across separate tasks which is not the correct ECS pattern for intra-task sharing.
    tags: 
    difficulty: 
    points: 

  - id: q124
    type: multiple_choice
    question: An ecommerce startup is preparing for an annual sales event. As the traffic to the company's application increases, the development team wants to be notified when the Amazon EC2 instance's CPU utilization exceeds 80%. Which solution will meet this requirement?
    options:
     - text: Create a custom Amazon CloudWatch alarm that sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%.
       is_correct: true
     - text: Create a custom AWS Cloud Trail alarm that sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%.
       is_correct: false
     - text: Create a cron job on the EC2 instance that executes the '–describe-instance-information' command on the host instance every 15 minutes and sends the results to an Amazon SNS topic.
       is_correct: false
     - text: Create an AWS Lambda function that queries the AWS CloudTrail logs for the CPUUtilization metric every 15 minutes and sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%.
       is_correct: false
    explanation: |
      A CloudWatch alarm on the EC2 instance metric is the native and reliable way to monitor CPU utilization and send SNS notifications when thresholds are exceeded.

      The incorrect options misuse CloudTrail (which logs API activity, not metrics), or propose ad-hoc polling solutions that are less reliable and more complex than CloudWatch alarms.
    tags: 
    difficulty: 
    points: 

  - id: q125
    type: multiple_choice
    question: An application running on Amazon EC2 opens connections to an Amazon RDS SQL Server database. The developer does not want to store the user name and password for the database in the code. The developer would also like to automatically rotate the credentials. What is the MOST secure way to store and access the database credentials?
    options:
     - text: Create an IAM role that has permissions to access the database. Attach the role to the EC2 instance.
       is_correct: false
     - text: Use AWS Secrets Manager to store the credentials. Retrieve the credentials from Secrets Manager as needed.
       is_correct: true
     - text: Store the credentials in an encrypted text file in an Amazon S3 bucket. Configure the EC2 instance's user data to download the credentials from Amazon S3 as the instance boots.
       is_correct: false
     - text: Store the user name and password credentials directly in the source code. No further action is needed because the source code is stored in a private repository.
       is_correct: false
    explanation: |
      AWS Secrets Manager stores and automatically rotates database credentials securely and integrates with RDS and EC2, making it the best choice for secret management and rotation.

      The incorrect options either rely on insecure storage, manual rotation, or misuse IAM roles (which do not replace secret storage and rotation) and are not recommended for credential management.
    tags: 
    difficulty: 
    points: 

  - id: q126
    type: multiple_choice
    question: A developer is updating an application deployed on AWS Elastic Beanstalk. The new version is incompatible with the old version. To successfully deploy the update, a full cutover to the new, updated version must be performed on all instances at one time, with the ability to roll back changes in case of a deployment failure in the new version. How can this be performed with the LEAST amount of downtime?
    options:
     - text: Use the Elastic Beanstalk All at once deployment policy to update all instances simultaneously.
       is_correct: false
     - text: Perform an Elastic Beanstalk Rolling with additional batch deployment.
       is_correct: false
     - text: Deploy the new version in a new Elastic Beanstalk environment and swap environment URLs.
       is_correct: true
     - text: Perform an Elastic Beanstalk Rolling deployment.
       is_correct: false
    explanation: |
      Creating a new environment with the updated version and performing a URL swap achieves a near-zero-downtime cutover and allows easy rollback by swapping back to the previous environment.

      The other deployment policies either update existing instances in place (risking incompatibility and downtime) or cannot guarantee a single simultaneous cutover with safe rollback.
    tags: 
    difficulty: 
    points: 

  - id: q127
    type: multiple_choice
    question: A developer is writing a web application that must share secure documents with end users. The documents are stored in a private Amazon S3 bucket. The application must allow only authenticated users to download specific documents when requested, and only for a duration of 15 minutes. How can the developer meet these requirements?
    options:
     - text: Copy the documents to a separate S3 bucket that has a lifecycle policy for deletion after 15 minutes.
       is_correct: false
     - text: Create a presigned S3 URL using the AWS SDK with an expiration time of 15 minutes.
       is_correct: true
     - text: Use server-side encryption with AWS KMS managed keys (SSE-KMS) and download the documents using HTTPS.
       is_correct: false
     - text: Modify the S3 bucket policy to only allow specific users to download the documents. Revert the change after 15 minutes.
       is_correct: false
    explanation: |
      Generating a presigned S3 URL with a 15-minute expiration allows authenticated users to securely download a specific object for a limited time without changing bucket policies.

      The incorrect options either rely on deletion-based lifecycles (inefficient), use encryption which does not provide timed access control, or demand constant policy changes which are impractical.
    tags: 
    difficulty: 
    points: 

  - id: q128
    type: multiple_choice
    question: A company is developing a report executed by AWS Step Functions, Amazon CloudWatch shows errors in the Step Functions task state machine. To troubleshoot each task, the state input needs to be included along with the error message in the state output. Which coding practice can preserve both the original input and the error for the state?
    options:
     - text: Use 'ResultPath' in a 'Catch' statement to include the error with the original input.
       is_correct: true
     - text: Use 'InputPath' in a 'Catch' statement and set the value to 'null'
       is_correct: false
     - text: Use 'Error Equals' in a 'Retry' statement to include the error with the original input.
       is_correct: false
     - text: Use 'OutputPath' in a 'Retry' statement and set the value to '$'
       is_correct: false
    explanation: |
      Using a 'Catch' with a 'ResultPath' allows you to attach the error details to the state output while preserving the original input, enabling better debugging and error inspection.

      The incorrect options misuse InputPath/Retry/OutputPath semantics and would either drop the original input or not capture the error in the desired way.
    tags: 
    difficulty: 
    points: 

  - id: q129
    type: multiple_choice
    question: A developer receives the following error message when trying to launch or terminate an Amazon EC2 instance using a boto3 script. What should the developer do to correct this error message?
    img: images/question129.jpg
    options:
     - text: Assign an IAM role to the EC2 instance to allow necessary API calls on behalf of the client.
       is_correct: false
     - text: Implement an exponential backoff algorithm for optimizing the number of API requests made to Amazon EC2.
       is_correct: true
     - text: Increase the overall network bandwidth to handle higher API request rates.
       is_correct: false
     - text: Upgrade to the latest AWS CLI version so that boto3 can handle higher request rates.
       is_correct: false
    explanation: |
      The error indicates throttling from excessive API calls; implementing exponential backoff reduces request frequency during spikes and is the recommended way to handle throttling.

      The other options do not address client-side throttling: assigning roles, increasing bandwidth, or upgrading the CLI will not resolve API rate-limit errors.
    tags: 
    difficulty: 
    points: 

  - id: q130
    type: multiple_choice
    question: Given the following AWS CloudFormation template. What is the MOST efficient way to reference the new Amazon S3 bucket from another AWS CloudFormation template?
    img: images/question130.jpg
    options:
     - text: Add an 'Export' declaration to the 'Outputs' section of the original template and use 'ImportValue' in other templates.
       is_correct: true
     - text: Add 'Exported true' to the 'Content.Bucket' in the original template and use 'ImportResource' in other templates.
       is_correct: false
     - text: Create a custom AWS CloudFormation resource that gets the bucket name from the 'ContentBucket' resource of the first stack.
       is_correct: false
     - text: Use 'Fn::Include' to include the existing template in other templates and use the 'ContentBucket' resource directly.
       is_correct: false
    explanation: |
      Exporting the bucket name in the Outputs and using ImportValue in other stacks is the supported and efficient CloudFormation mechanism for sharing resources across stacks.

      The incorrect options describe unsupported or more complex approaches: there is no 'Exported' attribute on resources, custom resources add overhead, and 'Fn::Include' is not a cross-stack reference mechanism.
    tags: 
    difficulty: 
    points: 

  - id: q131
    type: multiple_choice
    question: A developer is using AWS CodeDeploy to deploy an application running on Amazon EC2. The developer wants to change the file permissions for a specific deployment file. Which lifecycle event should a developer use to meet this requirement?
    options:
     - text: AfterInstall.
       is_correct: true
     - text: DownloadBundle.
       is_correct: false
     - text: BeforeInstall.
       is_correct: false
     - text: ValidateService.
       is_correct: false
    explanation: |
      The 'AfterInstall' hook runs on instances after the application files are in place, making it the right time to change file permissions before the application starts.

      The other lifecycle events occur either too early (DownloadBundle/BeforeInstall) or later for validation (ValidateService), so they are not the ideal time to modify file permissions.
    tags: 
    difficulty: 
    points: 

  - id: q132
    type: multiple_choice
    question: A developer is using Amazon DynamoDB to store application data. The developer wants to further improve application performance by reducing response times for read and write operations. Which DynamoDB feature should be used to meet these requirements?
    options:
     - text: Amazon DynamoDB Streams.
       is_correct: false
     - text: Amazon DynamoDB Accelerator.
       is_correct: true
     - text: Amazon DynamoDB global tables.
       is_correct: false
     - text: Amazon DynamoDB transactions.
       is_correct: false
    explanation: |
      DynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB that can significantly reduce read latency and improve overall application performance.

      The other features address streaming, cross-region replication, or transactional semantics but do not directly provide the in-memory low-latency caching that DAX offers.
    tags: 
    difficulty: 
    points: 

  - id: q133
    type: multiple_choice
    question: A developer is creating a script to automate the deployment process for a serverless application. The developer wants to use an existing AWS Serverless Application Model (AWS SAM) template for the application. What should the developer use for the project? (Choose TWO)
    options:
     - text: Call 'aws cloudformation package' to create the deployment package. Call 'aws cloudformation deploy' to deploy the package afterward.
       is_correct: true
     - text: Call 'sam package' to create the deployment package. Call 'sam deploy' to deploy the package afterward.
       is_correct: true
     - text: Call 'aws s3 cp' to upload the AWS SAM template to Amazon S3. Call 'aws lambda update-function-code' to create the application.
       is_correct: false
     - text: Create a 'ZIP' package locally and call 'aws serverlessrepo create-application' to create the application.
       is_correct: false
     - text: Create a 'ZIP' package and upload it to Amazon S3. Call 'aws cloudformation create-stack' to create the application.
       is_correct: false
    explanation: |
      Both the AWS CLI CloudFormation package/deploy commands and the SAM-specific package/deploy commands are valid ways to package and deploy SAM templates; using either approach automates artifact upload and stack deployment.

      The incorrect options attempt manual or unsupported workflows (direct S3 copy plus update-function or Serverless Application Repository misuse) instead of the standard package-and-deploy flow.
    tags: 
    difficulty: 
    points: 

  - id: q134
    type: multiple_choice
    question: A development team is designing a mobile app that requires multi-factor authentication. Which steps should be taken to achieve this? (Choose TWO)
    options:
     - text: Use Amazon Cognito to create a user pool and create users in the user pool.
       is_correct: true
     - text: Send multi-factor authentication text codes to users with the Amazon SNS Publish API call in the app code.
       is_correct: false
     - text: Enable multi-factor authentication for the Amazon Cognito user pool.
       is_correct: true
     - text: Use AWS IAM to create IAM users.
       is_correct: false
     - text: Enable multi-factor authentication for the users created in AWS IAM.
       is_correct: false
    explanation: |
      Creating a Cognito user pool and enabling MFA in that user pool provides managed user authentication with multi-factor capabilities suitable for mobile apps.

      The incorrect options either misuse IAM (which is for AWS accounts, not app users) or suggest building custom MFA via SNS which is less integrated and more error-prone than Cognito's built-in MFA features.
    tags: 
    difficulty: 
    points: 

  - id: q135
    type: multiple_choice
    question: Two containerized microservices are hosted on Amazon EC2 ECS. The first microservice reads an Amazon RDS Aurora database instance, and the second microservice reads an Amazon DynamoDB table. How can each microservice be granted the minimum privileges?
    options:
     - text: Set 'ECS_ENABLE_TASK_IAM_ROLE' to 'false' on EC2 instance boot in ECS agent configuration file. Run the first microservice with an IAM role for ECS tasks with read-only access for the Aurora database. Run the second microservice with an IAM role for ECS tasks with read-only access to DynamoDB.
       is_correct: false
     - text: Set 'ECS_ENABLE_TASK_IAM_ROLE' to 'false' on EC2 instance boot in the ECS agent configuration file. Grant the instance profile role read-only access to the Aurora database and DynamoDB.
       is_correct: false
     - text: Set 'ECS_ENABLE_TASK_IAM_ROLE' to 'true' on EC2 instance boot in the ECS agent configuration file. Run the first microservice with an IAM role for ECS tasks with read-only access for the Aurora database. Run the secondmicroservice with an IAM role for ECS tasks with read-only access to DynamoDB.
       is_correct: true
     - text: Set 'ECS_ENABLE_TASK_IAM_ROLE' to 'true' on EC2 instance boot in the ECS agent configuration file. Grant the instance profile role read-only access to the Aurora database and DynamoDB.
       is_correct: false
    explanation: |
      Enabling task IAM roles and assigning per-task roles provides least-privilege access so each microservice gets only the permissions it needs regardless of the EC2 instance role.

      The incorrect options either disable task roles or grant broad instance-level permissions, which break least-privilege principles and can expose more permissions than required.
    tags: 
    difficulty: 
    points: 

  - id: q136
    type: multiple_choice
    question: A developer has written an AWS Lambda function using Java as the runtime environment. The developer wants to isolate a performance bottleneck in the code. Which steps should be taken to reveal the bottleneck?
    options:
     - text: Use the Amazon CloudWatch API to write timestamps to a custom CloudWatch metric. Use the CloudWatch console to analyze the resulting data.
       is_correct: false
     - text: Use the AWS X-Ray API to write trace data into X-Ray from strategic places within the code. Use the Amazon CloudWatch console to analyze the resulting data.
       is_correct: false
     - text: Use the AWS X-Ray API to write trace data into X-Ray from strategic places within the code. Use the X-Ray console to analyze the resulting data.
       is_correct: true
     - text: Use the Amazon CloudWatch API to write timestamps to a custom CloudWatch metric. Use the AWS X-Ray console to analyze the resulting data.
       is_correct: false
    explanation: |
      Instrumenting the code to emit traces to AWS X-Ray and analyzing them in the X-Ray console helps locate latency hotspots and visualize where time is spent inside the Lambda function.

      The incorrect options mix services in ways that are less effective for tracing (custom CloudWatch metrics are coarser, and analyzing X-Ray traces in CloudWatch is not the intended workflow).
    tags: 
    difficulty: 
    points: 

  - id: q137
    type: multiple_choice
    question: A developer added a new feature to an application running on an Amazon EC2 instance that uses Amazon SQS. After deployment, the developer noticed a significant increase in Amazon SQS costs. When monitoring the Amazon SQS metrics on Amazon CloudWatch, the developer found that on average one message per minute is posted on this queue. What can be done to reduce Amazon SQS costs for this application?
    options:
     - text: Increase the Amazon SQS queue polling timeout.
       is_correct: true
     - text: Scale down the Amazon SQS queue to the appropriate size for low traffic demand.
       is_correct: false
     - text: Configure push delivery via Amazon SNS instead of polling the Amazon SQS queue.
       is_correct: false
     - text: Use an Amazon SQS first-in, first-out (FIFO) queue instead of a standard queue.
       is_correct: false
    explanation: |
      Increasing the long polling timeout reduces empty ReceiveMessage API calls and lowers request charges when message frequency is low, making SQS usage more cost efficient.

      The incorrect options either describe unsupported scaling of queue size, switching to SNS (which changes semantics), or using FIFO which does not reduce the polling costs.
    tags: 
    difficulty: 
    points: 

  - id: q138
    type: multiple_choice
    question: A developer is building an application using an Amazon API Gateway REST API backend by an AWS Lambda function that interacts with an Amazon DynamoDB table. During testing, the developer observes high latency when making requests to the API. How can the developer evaluate the end-to-end latency and identify performance bottlenecks?
    options:
     - text: Enable AWS CloudTrail logging and use the logs to map each latency and bottleneck.
       is_correct: false
     - text: Enable and configure AWS X-Ray tracing on API Gateway and the Lambda function. Use X-Ray to trace and analyze user requests.
       is_correct: true
     - text: Enable Amazon CloudWatch Logs for the Lambda function. Enable execution logs for API Gateway to view and analyze user request logs.
       is_correct: false
     - text: Enable VPC Flow Logs to capture and analyze network traffic within the VPC.
       is_correct: false
    explanation: |
      Enabling X-Ray on API Gateway and Lambda provides distributed tracing that shows end-to-end request timing and exposes bottlenecks across services.

      The incorrect options either use logging or network flow capture which are less effective for tracing request latency across services, or CloudTrail which records API activity but not detailed timing for end-to-end latency.
    tags: 
    difficulty: 
    points: 

  - id: q139
    type: multiple_choice
    question: An IAM role is attached to an Amazon EC2 instance that explicitly denies access to all Amazon S3 API actions. The EC2 instance credentials file specifies the IAM access key and secret access key, which allow full administrative access. Given that multiple modes of IAM access are present for this EC2 instance, which of the following is correct?
    options:
     - text: The EC2 instance will only be able to list the S3 buckets.
       is_correct: false
     - text: The EC2 instance will only be able to list the contents of one S3 bucket at a time.
       is_correct: false
     - text: The EC2 instance will be able to perform all actions on any S3 bucket.
       is_correct: false
     - text: The EC2 instance will not be able to perform any S3 action on any S3 bucket.
       is_correct: true
    explanation: |
      When multiple credential sources exist, the AWS SDK uses a credential chain and the explicit IAM role deny will take precedence, so the instance will be denied S3 actions despite the stored keys.

      The incorrect answers suggest partial or full access which contradicts the precedence of explicit deny policies that override other permissions.
    tags: 
    difficulty: 
    points: 

  - id: q140
    type: multiple_choice
    question: A development team uses AWS Elastic Beanstalk for application deployment. The team has configured the application version lifecycle policy to limit the number of application versions to 25. However, even with the lifecycle policy, the source bundle is deleted from the Amazon S3 source bucket. What should a developer do in the Elastic Beanstalk application version lifecycle settings to retain the source code in the S3 bucket?
    options:
     - text: Change the Set the application versions limit by total count setting to zero.
       is_correct: false
     - text: Disable the Lifecycle policy setting.
       is_correct: false
     - text: Change the Set the application version limit by age setting to zero.
       is_correct: false
     - text: Set Retention to Retain source bundle in S3.
       is_correct: true
    explanation: |
      Setting Retention to 'Retain source bundle in S3' tells Elastic Beanstalk to keep the uploaded source bundles in the S3 bucket even when the lifecycle policy removes application version records.

      The incorrect options adjust counts or ages or disable lifecycle policy but do not explicitly instruct Elastic Beanstalk to retain the actual source bundle in S3.
    tags: 
    difficulty: 
    points: 

  - id: q141
    type: multiple_choice
    question: A developer has built a market application that stores pricing data in Amazon DynamoDB with Amazon ElastiCache in front. The prices of items in the market change frequently. Sellers have begun complaining that, after they update the price of an item, the price does not actually change in the product listing. What could be causing this issue?
    options:
     - text: The cache is not being invalidated when the price of the item is changed.
       is_correct: true
     - text: The price of the item is being retrieved using a write-through ElastiCache cluster.
       is_correct: false
     - text: The DynamoDB table was provisioned with insufficient read capacity.
       is_correct: false
     - text: The DynamoDB table was provisioned with insufficient write capacity.
       is_correct: false
    explanation: |
      If the cache is not invalidated or updated when the underlying price changes, the application will continue to serve stale data from ElastiCache, causing the observed inconsistency.

      The other options do not directly explain stale cache behavior: write-through would typically update cache on writes, and insufficient capacity would cause throttling errors rather than silent stale reads.
    tags: 
    difficulty: 
    points: 

  - id: q142
    type: multiple_choice
    question: A developer is provided with an HTTPS clone URL for an AWS CodeCommit repository. What needs to be configured before cloning this repository?
    options:
     - text: Use AWS KMS to set up public and private keys for use with AWS CodeCommit.
       is_correct: false
     - text: Set up the Git credential helper to use an AWS credential profile, and enable the helper to send the path to the repositories.
       is_correct: true
     - text: Use AWS Certificate Manager to provision public and private SSL/TLS certificates.
       is_correct: false
     - text: Generate encryption keys using AWS CloudHSM, then export the key for use with AWS CodeCommit.
       is_correct: false
    explanation: |
      For HTTPS access to CodeCommit, you should configure the Git credential helper (AWS CLI credential helper) to use an AWS credential profile so Git can authenticate using IAM credentials.

      The incorrect options reference KMS/ACM/CloudHSM which are unrelated to the typical HTTPS Git authentication workflow for CodeCommit.
    tags: 
    difficulty: 
    points: 

  - id: q143
    type: multiple_choice
    question: What is required to trace Lambda-based applications with AWS X-Ray?
    options:
     - text: Send logs from the Lambda application to an S3 bucket; trigger a Lambda function from the bucket to send data to AWS X-Ray.
       is_correct: false
     - text: Trigger a Lambda function from the application logs in Amazon CloudWatch to submit tracing data to AWS X-Ray.
       is_correct: false
     - text: Use an IAM execution role to give the Lambda function permissions and enable tracing.
       is_correct: true
     - text: Update and add AWS X-Ray daemon code to relevant parts of the Lambda function to set up the trace.
       is_correct: false
    explanation: |
      To enable X-Ray tracing for Lambda, grant the function an execution role with X-Ray permissions and enable tracing in the function configuration so the platform can send traces.

      The incorrect options propose indirect or manual approaches; for Lambda, you do not run a daemon or forward logs via S3 to X-Ray—the platform handles trace emission when tracing is enabled and the role permits it.
    tags: 
    difficulty: 
    points: 

  - id: q144
    type: multiple_choice
    question: A development team is building a new application that will run on Amazon EC2 and use Amazon DynamoDB as a storage layer. The developers all have assigned IAM user accounts in the same IAM group. The developers currently can launch EC2 instances, and they need to be able to launch EC2 instances with an instance role allowing access to Amazon DynamoDB. Which AWS IAM changes are needed when creating an instance role to provide this functionality?
    options:
     - text: Create an IAM permission policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows DynamoDB to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:GetRole and iam:PassRole permissions for the role.
       is_correct: false
     - text: Create an IAM permissions policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows Amazon EC2 to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:PassRole permission for the role.
       is_correct: true
     - text: Create an IAM permission policy attached to the role that allows access to Amazon EC2. Add a trust policy to the role that allows DynamoDB to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:PassRole permission for the role.
       is_correct: false
     - text: Create an IAM permissions policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows Amazon EC2 to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:GetRole permission for the role.
       is_correct: false
    explanation: |
      To let developers launch instances with an instance role, create a role trusted by EC2 with DynamoDB permissions and grant developers the iam:PassRole permission so they can assign that role to instances during launch.

      The incorrect options either set the wrong trust relationship, propose incorrect or insufficient permissions, or confuse GetRole with PassRole which is required to attach roles to instances.
    tags: 
    difficulty: 
    points: 

  - id: q145
    type: multiple_choice
    question: A developer converted an existing program to an AWS Lambda function in the console. The program runs properly on a local laptop, but shows an 'Unable to import module' error when tested in the Lambda console. Which of the following can fix the error?
    options:
     - text: Install the missing module and specify the current directory as the target. Create a 'ZIP' file to include all files under the current directory, and upload the 'ZIP' file.
       is_correct: true
     - text: Install the missing module in a lib directory. Create a 'ZIP' file to include all files under the lib directory, and upload the 'ZIP' file as dependency file.
       is_correct: false
     - text: In the Lambda code, invoke a Linux command to install the missing modules under the '/usr/lib directory'
       is_correct: false
     - text: In the Lambda console, create a 'LB_LIBRARY_PATH' environment and specify the value for the system library plan.
       is_correct: false
    explanation: |
      Packaging the function with its dependencies in a zip and uploading it ensures Lambda has the required modules available in the execution environment, resolving import errors.

      The incorrect options suggest impractical runtime installs, incorrect packaging locations, or non-existent console settings that will not fix the missing module in Lambda's runtime.
    tags: 
    difficulty: 
    points: 

  - id: q146
    type: multiple_choice
    question: A front-end web application is using Amazon Cognito user pools to handle the user authentication flow. A developer is integrating Amazon DynamoDB into the application using the AWS SDK for JavaScript. How would the developer securely call the API without exposing the access or secret keys?
    options:
     - text: Configure Amazon Cognito identity pools and exchange the JSON Web Token (JWT) for temporary credentials.
       is_correct: true
     - text: Run the web application in an Amazon EC2 instance with the instance profile configured.
       is_correct: false
     - text: Hardcore the credentials, use Amazon S3 to host the web application, and enable server-side encryption.
       is_correct: false
     - text: Use Amazon Cognito user pool JSON Web Tokens (JWITs) to access the DynamoDB APIs.
       is_correct: false
    explanation: |
      Use a Cognito identity pool to exchange user pool JWTs for temporary AWS credentials, enabling browser-based access to DynamoDB without embedding long-lived keys in the client.

      The incorrect options either rely on insecure hardcoding, run the client in EC2 (not applicable for browser apps), or misunderstand that user pool JWTs are not direct credentials for AWS service APIs.
    tags: 
    difficulty: 
    points: 

  - id: q147
    type: multiple_choice
    question: A developer needs to manage AWS infrastructure as code and must be able to deploy multiple identical copies of the infrastructure, stage changes, and revert to previous versions. Which approach addresses these requirements?
    options:
     - text: Use cost allocation reports and AWS OpsWorks to deploy and manage the infrastructure.
       is_correct: false
     - text: Use Amazon CloudWatch metrics and alerts along with resource tagging to deploy and manage the infrastructure.
       is_correct: false
     - text: Use AWS Elastic Beanstalk and AWS CodeCommit to deploy and manage the infrastructure.
       is_correct: false
     - text: Use AWS CloudFormation and AWS CodeCommit to deploy and manage the infrastructure.
       is_correct: true
    explanation: |
      AWS CloudFormation expresses infrastructure as templates that can be versioned and deployed repeatedly, and pairing with CodeCommit enables code-based staging and rollbacks.

      The incorrect options either use monitoring/tagging tools or PaaS services that do not provide the same declarative, versioned infrastructure-as-code capabilities as CloudFormation.
    tags: 
    difficulty: 
    points: 

  - id: q148
    type: multiple_choice
    question: A Developer needs to deploy an application running on AWS Fargate using Amazon ECS. The application has environment variables that must be passed to a container for the application to initialize. How should the environment variables be passed to the container?
    options:
     - text: Define an array that includes the environment variables under the environment parameter within the service definition.
       is_correct: false
     - text: Define an array that includes the environment variables under the environment parameter within the task definition.
       is_correct: true
     - text: Define an array that includes the environment variables under the entryPoint parameter within the task definition.
       is_correct: false
     - text: Define an array that includes the environment variables under the entryPoint parameter within the service definition.
       is_correct: false
    explanation: |
      Environment variables for containers are specified in the task definition's 'environment' parameter so the container receives them at start time when the task runs.

      The incorrect options place variables in the service or entryPoint fields, which are not the correct places to define runtime environment variables for containers.
    tags: 
    difficulty: 
    points: 

  - id: q149
    type: multiple_choice
    question: A company's fleet of Amazon EC2 instances receives data from millions of users through an API. The servers batch the data, add an object for each user, and upload the objects to an S3 bucket to ensure high access rates. The object attributes are 'Customer ID', 'Server ID', 'TS-Server' ('TimeStamp' and 'Server ID'), the size of the object, and a timestamp. A Developer wants to find all the objects for a given user collected during a specified time range. After creating an S3 object created event, how can the Developer achieve this requirement?
    options:
     - text: Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon DynamoDB record for every object with the 'Customer ID' as the partition key and the 'Server ID' as the sort key. Retrieve all the records using the 'Customer ID' and 'Server ID' attributes.
       is_correct: false
     - text: Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon Redshift record for every object with the 'Customer ID' as the partition key and 'TS-Server' as the sort key. Retrieve all the records using the 'Customer ID' and 'TS-Server' attributes.
       is_correct: false
     - text: Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon DynamoDB record for every object with the 'Customer ID' as the partition key and 'TS-Server' as the sort key. Retrieve all the records using the 'Customer ID' and 'TS-Server' attributes.
       is_correct: true
     - text: Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon Redshift record for every object with the 'Customer ID' as the partition key and the 'Server ID' as the sort key. Retrieve all the records using the 'Customer ID' and 'Server ID' attributes.
       is_correct: false
    explanation: |
      Storing metadata in DynamoDB keyed by 'Customer ID' with 'TS-Server' as the sort key enables efficient range queries for a user's objects within a time range.

      The incorrect options either choose the wrong sort key (Server ID) which does not support time-range queries, or suggest Redshift which is less suitable for high-rate metadata lookups compared to DynamoDB.
    tags: 
    difficulty: 
    points: 

  - id: q150
    type: multiple_choice
    question: A company is managing a NoSQL database on-premises to host a critical component of an application, which is starting to have scaling issues. The company wants to migrate the application to Amazon DynamoDB with the following considerations; Optimize frequent queries. Reduce read latencies. Plan for frequent queries on certain key attributes of the table. Which solution would help achieve these objectives?
    options:
     - text: Create global secondary indexes on keys that are frequently queried. Add the necessary attributes into the indexes.
       is_correct: true
     - text: Create local secondary indexes on keys that are frequently queried. DynamoDB will fetch needed attributes from the table.
       is_correct: false
     - text: Create DynamoDB global tables to speed up query responses. Use a scan to fetch data from the table.
       is_correct: false
     - text: Create an AWS Auto Scaling policy for the DynamoDB table.
       is_correct: false
    explanation: |
      Global Secondary Indexes provide alternative query keys and can be tailored to include the needed attributes, enabling efficient queries and lower read latency for frequently accessed attributes.

      The incorrect options either misuse local secondary indexes (which require the same partition key), suggest scans (inefficient), or rely only on autoscaling which doesn't optimize query patterns.
    tags: 
    difficulty: 
    points: 
