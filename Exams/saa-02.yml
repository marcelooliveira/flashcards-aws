questions:
  - id: q51
    type: multiple_choice
    question: |
      A company is developing an application that provides order shipping statistics for retrieval by a REST API. The company wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and send the report to several email addresses at the same time every morning.
      Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)
    options:
     - text: Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.
       is_correct: true
     - text: Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.
       is_correct: true
     - text: Create an Amazon EC2 instance to run a cron job that queries the API and uses Amazon SES to send the report.
       is_correct: false
     - text: Use Amazon CloudWatch alarms to trigger a Lambda function for querying the API and Amazon SNS for emailing the report.
       is_correct: false
    explanation: |
      Correct: Use EventBridge to schedule a Lambda function to query the API and Amazon SES to format and send the report by email, automating the daily report delivery with minimal operational overhead.
      Incorrect: 
        Creating an EC2 instance requires provisioning and managing infrastructure, increasing operational overhead compared to serverless options.
        Amazon SNS is designed for notifications and does not support formatting data into HTML reports or sending to multiple email addresses directly.
    tags: 
    difficulty: 
    points: 

  - id: q52
    type: multiple_choice
    question: |
      A company wants to migrate its on-premises application to AWS. The application produces output files that vary in size from tens of gigabytes to hundreds of terabytes. The application data must be stored in a standard file system structure. The company wants a solution that scales automatically, is highly available, and requires minimum operational overhead.
      Which solution will meet these requirements?
    options:
     - text: Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.
       is_correct: true
     - text: Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon S3 for storage.
       is_correct: false
     - text: Migrate the application to Amazon ECS on AWS Fargate. Use Amazon EBS for storage.
       is_correct: false
    explanation: |
      Correct: Amazon EFS provides a scalable, highly available, and fully managed file system that integrates with EC2, supporting large and variable file sizes with minimal operational overhead.
      Incorrect: 
        Amazon S3 is object storage, not a file system, and does not support standard file system structures or direct file access.
        Amazon EBS volumes are attached to individual EC2 instances and do not provide shared, scalable file storage across multiple instances.
    tags: 
    difficulty: 
    points: 

  - id: q53
    type: multiple_choice
    question: |
      A company needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then must be archived for an additional 9 years. No one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period. The records must be stored with maximum resiliency.
      Which solution will meet these requirements?
    options:
     - text: Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.
       is_correct: true
     - text: Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier after 1 year. Use S3 Versioning for a period of 10 years.
       is_correct: false
     - text: Use Amazon Glacier for immediate access and S3 Object Lock in governance mode for 10 years.
       is_correct: false
    explanation: |
      Correct: S3 Object Lock in compliance mode enforces WORM (Write Once, Read Many) for 10 years, and S3 Glacier Deep Archive provides cost-effective, resilient long-term storage after 1 year.
      Incorrect: 
        S3 Versioning allows deletions and overwrites, not preventing deletion as required.
        S3 Glacier does not provide immediate access, and governance mode allows privileged users to delete objects.
    tags: 
    difficulty: 
    points: 

  - id: q54
    type: multiple_choice
    question: |
      A company runs multiple Windows workloads on AWS. The company's employees use Windows file shares that are hosted on two Amazon EC2 instances. The file shares synchronize data between themselves and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves how users currently access the files.
      What should a solutions architect do to meet these requirements?
    options:
     - text: Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.
       is_correct: true
     - text: Extend the file share environment to Amazon EFS with a Multi-AZ configuration. Migrate all the data to EFS.
       is_correct: false
     - text: Use Amazon S3 with cross-Region replication to maintain duplicate copies and access via AWS Storage Gateway.
       is_correct: false
    explanation: |
      Correct: Amazon FSx for Windows File Server with Multi-AZ provides a highly available, durable, and fully managed Windows file system, preserving SMB access patterns.
      Incorrect: 
        Amazon EFS is designed for Linux workloads and does not support Windows file shares or SMB protocol.
        Amazon S3 is object storage and requires a gateway or client to access as file shares, not preserving native Windows access.
    tags: 
    difficulty: 
    points: 

  - id: q55
    type: multiple_choice
    question: |
      A solutions architect is designing a VPC architecture that includes multiple subnets. The architecture will host applications that use Amazon EC2 instances and Amazon RDS DB instances. The architecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated subnet for databases. Only EC2 instances that run in the private subnets can have access to the RDS databases.
      Which solution will meet these requirements?
    options:
     - text: Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.
       is_correct: true
     - text: Create a network ACL that allows inbound traffic from the private subnets. Attach the network ACL to the database subnets.
       is_correct: false
     - text: Create an IAM role that allows access from the private subnets. Attach the IAM role to the DB instances.
       is_correct: false
    explanation: |
      Correct: Using security group references allows only EC2 instances in the private subnets to access the RDS databases, enforcing least privilege.
      Incorrect: 
        Network ACLs are stateless and apply to entire subnets, not providing instance-level granularity.
        IAM roles are for AWS service permissions, not for controlling network traffic between instances and databases.
    tags: 
    difficulty: 
    points: 

  - id: q56
    type: multiple_choice
    question: |
      A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company's domain name and corresponding certificate so that the third-party services can use HTTPS.
      Which solution will meet these requirements?
    options:
     - text: Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.
       is_correct: true
     - text: Create an Edge-optimized API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Request a certificate from ACM in us-east-1. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.
       is_correct: false
     - text: Use Amazon CloudFront with a custom origin pointing to the API Gateway. Associate the domain name with CloudFront. Import the certificate into ACM in us-east-1. Configure Route 53 to route traffic to CloudFront.
       is_correct: false
    explanation: |
      Correct: This approach enables secure, custom domain HTTPS access to API Gateway using ACM certificates and Route 53 routing.
      Incorrect: 
        Edge-optimized endpoints use CloudFront globally, but certificates must be in us-east-1 for global distribution, not matching the Regional requirement.
        CloudFront adds complexity and cost without need, as Regional API Gateway can handle custom domains directly.
    tags: 
    difficulty: 
    points: 

  - id: q57
    type: multiple_choice
    question: |
      A company needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then must be archived for an additional 9 years. No one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period. The records must be stored with maximum resiliency.
      Which solution will meet these requirements?
    options:
     - text: Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence predictions.
       is_correct: true
     - text: Implement manual content moderation by training moderators to review all uploaded images.
       is_correct: false
     - text: Use Amazon Comprehend to analyze image metadata for inappropriate content.
       is_correct: false
    explanation: |
      Correct: Amazon Rekognition automates inappropriate content detection, reducing development effort, and human review handles edge cases.
      Incorrect: 
        Manual moderation is labor-intensive, error-prone, and does not minimize development effort.
        Amazon Comprehend is for text analysis, not suitable for detecting inappropriate content in images.
    tags: 
    difficulty: 
    points: 

  - id: q58
    type: multiple_choice
    question: |
      A company wants to run its critical applications in containers to meet requirements for scalability and availability. The company prefers to focus on maintenance of the critical applications. The company does not want to be responsible for provisioning and managing the underlying infrastructure that runs the containerized workload.
      What should a solutions architect do to meet these requirements?
    options:
     - text: Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.
       is_correct: true
     - text: Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 instances.
       is_correct: false
     - text: Use Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed nodes.
       is_correct: false
    explanation: |
      Correct: ECS on Fargate is a serverless container solution that abstracts infrastructure management, maximizing scalability and availability.
      Incorrect: 
        ECS on EC2 requires managing EC2 instances, increasing operational overhead.
        EKS with self-managed nodes involves provisioning and managing Kubernetes nodes, not minimizing infrastructure responsibility.
    tags: 
    difficulty: 
    points: 

  - id: q59
    type: multiple_choice
    question: |
      A company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day.
      What should a solutions architect do to transmit and process the clickstream data?
    options:
     - text: Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis.
       is_correct: true
     - text: Send the data directly to Amazon S3. Use Amazon Athena to query the data for analysis.
       is_correct: false
     - text: Use Amazon EMR to process the data in real-time and store results in DynamoDB.
       is_correct: false
    explanation: |
      Correct: Kinesis Data Streams and Firehose provide scalable, managed streaming and delivery to S3, and Redshift enables large-scale analytics.
      Incorrect: 
        Direct S3 upload lacks real-time processing and streaming capabilities for large-scale data ingestion.
        EMR is for big data processing but not optimized for real-time streaming and daily TB-scale ingestion.
    tags: 
    difficulty: 
    points: 

  - id: q60
    type: multiple_choice
    question: |
      A company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward all requests to the website so that the requests will use HTTPS.
      What should a solutions architect do to meet this requirement?
    options:
     - text: Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.
       is_correct: true
     - text: Configure the ALB to use a Network Load Balancer (NLB) for HTTPS traffic.
       is_correct: false
     - text: Use Amazon CloudFront to handle HTTPS redirection in front of the ALB.
       is_correct: false
    explanation: |
      Correct: ALB listener rules can redirect HTTP to HTTPS, ensuring all traffic is encrypted in transit.
      Incorrect: 
        NLB does not support HTTP redirection; it operates at layer 4.
        CloudFront adds unnecessary complexity and cost for simple redirection within the ALB.
    tags: 
    difficulty: 
    points: 

  - id: q61
    type: multiple_choice
    question: |
      A company is developing a two-tier web application on AWS. The company's developers have deployed the application on an Amazon EC2 instance that connects directly to a backend Amazon RDS database. The company must not hardcode database credentials in the application. The company must also implement a solution to automatically rotate the database credentials on a regular basis.
      Which solution will meet these requirements with the LEAST operational overhead?
    options:
     - text: Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the secret. Attach the required permission to the EC2 role to grant access to the secret.
       is_correct: true
     - text: Hardcode the credentials in the application and manually rotate them quarterly.
       is_correct: false
     - text: Store the credentials in AWS Systems Manager Parameter Store and rotate them manually.
       is_correct: false
    explanation: |
      Correct: Secrets Manager securely stores and rotates credentials, and EC2 roles provide secure access without hardcoding.
      Incorrect: 
        Hardcoding credentials violates security best practices and requires code changes for rotation.
        Parameter Store does not support automatic rotation, increasing operational overhead.
    tags: 
    difficulty: 
    points: 

  - id: q62
    type: multiple_choice
    question: |
      A company is deploying a new public web application to AWS. The application will run behind an Application Load Balancer (ALB). The application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by an external certificate authority (CA). The certificate must be rotated each year before the certificate expires.
      What should a solutions architect do to meet these requirements?
    options:
     - text: Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the ALB. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the certificate is nearing expiration. Rotate the certificate manually.
       is_correct: true
     - text: Use ACM to request a public certificate. Apply the certificate to the ALB. ACM will automatically rotate the certificate.
       is_correct: false
     - text: Use a self-signed certificate on the ALB and configure clients to trust it.
       is_correct: false
    explanation: |
      Correct: ACM supports importing external certificates, and EventBridge can notify for manual rotation before expiration.
      Incorrect: 
        ACM-issued certificates are for AWS services and cannot be used for external CAs; rotation is not automatic for imported certs.
        Self-signed certificates are not trusted by browsers and do not meet encryption requirements.
    tags: 
    difficulty: 
    points: 

  - id: q63
    type: multiple_choice
    question: |
      A company is running a popular social media website. The company gives users the ability to upload images to share with other users. The company wants to make sure that the images do not contain inappropriate content. The company needs a solution that minimizes development effort.
      What should a solutions architect do to meet these requirements?
    options:
     - text: Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.
       is_correct: true
     - text: Use Amazon EC2 instances in an Auto Scaling group to process the files in batches.
       is_correct: false
     - text: Use AWS Glue to transform the files and store them in Amazon S3.
       is_correct: false
    explanation: |
      Correct: S3 and Lambda provide a scalable, event-driven, and cost-effective solution for file storage and conversion.
      Incorrect: 
        EC2 instances require provisioning and management, increasing operational overhead.
        AWS Glue is for ETL on structured data, not suitable for simple file conversions.
    tags: 
    difficulty: 
    points: 

  - id: q64
    type: multiple_choice
    question: |
      A company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft Windows shared file storage. The company wants to migrate this workload to the AWS Cloud and is considering various storage options. The storage solution must be highly available and integrated with Active Directory for access control.
      Which solution will satisfy these requirements?
    options:
     - text: Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the on-premises file data to the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway.
       is_correct: true
     - text: Use AWS Direct Connect to connect on-premises to AWS and access file shares directly.
       is_correct: false
     - text: Migrate to Amazon EFS and integrate with Active Directory.
       is_correct: false
    explanation: |
      Correct: FSx for Windows File Server and FSx File Gateway provide seamless, low-latency access to file data across AWS and on-premises with minimal changes.
      Incorrect: 
        Direct Connect provides connectivity but does not offer file storage or synchronization.
        EFS does not support Windows file shares or native Active Directory integration.
    tags: 
    difficulty: 
    points: 

  - id: q65
    type: multiple_choice
    question: |
      A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The hospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG format. The hospital needs to modify the Lambda code to detect protected health information (PHI) in the reports.
      Which solution will meet these requirements with the LEAST operational overhead?
    options:
     - text: Use Amazon Textract to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.
       is_correct: true
     - text: Manually review the reports for PHI using hospital staff.
       is_correct: false
     - text: Use Amazon Rekognition to detect PHI in images and text.
       is_correct: false
    explanation: |
      Correct: Textract and Comprehend Medical automate PHI detection in documents, minimizing manual effort and code changes.
      Incorrect: 
        Manual review is labor-intensive and does not minimize operational overhead.
        Rekognition is for general image analysis, not specialized for medical PHI detection.
    tags: 
    difficulty: 
    points: 

  - id: q66
    type: multiple_choice
    question: |
      A company is storing sensitive user information in an Amazon S3 bucket. The company wants to provide secure access to this bucket from the application tier running on Amazon EC2 instances inside a VPC.
      Which combination of steps should a solutions architect take to accomplish this? (Choose two.)
    options:
     - text: Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.
       is_correct: true
     - text: Store files in S3 Standard for the entire 4 years to ensure immediate access.
       is_correct: false
     - text: Move files to S3 Glacier immediately after upload for cost savings.
       is_correct: false
    explanation: |
      Correct: S3 Standard-IA reduces storage costs for infrequently accessed files while maintaining immediate access, and lifecycle policies automate retention.
      Incorrect: 
        Keeping files in S3 Standard increases costs unnecessarily for infrequently accessed data.
        S3 Glacier does not provide immediate access, violating the requirement for critical business data.
    tags: 
    difficulty: 
    points: 

  - id: q67
    type: multiple_choice
    question: |
      A company runs an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.
      What should a solutions architect do to ensure messages are being processed once only?
    options:
     - text: Use the ChangeMessageVisibility API call to increase the visibility timeout.
       is_correct: true
     - text: Change the SQS queue type to FIFO to prevent duplicates.
       is_correct: false
     - text: Implement deduplication logic in the application code.
       is_correct: false
    explanation: |
      Correct: Increasing the SQS visibility timeout prevents multiple consumers from processing the same message simultaneously, reducing duplicates.
      Incorrect: 
        FIFO queues are for ordered, exactly-once processing, but the issue is with processing time, not queue type.
        Adding deduplication logic increases application complexity and does not address the root cause.
    tags: 
    difficulty: 
    points: 

  - id: q68
    type: multiple_choice
    question: |
      A solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.
      What should the solutions architect do to meet these requirements?
    options:
     - text: Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.
       is_correct: true
     - text: Use only a VPN connection for all traffic.
       is_correct: false
     - text: Use AWS Direct Connect with multiple connections for redundancy.
       is_correct: false
    explanation: |
      Correct: Direct Connect provides low-latency, highly available connectivity, and a VPN backup ensures continued access at lower cost if the primary fails.
      Incorrect: 
        VPN alone may not provide consistent low latency and high availability for all traffic.
        Multiple Direct Connect connections increase costs unnecessarily beyond the requirement.
    tags: 
    difficulty: 
    points: 

  - id: q69
    type: multiple_choice
    question: |
      A company is running a business-critical web application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The company wants the application to be highly available with minimum downtime and minimum loss of data.
      Which solution will meet these requirements with the LEAST operational effort?
    options:
     - text: Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for the database.
       is_correct: true
     - text: Configure the Auto Scaling group in a single Availability Zone for simplicity.
       is_correct: false
     - text: Use Amazon Aurora Serverless for the database without Multi-AZ.
       is_correct: false
    explanation: |
      Correct: Multi-AZ deployments and RDS Proxy provide high availability and minimize downtime and data loss for both the application and database.
      Incorrect: 
        Single AZ configuration does not provide high availability and increases risk of downtime.
        Aurora Serverless without Multi-AZ does not ensure high availability and data durability.
    tags: 
    difficulty: 
    points: 

  - id: q70
    type: multiple_choice
    question: |
      A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group is configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web service.
      The company notices that the NLB is not detecting HTTP errors for the application. These errors require a manual restart of the EC2 instances that run the web service. The company needs to improve the application's availability without writing custom scripts or code.
      What should a solutions architect do to meet these requirements?
    options:
     - text: Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the URL of the company's application. Configure an Auto Scaling action to replace unhealthy instances.
       is_correct: true
     - text: Keep the NLB and configure TCP health checks with custom scripts.
       is_correct: false
     - text: Use an Elastic Load Balancer (ELB) Classic with HTTP health checks.
       is_correct: false
    explanation: |
      Correct: ALB supports HTTP health checks and can automatically replace unhealthy instances, improving availability without custom code.
      Incorrect: 
        NLB with TCP checks does not detect HTTP errors, and custom scripts violate the no-code requirement.
        ELB Classic is legacy and does not support advanced health checks or Auto Scaling integration as effectively.
    tags: 
    difficulty: 
    points: 

  - id: q71
    type: multiple_choice
    question: |
      A company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.
      What should the solutions architect recommend to meet these requirements?
    options:
      - text: Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.
        is_correct: true
      - text: Create an AWS Lambda function to take daily snapshots of the DynamoDB table and store them in S3.
        is_correct: false
      - text: Export the DynamoDB table to Amazon S3 every 15 minutes using AWS Glue.
        is_correct: false
      - text: Enable DynamoDB Streams and replicate data to another table in a different Region.
        is_correct: false
    explanation: |
      Correct: Point-in-time recovery (PITR) allows restoring DynamoDB tables to any point (per second) within the last 35 days. This easily meets the 15-minute RPO and provides a fast restoration path for a 1-hour RTO.
      Incorrect: 
        - Daily snapshots provide an RPO of 24 hours, which fails the 15-minute requirement.
        - Exporting via AWS Glue every 15 minutes is operationally complex and would likely exceed the 1-hour RTO for restoration.
        - DynamoDB Streams/Replication is for high availability and disaster recovery across regions, but it would replicate "corrupted" data instantly to the secondary table, not solving the corruption issue.

  - id: q72
    type: multiple_choice
    question: |
      A company runs a photo processing application that needs to frequently upload and download pictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs.
      How can the solutions architect meet this requirement?
    options:
      - text: Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets.
        is_correct: true
      - text: Configure an AWS Direct Connect connection between the VPC and Amazon S3.
        is_correct: false
      - text: Use an NAT gateway in each Availability Zone to route traffic to Amazon S3.
        is_correct: false
      - text: Provision an AWS PrivateLink interface endpoint for Amazon S3 in the VPC.
        is_correct: false
    explanation: |
      Correct: S3 VPC gateway endpoints enable private, cost-effective access to S3 within the same region. Traffic stays within the AWS network and does not incur data transfer charges typically associated with NAT gateways or public internet.
      Incorrect: 
        - Direct Connect is for on-premises to AWS connectivity, not for intra-region traffic cost reduction.
        - NAT gateways charge per GB processed, which would increase rather than decrease costs.
        - While PrivateLink (Interface Endpoints) works for S3, it has an hourly cost and a per-GB cost, whereas Gateway Endpoints are free of charge.

  - id: q73
    type: multiple_choice
    question: |
      A company recently launched Linux-based application instances on Amazon EC2 in a private subnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VPC. A solutions architect needs to connect from the on-premises network, through the company's internet connection, to the bastion host, and to the application servers. The solutions architect must make sure that the security groups of all the EC2 instances will allow that access.
      Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)
    options:
      - text: Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company.
        is_correct: true
      - text: Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.
        is_correct: true
      - text: Replace the current security group of the bastion host with one that allows inbound ICMP traffic from 0.0.0.0/0.
        is_correct: false
      - text: Configure the application instances' security group to allow inbound SSH access from the company's external IP range.
        is_correct: false
      - text: Attach an IAM role to the bastion host that allows it to communicate with the application instances.
        is_correct: false
    explanation: |
      Correct: Restricting bastion host access to the company's specific external IP ensures security. Limiting application server SSH access to the bastion's private IP (or its security group ID) ensures that only the bastion can reach the private instances.
      Incorrect: 
        - ICMP (ping) does not allow SSH access.
        - The application instances are in a private subnet; they cannot be reached directly from a company's external IP via the internet.
        - IAM roles control permissions for AWS services/APIs, not network-level traffic (which is the role of Security Groups).

  - id: q74
    type: multiple_choice
    question: |
      A solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.
      How should security groups be configured in this situation? (Choose two.)
    options:
      - text: Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.
        is_correct: true
      - text: Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier.
        is_correct: true
      - text: Configure the security group for the web tier to allow outbound traffic on port 1433 to the database tier's security group.
        is_correct: false
      - text: Configure the security group for the database tier to allow inbound traffic on port 1433 from 0.0.0.0/0.
        is_correct: false
      - text: Configure the security group for the web tier to allow inbound traffic on port 1433 from the database tier.
        is_correct: false
    explanation: |
      Correct: Allowing HTTPS (443) from everywhere is necessary for a public web tier. Restricting the database (1433) to only accept traffic from the web tier's security group is a security best practice (chaining security groups).
      Incorrect: 
        - While the web tier needs to send traffic to the DB, Security Groups are stateful; if inbound 1433 is allowed on the DB, the return traffic is automatically allowed.
        - Allowing the database to be accessed from 0.0.0.0/0 is a severe security risk and unnecessary for a private subnet.
        - The web tier should not receive inbound traffic from the database on port 1433.

  - id: q75
    type: multiple_choice
    question: |
      A company wants to move a multi-tiered application from on premises to the AWS Cloud to improve the application's performance. The application consists of application tiers that communicate with each other by way of RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect must design a solution that resolves these issues and modernizes the application.
      Which solution meets these requirements and is the MOST operationally efficient?
    options:
      - text: Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application layer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between application services.
        is_correct: true
      - text: Use Amazon EC2 instances in an Auto Scaling group to host the RESTful services. Use an Amazon MQ broker to manage communication between tiers.
        is_correct: false
      - text: Deploy the application on Amazon EKS using Kubernetes pods. Use a Service Mesh (App Mesh) to manage communication and retries between services.
        is_correct: false
      - text: Use an Application Load Balancer to route traffic to EC2 instances. Increase the instance size (vertical scaling) to handle peak loads.
        is_correct: false
    explanation: |
      Correct: A serverless approach (API Gateway + Lambda) combined with SQS provides a highly scalable and decoupled architecture. SQS acts as a buffer, preventing dropped transactions during load spikes, and it is the most operationally efficient (no servers to manage).
      Incorrect: 
        - Amazon MQ and EC2 require significant operational overhead for patching and scaling compared to serverless.
        - EKS and App Mesh are modern but significantly more complex to manage than Lambda and SQS.
        - Vertical scaling (increasing instance size) is not a modern cloud-native solution and has a hard ceiling that doesn't resolve the "dropped transactions" issue as effectively as a decoupled queue.

  - id: q76
    type: multiple_choice
    question: |
      A company receives 10 TB of instrumentation data each day from several machines located at a single factory. The data consists of JSON files stored on a storage area network (SAN) in an on-premises data center located within the factory. The company wants to send this data to Amazon S3 where it can be accessed by several additional systems that provide critical near-real-time analytics. A secure transfer is important because the data is considered sensitive.
      Which solution offers the MOST reliable data transfer?
    options:
      - text: AWS DataSync over AWS Direct Connect
        is_correct: true
      - text: Use AWS Snowball Edge devices to transfer the data to Amazon S3.
        is_correct: false
      - text: Create an AWS Site-to-Site VPN and use the AWS CLI to copy files to Amazon S3.
        is_correct: false
      - text: Deploy an AWS Storage Gateway file gateway and use it to upload data to Amazon S3.
        is_correct: false
    explanation: |
      Correct: AWS DataSync over Direct Connect provides a high-throughput, dedicated, and secure connection. DataSync is purpose-built for large-scale data migration, offering built-in validation and reliability for 10 TB daily transfers.
      Incorrect: 
        - Snowball Edge is for one-time migrations or disconnected environments; it is not practical for daily 10 TB transfers due to shipping delays.
        - Site-to-Site VPN is limited by the public internet's bandwidth and reliability, making it unsuitable for consistent 10 TB daily loads.
        - Storage Gateway is better for hybrid cloud storage and caching, not for high-speed, bulk data ingestion of this magnitude.

  - id: q77
    type: multiple_choice
    question: |
      A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data.
      Which solution will meet these requirements with the LEAST operational overhead?
    options:
      - text: Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.
        is_correct: true
      - text: Deploy Amazon EC2 instances running an Apache Kafka cluster to receive data. Use Apache Spark Streaming on Amazon EMR to transform the data and save it to Amazon S3.
        is_correct: false
      - text: Create an AWS AppSync GraphQL API. Use AWS Glue ETL jobs to process and transform the data from the API's database and store it in Amazon S3.
        is_correct: false
      - text: Use an Application Load Balancer to route traffic to an Auto Scaling group of EC2 instances. Use a custom Python script to transform data and write it directly to Amazon RDS.
        is_correct: false
    explanation: |
      Correct: This is a serverless, managed pipeline. API Gateway provides the API, Kinesis handles the stream, Lambda transforms the data, and Firehose delivers it to S3, all with minimal management.
      Incorrect: 
        - Apache Kafka and EMR require significant operational overhead for cluster management and scaling.
        - AWS Glue ETL is typically for batch processing, not real-time streaming ingestion.
        - Custom scripts on EC2 and writing to RDS involves managing servers and database overhead, which doesn't scale as easily for "least operational overhead."

  - id: q78
    type: multiple_choice
    question: |
      A company needs to keep user transaction data in an Amazon DynamoDB table. The company must retain the data for 7 years.
      What is the MOST operationally efficient solution that meets these requirements?
    options:
      - text: Use AWS Backup to create backup schedules and retention policies for the table.
        is_correct: true
      - text: Write a custom AWS Lambda function to export the table to Amazon S3 every month and set an S3 Lifecycle policy to 7 years.
        is_correct: false
      - text: Enable DynamoDB Streams and use a Lambda function to record changes in an Amazon RDS database.
        is_correct: false
      - text: Use the DynamoDB TTL (Time to Live) feature to mark items for deletion after 7 years.
        is_correct: false
    explanation: |
      Correct: AWS Backup is a fully managed service that centralizes and automates data protection. It natively supports DynamoDB and handles long-term retention policies with a few clicks.
      Incorrect: 
        - Custom Lambda scripts for S3 exports require maintenance, monitoring, and error handling (high overhead).
        - Replicating to RDS doubles storage costs and adds management complexity.
        - TTL is for deleting data automatically, but it doesn't provide a "backup" or "retention" mechanism for auditing/recovery over 7 years; it just removes old items.

  - id: q79
    type: multiple_choice
    question: |
      A company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about cost optimization. The table will not be used on most mornings. In the evenings, the read and write traffic will often be unpredictable. When traffic spikes occur, they will happen very quickly.
      What should a solutions architect recommend?
    options:
      - text: Create a DynamoDB table in on-demand capacity mode.
        is_correct: true
      - text: Create a DynamoDB table with provisioned capacity and configure Application Auto Scaling.
        is_correct: false
      - text: Use DynamoDB Accelerator (DAX) to cache the frequent read requests during the evening.
        is_correct: false
      - text: Create a DynamoDB table with provisioned capacity and set high RCU/WCU values to handle spikes.
        is_correct: false
    explanation: |
      Correct: On-demand mode is ideal for unpredictable workloads and tables that sit idle. It scales instantly to accommodate sudden spikes and you only pay for what you use, making it the most cost-optimized for this profile.
      Incorrect: 
        - Provisioned capacity with Auto Scaling is better for gradual changes; sudden, very quick spikes can lead to Throttling before Auto Scaling can react.
        - DAX helps with read performance/latency but does not solve the cost optimization for unpredictable "write" traffic or the idle morning periods.
        - High fixed RCU/WCU values would be extremely expensive since the table is not used in the mornings.

  - id: q80
    type: multiple_choice
    question: |
      A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an application migration initiative. A solutions architect needs to share an Amazon Machine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS volume snapshots.
      What is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account?
    options:
      - text: Modify the key policy to grant the MSP Partner's account permissions to use the KMS key. Share the AMI with the MSP Partner's account.
        is_correct: true
      - text: Make the AMI public and share the KMS key's ARN with the MSP Partner.
        is_correct: false
      - text: Copy the AMI to the MSP Partner's account and use a new KMS key in the destination account for encryption.
        is_correct: false
      - text: Create a pre-signed URL for the AMI and provide it to the MSP Partner for download.
        is_correct: false
    explanation: |
      Correct: Since the AMI is encrypted with a customer managed key, you must share both the AMI and provide the target account permissions (via the Key Policy) to use the KMS key for decryption.
      Incorrect: 
        - Making an AMI public is insecure and would not allow the use of a private KMS key.
        - You cannot "copy" an AMI directly into another account if you don't first have permission to use the encryption key used by that AMI.
        - Pre-signed URLs are for S3 objects, not for sharing AMIs between AWS accounts.