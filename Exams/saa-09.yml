questions:
  - id: q401
    type: multiple_choice
    question: |
      A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage.

      The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand.

      Which solution will meet these requirements?
    options:
      - text: Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.
        is_correct: true
      - text: Deploy the application on a single large Amazon EC2 instance. Use an Amazon RDS DB instance with a Read Replica in a different Region.
        is_correct: false
      - text: Use an Amazon EC2 instance with an EBS volume for the database. Set up a cron job to take snapshots every hour.
        is_correct: false
      - text: Deploy the application using AWS Elastic Beanstalk in a single Availability Zone. Use an Amazon DynamoDB table with on-demand capacity.
        is_correct: false
    explanation: |
      Correct: Auto Scaling across multiple Availability Zones combined with an RDS Multi-AZ configuration ensures that there is no single point of failure at the compute or database layer. If an AZ fails, the ASG launches new instances in healthy zones, and RDS automatically fails over to the standby.
      Incorrect: 
        - A single large EC2 instance is a single point of failure (SPOF).
        - EBS snapshots every hour do not prevent data loss between snapshots and require significant RTO to restore.
        - Using a single Availability Zone violates the high availability requirement.



  - id: q403
    type: multiple_choice
    question: |
      A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. 
      What is the MOST secure way to grant these permissions?
    options:
      - text: Create an IAM execution role for the Lambda function that includes a policy allowing the s3:PutObject action on the specific S3 bucket.
        is_correct: true
      - text: Create an IAM user with S3 upload permissions and store the access keys as environment variables in the Lambda function.
        is_correct: false
      - text: Attach a bucket policy to the S3 bucket that allows public write access from any source.
        is_correct: false
      - text: Grant the Lambda function the AdministratorAccess managed policy to ensure it has all necessary permissions.
        is_correct: false
    explanation: |
      Correct: The principle of least privilege dictates that you should grant only the permissions necessary to perform a task. An IAM execution role assigned to the Lambda function is the standard and most secure way to provide temporary credentials.
      Incorrect: 
        - Storing static access keys in environment variables is a security risk; IAM roles provide temporary, rotating credentials.
        - Public write access on a bucket is a major security vulnerability.
        - AdministratorAccess violates the principle of least privilege by granting far more permissions than required.

  - id: q449
    type: multiple_choice
    question: |
      A company is planning to migrate a three-tier web application to AWS. The application consists of a web tier, an application tier, and a MySQL database. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency.

      Which combination of solutions will meet these requirements? (Choose three.)
    options:
      - text: Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.
        is_correct: true
      - text: Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups.
        is_correct: true
      - text: Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups.
        is_correct: true
      - text: Place the web tier in a public subnet and assign a public IP address to every EC2 instance to simplify internet access.
        is_correct: false
      - text: Use a single Availability Zone to reduce data transfer costs between the application and database tiers.
        is_correct: false
      - text: Store all application data and the database on a single large Amazon EBS volume shared across multiple instances.
        is_correct: false
    explanation: |
      Correct: These three options follow the Well-Architected Framework: Multi-AZ for resiliency, private subnets for security, and security group chaining (referencing other SGs) for minimal lateral movement risk.
      Incorrect: 
        - Assigning public IPs to every instance increases the attack surface. Use a Load Balancer in the public subnet instead.
        - Single AZ deployment creates a single point of failure.
        - EBS volumes (standard) cannot be shared across multiple instances in the way described for a database tier; RDS is the managed, high-availability solution.



  - id: q450
    type: multiple_choice
    question: |
      A solutions architect is designing a data processing application that will run on Amazon EC2 instances. The application will process large amounts of data stored in an Amazon S3 bucket. The processing involves frequent reads and writes to temporary storage.
      Which storage option provides the HIGHEST performance for the temporary data?
    options:
      - text: Amazon EC2 Instance Store
        is_correct: true
      - text: Amazon EBS Provisioned IOPS SSD (io2)
        is_correct: false
      - text: Amazon Elastic File System (Amazon EFS)
        is_correct: false
      - text: Amazon S3 with Transfer Acceleration
        is_correct: false
    explanation: |
      Correct: Instance Store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer, offering the lowest latency and highest IOPS for temporary/scratch data.
      Incorrect: 
        - While io2 is high performance, it is network-attached (EBS), which introduces slightly more latency than physically attached Instance Store.
        - EFS is a shared network file system and is significantly slower than local block storage.
        - S3 is object storage, not suitable for high-speed temporary block-level processing.